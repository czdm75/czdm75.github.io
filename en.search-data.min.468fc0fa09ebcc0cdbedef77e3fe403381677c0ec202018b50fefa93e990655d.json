[{"id":0,"href":"/notes/core-java-impatient/1/","title":"1. Basic OOP","section":"Core Java for Impatients","content":" 第一章 基本的编程结构 # 基本类型和变量 # 数字 # 4 字节整型 int 约正负 21 亿，8字节整型 long 约正负 9×10^19，约 900 千亿。4 字节浮点数 float 约 6 位有效数字，范围至正负 10^38 ；8 字节浮点数 double 约 15 位有效数字，范围至正负 10^308。\nInteger Byte Short Long Double Float 均有 MAX_VALUE 和 MIN_VALUE 成员变量，指示其边界范围。Float 和 Double 还有 NaN、POSITIVE_INFINITY 和 NEGATIVE_INFINITY 三个静态变量。NaN 之间互不相等。\nJava 还提供了 BigInteger 和 BigDemical 两个类，尤其适用于金融。\n在部分机器上，如 Intel x86平台，使用 80 bit 的浮点单元来提高浮点运算的精度。如果需要严格的 64 bit 浮点运算，可以在方法前加上 strictfp 修饰符。另外几个不常见的关键字是与多线程有关的 volatile，与序列化有关的 transient 和与跨语言调用有关的 native。此外，StrictMath 类也提供了类似的功能。\n字符 # char 描述的是 Unicode 中的编码单元，即一个 16 bit 整数。可以直接使用 Unicode 编码如 \\u004A，甚至直接用数字 0x004A 来声明 char。\n此外，还有转义符：\\n \\r \\t \\b \\\\ 以及单引号。\n操作符 # 运算符 操作 运算符 操作 运算符 操作 ~ 按位取反 (cast) 强制转换 new 构造对象 \u0026lt;\u0026lt; 左移，补0 \u0026gt;\u0026gt; 逻辑右移 \u0026gt;\u0026gt;\u0026gt; 算数右移 instanceof 判断实例 \u0026amp; 按位与 ^ 按位异或 ` ` 按位或 ? : 条件语句 \u0026amp;\u0026amp; 求模操作符 % 对于负数是不安全的，如 -7%5结果为 -2。最好使用 Math.floorMod(-7,5) 得到 3。 如果整数运算中需要在溢出等特殊情况时抛出异常，需要使用 Math.xxxExact() 系列的方法。 在算术操作时，基本数字类型之间会发生转型。转型的 方向为：int-\u0026gt;long-\u0026gt;float-\u0026gt;double。注意，这意味着 short 和 byte 在运算时也被作为 int 处理，并需要重新强制转换回来。 四舍五入使用 Math.round 方法，返回 long 型。如果在 cast 中担心丢失数字，可以使用 Math.toIntExact() 方法，溢出时会报异常。 Java 的逻辑操作是短路的。但是，为了代码的可读性，不建议在判断语句中使用带有副作用的语句。 \u0026gt;\u0026gt; 在高位补 0，\u0026gt;\u0026gt;\u0026gt; 在高位补符号位。另外，1\u0026lt;\u0026lt;35 == 1\u0026lt;\u0026lt;3。如果是 long 类型，则模 64。 字符串 # 连接与加号混合可能出现问题。如，\u0026quot;\u0026quot;+42+1 可能得到 421。使用圆括号来避免。连接的实现实际上是 StringBuilder 类。\nString.equalsIgnoreCase 方法忽略大小写。String.compareTo 方法使用 UTF-16 顺序，结果可能和直觉不一样。要使用自然语言顺序，使用 Collator 对象。这是一个单例的 Comparator 对象，其排序与 Locale 和其他设置有关。\nInteger.parseInt(String) 返回一个 int，Integer.valueOf(String) 返回一个 Integer。parseInt(String, int) 指定数字字符串的进制。Double 的情况类似。\nJava 的 String 是 Immutable 的。\nUnicode 编码问题 # Java 基于 UTF-16，但 Unicode 的范围已经超过了 16 bit。于是，超出的字符使用连续两个 16 bit 的整数，即两个编码单元（Code Unit），组成一个编码点（Code Point）。这个过程与反斜杠转义类似。如果不使用高位区的字符，则可以直接用 String.charAt(int) 方法。这样得到的返回值是 char，即 16 bit 整数。如果这个字符是高位的，就会面临得到半个字符的情况。这时应该使用 String.codePointAt 以得到两个编码单元，其返回值是一个 int，即 32 bit 整数。如果字符是低位的，那么返回值的一半是全 0。否则，返回值内就是两个编码单元。还可以使用：\nint[] codepoints = \u0026#34;abc\u0026#34;.codePoints().toArray() 控制台 IO # Scanner 类的 nextLine() 方法读取一行，next() 方法读取以空格为分割的单词。\n读取密码时，使用：\nConsole terminal = System.console(); char[] password = terminal.readPassword(\u0026#34;hint info\u0026#34;); 由于 String 是 Immutable 的，在 GC 之前，它都会留在内存里，不够安全。相比之下，char[] 可以在使用之后就覆盖掉。\nSystem.out.printf() 和 String.format() 接受类似的参数，进行格式化。\n控制流和数组 # switch 语句可以接受的标签包括：char byte short int 及其包装类，String 和枚举类型。需要注意的是，由于 fall through 特性，switch 的各个分支之间的声明变量是共用的。\nbreak 和 continue 语句可以使用标签，以方便跳出循环：\nouter: while(){ while(){ break outer; } } 对于基本类型的数组，默认使用 0 和 false 来填充。对于对象数组，默认使用 null 来填充。\n数组和 ArrayList 之间 常常互相转换和进行深拷贝：\nString[] arr = list.toArray(new String[]); ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(Arrays.asList(arr)); ArrayList\u0026lt;String\u0026gt; newList = new ArrayList\u0026lt;\u0026gt;(list); String[] newArr = Arrays.copyOf(arr); Arrays 和 Collections 类提供了数组的常见算法。\nArrays.fill(arr, value); Arrays.sort(arr); // 扩展一个数组 arr = Arrays.copyOf(arr, arr.length * 2); // 多线程的排序，Collections 里没有这个方法 Array.parallelSort(arr); // Arrays 里没有这两个方法 Collections.shuffle(list); Collections.reverse(list); 在使用 toString 时，数组和 ArrayList 都能够打印出元素内容。但对于多维数组，则要使用 deepToString。\n可变参数 # 可变参数只能是最后一个参数。\nvoid method(double... nums) { //nums is a double[] } //call this method method(1.0, 2.0); double[] nums = {1.0, 2.0} method(nums) 第二章 面向对象编程 # 包 # 当使用 javac 程序时，class 文件将会被放在与源代码相同的位置。但在运行类文件时，必须将其放置在对应包的文件夹里。\n# 指定class文件的位置 javac -d \u0026lt;path\u0026gt; # jar程序的参数与tar程序类似 jar cvf xxx.jar com/*.class # 指定jar文件的主类 jar cvfe xxx.jar com.MainClass com/*.class # 运行jar文件 java -jar xxx.jar Class Path # class path 可以包含：\n匹配包名的包含 class 文件的子目录 jar 文件 包含 jar 文件的子目录 java -cp .:../libs/lib1.jar com.MainClass.class # 或者： export CLASSPATH=.:../libs/lib1.jar 这意味着使用当前目录和 libs 目录中的 lib1.jar 作为 class path。对于 Windows：\njava -cp .;..\\libs\\lib1.jar com.MainClass.class # 或者： SET CLASSPATH=.;..\\libs\\lib1.jar class path 默认为当前目录。但如果额外设置了 class path，则需要包含 .。\n包密封 # 在 Java 中，所有以 java 开始的包都不会被加载，这是为了防止 Java 本身被篡改。在封装 jar 包时，也可以使用类似的办法。提供一个 jar 包的 manifest 文件：\nName: com/ Sealed: true Name: net/ Sealed: true 或者，仅包含第二行的内容，这意味着整个 jar 中的包都被封闭。\nJavaDoc # JavaDoc 工具会将符合格式的所有包，public 类和 public 及 protected成员的注释转化为 HTML 文档。注释使用 /**...*/ 格式，放在描述对象的前面，其中可以使用 HTML 的标签。只推荐使用 \u0026lt;em\u0026gt; \u0026lt;code\u0026gt; \u0026lt;strong\u0026gt; \u0026lt;img\u0026gt; 等标签，避免使用标题，以免打乱文档。\n当使用图片时，将图片保存在 doc-files 目录下，并使用 HTML 标签：\u0026lt;img src=\u0026quot;doc-files/uml.jpg\u0026quot; alt=\u0026quot;UML\u0026quot;\u0026gt;。最后：\njavadoc -d path package1 packege2 ... # 添加作者和版本信息 javadoc -version -author # 将标准库链接到Oracle文档 javadoc -link http://docs.oracle.com/javase/8/docs/api *.java # 将源代码生成为HTML并链接 javadoc -linksource package 类注释 # /** * Something * @author someone * @version 1.1 */ 方法注释 # /** * Something * @param param1 something * @return something * @throws something */ 变量注释 # 通常用在静态常量处\n/** * something */ 通用标记 # /** * @since version 1.1 * @deprecated */ 另外，还可以使用 @Deprecated 注解在编译中发出警告。\n链接 # /** * see 中可以省去包名或类名，这样会在当前包或类中查找 * @see com.test.Clazz#method(double) * @see \u0026lt;a href=\u0026#34;http://xxx\u0026#34;\u0026gt;Link\u0026lt;/a\u0026gt; * @see something * @link link的用法类似 */ 包和概述注释 # 对于包，使用一个 package-info.java ：\n/** * something */ package com.test; 对于整个项目的 Overview 注释，提供一个 overview.html 文件，放在所有源文件的父目录里。JavaDoc 会将其 \u0026lt;body\u0026gt; 标签的内容放在 Overview 里。\n静态导入 # import static java.lang.Math.*; 此时可以使用 Math 类的静态方法和静态变量。或：\nimport static java.lang.Math.sqrt; import static java.lang.Math.PI; 嵌套类 # 静态嵌套类 # 私有静态嵌套类 # public class Invoice { private static class Item { //此处无需访问控制，因为Item类已经是private String decription; double price() { return quantity * unitiPrice; } } private ArrayList\u0026lt;Item\u0026gt; items = new Arraylist\u0026lt;\u0026gt;(); public void addItem(String description) { ... } } 这时，只有 Invoice 类中的方法能够访问 Item 类。\n公有静态嵌套类 # public class Invoice { public static class Item {} } 此时任何人都可以直接访问 Item 类\npublic class Other { Invoice.Item item = new Item(); //其他位置可直接访问Item类 } 在静态嵌套类中，两个类除了访问权限和方法之外并没有实际的关系，嵌套类的使用和正常的类没有任何区别。\n内部类 # public class Network { public class Member { //Member对象将会知道自己从属的Network对象 private String name; public void leave() { members.remove(this); //从内部类中访问外部类的变量 //等同于 Network.this.members.remove(this) } } private ArrayList\u0026lt;Member\u0026gt; members; public Member enroll(String name) { members.add(new Member(name); //等同于this.new Member() } } 此时可以在其他位置引用：\nNetwork myFace = new Network(); //在Network类外使用 Member 类 Network.Member fred = myFace.enroll(\u0026#34;fred\u0026#34;); Network.Member f = myFace.new Member(); fred.leave(); 内部类的特殊情况 # 内部类的一个对象从属于外部类的一个对象，这会带来一些复杂的情况。\n首先，内部类除了编译时常量 static final Class variable = xxx; 之外，并不能拥有静态成员，因为我们无法确定，这个成员是对于外部对象静态，还是对于 JVM 静态。\n通常，我们可以直接在内部类中访问外部类的对象。当我们需要外部对象本身的时候，可以使用 OuterClass.this。\n无论是嵌套类还是内部类，都将被编译成一个 Outer$Inner.class 文件。实现上，内部类的每个对象持有一个指向自己所从属的外部对象的引用。\n局部类 # 局部类用于产生一个实现某个接口的对象，在 Java 8 之后逐渐被 lambda 表达式取代一部分。这样定义的类被控制在方法内。由于这个类不能被外部访问，所以不需要访问控制。另外，也可以声明匿名的局部类。\npublic static IntSequence randomInts(int low, int high) { class RandomSequence implements IntSequence { // 此处可以随意访问类的变量和randomInts方法的参数等 } return new RandomSequence(); } public static InySequence randomDoubles(double low, double high) { return new IntSequence { ... } } "},{"id":1,"href":"/notes/programming-scala/1/","title":"1. Basics","section":"Programming in Scala","content":" 入门 # apply 方法 # 对于代码：\nval arr = Array(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) arr(0) arr(0) = \u0026#34;c\u0026#34; 实际上是调用了：\nval arr = Array.apply(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) arr.apply(0) arr.update(0, \u0026#34;c\u0026#34;) 列表 # Scala 默认的 List 是 Immutable 的。可以对列表进行拼接：\nval l = List(1, 2) 1 :: l // List(1, 1, 2) l ::: l // List(1, 2, 1, 2) l :: l // List(List(1, 2), 1, 2) 首先，由于 List 是 Immutable 的，所以所有的拼接操作都返回一个新的 List。\n三冒号的写法比较容易理解：它将两个列表连接起来。对于双冒号，则是将前面的元素与后面的列表连接起来。在第四行代码中，由于双冒号前面的元素被作为一个对象来操作，因此得到的是一个具有嵌套结构的 Any 列表。\n而且，理所应当，双冒号应当是右结合的操作符，因为任何一个对象不应该持有这样与其没有太大直接关系的操作，所以 :: 方法应该是列表的方法而不是所有对象都具有的方法。但是，通常调用的操作符是左结合的，如 1 + 2 实际为 1.+(2)。Scala 简单地使用冒号来区分，如果操作符的最后一个字符是冒号，那么操作符就是右结合的。\n对于列表，在其前面增加元素是一个高效的 O(1) 的操作，而 append 则是一个 O(n) 的操作，其中 n 是列表的长度。相应地，可变的集合的追加操作就是高效的，例如 ArrayBuffer。这两种相反方向的集合适用于不同的场景。也可以使用 :: 来创建列表，再调用 reverse()。列表还提供了其他一些函数：\nval l = List(1, 2 ,3) l.init // List(1, 2) l.tail // List(2, 3) l.drop(2) // List(3) l.dropRight(2) // List(1) List() 或 Nil 表示空列表。如果要从头使用双冒号来定义一个列表当然是可行的，但最后一个元素必须是 Nil，因为 ::() 是列表上的方法。形如：\nval list = 1 :: 2 :: 3 :: Nil 最后一个 :: 需要在 Nil 上进行调用。\n元组 # 元组实际上是在 scala 包中定义的一系列类。其访问方法是：\nval t = (1, \u0026#34;a\u0026#34;) // class: scala.Tuple2[Int, String] t._1 // 1 t._2 // \u0026#34;a\u0026#34; 元组的序号之所以从 1 开始是继承了其他语言，如 ML 的传统。\n元组与列表的区别是，元组对每个元素保留泛型的类型参数，它能够保留每一个元素的类型信息，而列表不能。它只能保留所有元素的父类型。\n(1, \u0026#34;a\u0026#34;) // Tuple2[Int, String] List(1, \u0026#34;a\u0026#34;) // List[Any] Immutable / Mutable, Set 与 Map # 在 Scala 中，List 总是 Immutable 的，Array 总是 Mutable 的。Array 还有长度可变的版本 ArrayBuffer。而对于 Set 和 Map，Scala 分别提供了可变与不可变的两种类型，使用包和 Trait 进行区分：\n出于函数式的考虑，Scala 默认引入的是不可变的版本，使用可变的版本则需要显式调用。当然，也可以显式地指定要使用的集合的实现版本：\nimport scala.collection.mutable val s = mutable.Set(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) import scala.collection.immutable.HashSet val s2 = HashSet(\u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) 在对这两种集合进行操作时，就会形成不同的模式：\nvar m = Set(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) m += \u0026#34;c\u0026#34; // m is a new Set now val m = mutable.Set(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;) m += \u0026#34;c\u0026#34; // still the Set before 对于 Map 来说，主要的区别是其使用二元元组（Tuple2）作为输入元素，这里用到了 Scala 中生成元组的函数 -\u0026gt;()：\nMap(1 -\u0026gt; \u0026#34;a\u0026#34;, 2 -\u0026gt; \u0026#34;b\u0026#34;) // Map(1 -\u0026gt; a, 2 -\u0026gt; b) 1 -\u0026gt; \u0026#34;a\u0026#34; // (1,a) 基础类型、类和对象 # 单例对象 # 对于单例对象，scalac 会编译成一个名为 ObjectName$ 的 class 文件。如果需要进行大量的编译而不希望每一次调用 scalac 命令都要重新寻找 classpath 等，可以使用 fsc 命令来编译。在调用 fsc 之后，会拉起一个守护进程，再次调用 fsc 就会将源文件发送到这个守护进程的端口上。最后，使用 fsc -shutdown 停止守护进程。\n除了使用 main 函数，Scala 还提供了一个用于创建 App 的 Trait ：\nobject A extends App { args foreach println } 基础类型 # 0x00FF // Int = 255 0xCAFEBABE // Int = -889275714 0xCAFEBABEL // Long = 3405691582 1.23e-2 // Double = 0.0123 1e2 // Double = 100.0 \u0026#39;\\u0041\u0026#39; // Char = A 字符串 # Raw 字符串\n\u0026#34;\u0026#34;\u0026#34;AB BA\u0026#34;\u0026#34;\u0026#34; // \u0026#34;AB\\n BA\u0026#34; \u0026#34;\u0026#34;\u0026#34;ab |ba\u0026#34;\u0026#34;\u0026#34;.stripMargin // \u0026#34;AB\\nBA\u0026#34; 可以看到，RAW 字符串里保留了空格和换行符，如果在定义中使用管道符并就能通过 stripMargin 避免将缩进用的字符串包括在内。\n字符串插值器（Interpolater） # Scala 定义了三个字符串插值器。f 允许使用 C 风格的格式化，raw 中的转义符不会生效，s 是通常的插值器。由于 f 被实现为了宏，它可以在编译期进行类型检查，printf 则不能。\nprint(f\u0026#34;hello, $name, ${age + 0.5}%7.2f\u0026#34;) print(s\u0026#34;abd $name\u0026#34;) print(raw\u0026#34;\\t\\n\u0026#34;) 也可以自己定义插值器。\nimport java.time.LocalDate implicit class DateInterpolator(val sc: StringContext) extends AnyVal { def date(args: Any*): LocalDate = LocalDate.of( args(0).toString.toInt, args(1).toString.toInt, args(2).toString.toInt) } val y = 2018 val m = 4 val d = 5 date\u0026#34;$y, $m, $d\u0026#34; // java.time.LocalDate = 2018-04-05 符号 Symbol # Symbol 对象的作用和 Java 中的 Interned String 类似，通过一个单引号来声明。对于一般的字符串，频繁使用字面量可能造成大量字符串对象的创建，给系统的性能带来压力，解决办法是将其放入常量池，和 JVM 对数字的处理一样。\n对于 Symbol 来说，当符号创建时，实际上调用了 Symbol.apply(name: String) 在常量池中建立了一个对象，之后再次使用时引用的将会时同一个对象。这样，在比较时避免了字符串的重复构造和遍历，而是直接比较地址即可。\n\u0026#39;abc // Symbol = \u0026#39;abc \u0026#39;abc.name // String = abc 操作符 # 对于超过一个参数的方法，可以将参数用括号包起来使用中缀表达：\nobject A { def op(i: Int, j:Int): Unit = print(i + j) } A op (1, 2) Scala 只允许四个前缀操作符：+ - ! ~，使用 unary_ 来定义：\nclass Num(val a: Int) { def unary_-(): Num = new Num(-a) } -(new Num(3)).a 后缀操作符的形式是没有参数的方法。通常，当方法有副作用时保留括号来调用，而在方法没有副作用时不适用括号，使其看起来就像对变量成员的访问一样：\n\u0026#34;ABC\u0026#34;.reverse Scala 的 == 方法调用了 equals 方法，但是 null 安全的。eq 和 ne 方法比较引用。因此，Scala 程序惯用 == 来比较对象，编写 equals 就更加重要。\n因为使用函数作为操作符，操作符的有限集比较复杂。除了正常可以理解的，* 高于 + 这类规则外，对于自己定义的操作符，Scala 使用第一个字符来判断，这样做的目的是，例如我们自己定义了 **() 和 ++() 方法，那么 **() 的优先级更高，这符合我们的心理预期。\n另外任何赋值操作符的优先级与正常的赋值操作符 = 相同。也就是说，+= -= 这类以等号结尾的操作符优先级与赋值操作符相等，无论首字母是什么。因此，在 Scala 类的操作符的定义中，遵循公共的常规约定十分重要。\n对于这些特殊字符，Scala 会将其转换为一定的字符串，以和 Java 兼容。例如 :-\u0026gt; 需要在 Java 中使用 $colon$minus$greater 来访问。\n对于基本类型的包装类，Scala 实际上将复杂的操作定义在了它们对应的富包装类中，即 RichInt StringOps 等类，并提供隐式转换。\nApplication 特质 # 最后简单介绍一下 App 特质。这个特质可以这样使用：\nobject MyApp extends App { for (arg \u0026lt;- args) println(arg) } 这样编写之后，这个程序就可以正常地被编译和运行。只需要在命令行 scala MyApp 即可，还可以正常地使用命令行参数，方便创建简单的程序。这样做的原理是，这个对象继承了 App 这个 trait，这些代码会被父类的 main 函数中被调用，这里不做过多解释。\n构造函数 # 对于 Scala 类：\nclass A(i: Int) { println(i) } 大致上相当于这样一个 Java 类：\npublic class A { public A(int i) { System.out.println(i) } } 此外，还可以使用 require 对构造函数的参数进行限制，如果不满足则会自动抛出 IllegalArgumentException。\nclass Rational(n: Int, d: Int) { require(d != 0) override def toString = n + \u0026#34;/\u0026#34; + d } new Rational(1, 0) // java.lang.IllegalArgumentException: requirement failed // at scala.Predef$.require(Predef.scala:212) // ... 33 elided 这样声明的 n 和 d 的作用域在类内，相当于两个 private val，因此 toString 可以访问，但无法从对象外使用 obj.n 访问。要在对象外访问，要将其声明为字段。\nclass Rational(val n: Int, val d: Int) class Rational(n: Int, d: Int) { val numer = n val denom = d } 要创建其他的构造函数，使用 this ：\nclass Rational(val n: Int, val d: Int) { def this(n: Int) = this(n, 1) } 其他构造函数必须首先调用主构造函数，这样就保证了 Scala 中对象的单入口。\n最后，对于这样一个有理数类，还缺少一个合适的隐式转换，以便其和一般的整数一起工作：\nimplicit def intToRational(x: Int) = new Rational(x) 隐式转换遵守作用域的规则。因此，通常需要进行导入。\n控制流 # for # for (i \u0026lt;- 1 to collection.length - 1) {} for (i \u0026lt;- 1 until collection.length) {} for (i \u0026lt;- 1 until 4 if i / 2 = 0) {} for (i \u0026lt;- 1 to 10 if i % 2 == 0; // notice the semicolumn j \u0026lt;- 1 to 10 if j % 2 != 0) print((i, j)) for {i \u0026lt;- 1 to 10 if i % 2 == 0 j \u0026lt;- 1 to 10 if j % 2 != 0} print((i, j)) 在圆括号内 Scala 不会自动推断分号。for 语句内也可以依赖前一个变量，甚至另外定义变量。\nval col = Seq(Seq(1, 2), Seq(3, 4), Seq(5, 6)) for {i \u0026lt;- col; j \u0026lt;- i} print(j) for {i \u0026lt;- col; x = i.length; j \u0026lt;- i} print(x) // 222222 if # // 两个分支的值是相同的类型，那么返回值是这个类型。对于数值类型，可能自动转换 { x: Int =\u0026gt; if (x == 0) 1 else 1 } // Int =\u0026gt; Int { x: Int =\u0026gt; if (x == 0) 1 else 1.2 } // Int =\u0026gt; Double { x: Int =\u0026gt; if (x == 0) 1 else \u0026#39;a\u0026#39; } // Int =\u0026gt; Int // 两个分支的值是不同的类型，那么返回值是它们的公共父类。 class Base() object Sub1 extends Base object Sub2 extends Base { x: Int =\u0026gt; if (x == 0) Sub1 else Sub2 } // Int =\u0026gt; Base // 对于两个没有太多亲属关系的类型，常常得到的是 Any Object(即AnyRef) AnyVal，取决于值的类型是引用还是值。 { x: Int =\u0026gt; if (x == 0) x else false } // Int =\u0026gt; AnyVal { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else Sub1 } // Int =\u0026gt; Object { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else 1 } // Int =\u0026gt; Any // 赋值语句的值是 Unit，属于数值类型，情况类似 { x: Int =\u0026gt; if (x == 0) x else { val a = 1 } } // Int =\u0026gt; AnyVal { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else { val a = 1 } } // Int =\u0026gt; Any // throw 语句的类型是 Nothing。与 Unit 不同，不会被算入到寻找公共父类的过程。 { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else throw new Exception } // Int =\u0026gt; String 在求取父类的过程中，需要了解到 Scala 类型的继承结构。虽然 Scala 把数值类型在语法上包装成了对象，但它们不继承于 Object / AnyRef 而是继承于 AnyVal，在编译后会变成原本的基本类型。Nothing 和 null 的情况类似。\ntry-catch / try-finally # throw 语句有返回值，但其返回值是 Nothing，不会被算到求取父类的过程中。catch 语句采用模式匹配语法。由于 try 和 finally 都有返回值，在 finally 中返回值会造成一些特殊的结果，最好的方式是绝对避免在 finally 语句中返回值，而只用来进行资源释放一类的工作。\ndef f(): Int = try return 1 finally return 2 // Int = 2 def f(): Int = try 1 finally 2 // Int = 1 // warning: a pure expression does nothing in statement position; you may be omitting necessary parentheses - Language// def f(): Int = try 1 finally 2 // ^ def f(): Int = try 1 finally return 2 // Int = 2 将 break 和 continue 转化为尾递归 # 在没有 break 和 continue 的情况下，一个简单的办法是使用布尔值和 if 来控制流，不过这不够函数式。但实际上，许多类似的问题都可以被转化为尾递归。例如，假设我们在一系列文件名中寻找第一个不以 - 开头的 Scala 源文件的下标：\nint i; for (int i = 0; i \u0026lt; args.length; i++) { if (args[i].startsWith(\u0026#34;-\u0026#34;)) continue; if (args[i].endWith(\u0026#34;.scala\u0026#34;)) { i = 1; break; } } 那么，如果使用尾递归，可以变成：\ndef searchFrom(i: Int): Int = { if (i \u0026gt;= args.length) -1 else if (args(i).startsWith(\u0026#34;-\u0026#34;)) searchFrom(i + 1) else if (args(i).endsWith(\u0026#34;.scala\u0026#34;)) i else searchFrom (i + 1) } val i = searchFrom(0) 在这里，continue 语句被替换成了一个递归的以 i+1 为参数的调用。而且，由于是尾递归，可以被编译器优化。\n当然，标准库里也提供了对 break 的扩展语法，但这种方式使用异常捕获来运行。这样做的目的是，即使 breakable 出现在另一个函数内，通过异常的抛出，也能实现跨函数的 break。\nimport scala.util.control.Breaks._ breakable { while(true) { if (something) break } } 作用域 # Scala 在作用域上与 Java 的主要区别是 Scala 允许在更小的作用域上使用相同的名字来覆盖外面的变量。\nfor (i \u0026lt;- 1 to 3) { for (i \u0026lt;- 4 to 6) print(i) } // 456456456 val a = 1; { val a = 2 print(a) } print(a) // 21 值得顺便一提的是，在 REPL 中之所以可以随意覆盖变量，是因为解释器对每一行输入都划分了一个新的作用域。也就是说：\nscala\u0026gt; val a = 1 scala\u0026gt; val a = 2 scala\u0026gt; print(a) 实际上等同于：\nval a = 1; { val a = 2; { print(a) } } "},{"id":2,"href":"/notes/intro-algo/1/","title":"1. Compexity, Divide","section":"Introduction to Algorithms","content":" 代价分析和复杂度 # 示例与概念 # 例子：插入排序 例子：归并排序 最坏情况分析 vs 平均情况分析 函数增长的渐进记号 # $O(n)$, $\\Theta(n)$, $\\Omega(n)$ 表示函数增长的上界、上下界、下界 $o(n)$, $\\omega(n)$ 表示不紧确的上下界 常用 $T(n)$ 表示所需的实际时间的函数 分析分治算法，以归并排序为例 # 归并排序最坏运行时间的递归式：\n$$ T(n)= \\begin{cases} \\Theta(1) \u0026amp; \\text{if } n=1 \\cr 2T(n/2) + \\Theta(n) \u0026amp; \\text{if } n\u0026gt;1 \\end{cases} $$\n除使用主定理外，还可以这样理解递归式的值：将递归过程看做一个二叉树。递归调用中的每一层的总代价均为 $cn$，其中 $c$ 为常数。而二叉树的层数应为 $\\log_2n+1$，故整个算法的代价期望为 $\\Theta(n\\log_2n)$。\n分治法 # 分治法求最大和的子数组 # 分解。将数组划分为两个子数组。此时，只存在三种子数组：\n全部位于中点左侧的子数组 全部位于中点右侧的子数组 跨越中点的子数组 解决。\n对于位于中点一侧的子数组，可以直接用递归解决。 对于跨越中点的子数组，将其分为左侧和右侧两部分。那么，左右两个数组都必定是所有以中点为两个边界之一的子数组的最大者。因此，从中点出发，向两侧扫描，并计算从中点到此元素的总和，找到最大。 合并。找到以上三个数组中总和最大者，即为结果。\n矩阵乘法的 Strassen 算法 # 朴素的矩阵乘法 # 按照定义进行的矩阵乘法:\n$$ C_{ij} = \\sum_{k=1}^{n} a_{ik} \\cdot b_{kj} $$\n复杂度为 $\\Theta(n^3)$，在此不做过多叙述。\n简单的分治算法 # 对于 $n$ 阶矩阵乘法 $C = A\\cdot B$，将三个矩阵都划分为四个部分，划分的四个部分仍遵守矩阵乘法法则。即：\n$$ \\newcommand{\\bmx}{\\begin{bmatrix}} \\newcommand{\\emx}{\\end{bmatrix}} \\bmx C_{11} \u0026amp; C_{12} \\cr C_{21} \u0026amp; C_{22} \\emx = \\bmx A_{11} \u0026amp; A_{12} \\cr A_{21} \u0026amp; A_{22} \\emx \\cdot \\bmx B_{11} \u0026amp; B_{12} \\cr B_{21} \u0026amp; B_{22} \\emx $$\n对于 $C$ 矩阵中的第一部分，有：\n$$ C_{11} = A_{11} \\cdot B_{11} + A_{12} \\cdot B_{21} $$\n其余三个等式略。\n对这个算法进行代价分析。首先，由于利用下标进行原地分解，我们只需要常数时间完成分解过程。随后，对于矩阵 $C$ 的每一个部分，需要进行两次递归的矩阵乘法和一次代价为 $\\Theta(n^2)$ 的矩阵加法。共需要进行 8 次矩阵乘法和 4 次矩阵加法。最后，当 “递归回升” 时，$n = 1$，只需要进行一次标量乘法。故整体代价为：\n$$ T(n) = 8T(2/n) + \\Theta(n^2) $$\n注意 $\\Theta$ 符号常数项可以被省略，但递归不能，常数系数 $8$ 仍然保留。\n应用主定理（Master Theorem）可知，$T(n) = \\Theta(n^3)$。\nStrassen 方法 # Strassen 方法的具体流程比较复杂。算法的效果是，将每一次调用的 8 次矩阵乘法替换为 7 次矩阵乘法和常数次的矩阵加法（复杂度为 $\\Theta(n^2)$）这也就意味着，递归树从每层 8 个分叉减少到了 7 个。随之，整个算法的复杂度降低到了 $\\Theta(n^{\\log_27}) \\approx \\Theta(n^{2.81})$。\n将矩阵分解为四个部分。代价为 $\\Theta(1)$。 创建 10 个 $n/2$ 阶的矩阵，每个矩阵保存步骤 1 中创建的两个矩阵的和或差。代价为 $\\Theta(n^2)$。 使用这 14 个矩阵，递归地计算 7 个矩阵积。即进行 7 次递归。 通过这些矩阵积计算出结果的 4 个子矩阵 $C_{11}, C_{12}, C_{21}, C_{22}$，代价为 $\\Theta(n^2)$。 具体过程在这里省略，也可以参照这篇文章。\n需要注意的是，这个算法实际上并不一定是代价更低的（二次常数因子较大）。而且，实际应用中遇到的矩阵大多是稀疏的，可以有更加实用的其他方法。因此，Strassen 方法在实际应用中并不多见。\n求解递归式 # 代入法 / 简单的数学归纳法 # 递归树法 # 主定理 # 令 $a\\geqslant 1$ 和 $b\u0026gt;1$ 是常数，$f(n)$ 是一个函数，$T(n)$ 是定义在非负整数上的递归式：\n$$ T(n) = aT(n/b) + f(n) $$\n那么：\n若对某个常数 $\\epsilon$，有 $f(n) = O(n^{\\log_ba-\\epsilon})$，即 $f(n)$ 的代价比前项小，那么 $T(n) = \\Theta(n^{\\log_ba})$。 若 $f(n) = O(n^{\\log_ba})$，那么 $T(n) = \\Theta(n^{\\log_ba}\\log_2n)$。 若对某个常数 $\\epsilon$，有 $f(n) = \\Omega(n^{\\log_ba+\\epsilon})$，即 $f(n)$ 的代价比前项大，那么 $T(n) = \\Theta(f(n))$。 简要地说，可以这样理解主定理：由递归带来的代价为 $\\Theta(n^{\\log_ba})$。\n如果式中的常量更大，则常量决定了代价增长的速度。 反之，就是递归决定了函数增长的速度。 若二者相等，则需要乘上一个系数 $\\log_2n$。 从递归树解法可以证明主定理。\n概率分析和随机算法 # 平均情况运行时间\n随机算法：算法中使用随机数生成器，输出不仅取决于输入\n指示器变量：对于事件 $A$，指示器随机变量 $\\mathrm{I}(A) = \\begin{cases} 1 \u0026amp; \\text{if $A$ happens}\\cr 0 \u0026amp; \\text{if $A$ doesn\u0026rsquo;t happen} \\end{cases}$。如，抛硬币的随机指示器期望为 $\\frac12$。\n雇用问题 # 问题描述 # 你需要一名办公助理。为此，你雇佣了一名 HR，每天为你带来一位面试者。这个过程需要为 HR 支付一笔面试费（算法中的代价）。每当你遇见一位比现在的办公助理更好的助理，就换掉现在身边的，这个过程需要为 HR 支付更多的费用（切换时需要更高的代价）。估算这个过程的费用是多少。\n该问题可以简化为：遍历一个序列，每当遇到比当前内存中的变量更高的值，就换成序列里的值。在两个变量之间做比较的代价较小，切换的代价较大。\n显然，在最坏情况下，每一次面试都需要进行切换。这时，代价的期望为 $O(c_hn)$。其中，$c_h$ 是雇佣一个新的助理的代价。\n使用随机指示器分析雇用问题 # 显然，决定面试者是否被雇佣的随机指示器的期望 $\\mathrm{E}[X_i] = 1/i$。这意味着每个面试者有 $1/i$ 的概率比之前的所有人都好，因此被雇佣。那么总的雇佣人数的期望为：\n$$ \\mathrm{E}[X] = \\mathrm{E} \\sum_{i=1}^n X_i = \\sum_{i=1}^n \\frac 1 i = \\ln n + O(1) $$\n因此，在面试者随机出现的情况下，代价的期望为 $O(c_h\\ln n)$。为了逼近这个值，可以在算法开始之前先进行随机化。\n随机算法 # 随机排列数组 # Permute-by-sorting. 具体做法是：对于输入序列，生成一个相同长度的随机数的数组。为了让数组的内容足够唯一，让随机数取值在 $(1, n^3)$ 范围内。然后，在排序这个数组的同时，排序序列。这个做法的代价与比较排序的代价相等，为 $\\Theta(n\\log_2n)$。\n原地随机化 # 从 1 到 n，将当前元素与序列后方的任意一个元素交换。\n其他随机相关问题 # 球与箱子问题 # 向 $b$ 个箱子中投球，落入每个箱子的概率均为 $1/b$。这个问题对于散列算法十分有用。我们称落入一个新的箱子中为一次命中。我们以已经有球的箱子的数量划分阶段。如，第 1 阶段表示尚未投球，第 2 阶段表示已投入一个。那么，对于已经有 $i-1$ 个命中的情况，即第 $i$ 阶段，得到一次命中的概率为 $(b-i+1)/b$。该阶段需要投球次数的期望为：\n$$ \\mathrm{E}[n_i] = \\frac b {b-i+1} $$\n整个过程共需要投球次数的期望为：\n$$ \\mathrm{E} [n] = \\mathrm{E} [\\sum_{i=1}^b \\frac b {b-i+1}] = \\mathrm{E} [\\sum_{i=1}^b \\frac 1 i] = b(\\ln b+O(1)) $$\n所以，我们大约要投 $b\\ln b$ 次才能保证所有箱子里都有球。这个问题也被称为礼券收集者问题。\n特征序列 # 抛一枚硬币 $n$ 次，最长的连续正面的序列有多长？这个值的期望为 $\\Theta(\\log_2n)$。可以从上界和下界两个方向来证明。\n布尔不等式：一组事件并集的概率不大于这些事件概率的和，无论这些事件是否独立。 在线雇用问题 # 雇用问题的一个变形：面试一批面试者，并最终雇用其中的一个。但是，对于每一个面试者，需要立刻决定是否雇用。在这个问题中，我们需要在面试者的质量和面试次数之间取得平衡。\n一个有用的策略是：先淘汰 $k$ 名面试者，找到他们当中最好的一个。随后，在接下来的面试中，一旦遇到比前 $k$ 个都好的，就立刻雇用。\n我们首先来尝试计算得到最好面试者的概率。设最好面试者在第 $i$ 个且被我们取到的事件为 $S_i$，考虑到前 $k$ 个面试者直接被我们排除掉，其中，当 $i\\leqslant k$ 时，不可能取到最佳面试者。于是，取得最好面试者的概率密度函数为：\n$$ \\Pr\\{S\\} = \\sum_{i=1}^n \\Pr\\{S_i\\} = \\sum_{i=k+1}^n \\Pr\\{S\\} $$\n接下来求 $\\Pr\\{S_i\\}$。当 $S_i$ 发生时，第一，应聘者应该在第 $i$ 个位置。第二，$k$ 到 $i-1$ 范围内的所有值全部小于前 $k$ 个中的最大者。这两个事件是独立的。对于第二个事件，将其转化为：从 $1$ 到 $i-1$ 范围内的最大值出现在前 $k$ 个中。这个概率为 $k/(i-1)$，故\n$$ \\Pr\\{S\\} = \\sum_{i=k+1}^n \\frac k {n(i-1)} = \\frac k n \\sum_{i=k}^{n-1} \\frac 1 i $$\n又，利用积分来近似约束这个和数，有：\n$$ \\int_k^n \\frac 1 x \\mathrm d x \\leqslant \\sum_{i=k}^{n-1} \\frac 1 i \\leqslant \\int_{k-1}^{i-1} \\frac 1 x \\mathrm d x $$\n求解可以得到下面的上下界：\n$$ \\frac k n (\\ln n - \\ln k) \\leqslant \\Pr\\{S\\} \\leqslant \\frac k n (\\ln(n-1) - \\ln(k-1)) $$\n以 $k$ 为未知数求导。当 $k = \\frac ne$ 时，下界取得最大值。\n因此，以此决定我们的 $k$ 取值，那么将以至少 $\\frac 1e$ 的概率雇用到最好的面试者。\n"},{"id":3,"href":"/notes/ddia/1/","title":"1. Data System and Data Model","section":"Designing Data-Intensive Applications","content":" 可扩展与可维护的与应用系统 # 数据密集型应用系统的典型例子包括：\n数据库 高速缓存 索引 流式处理 批处理 可靠性 Reliability # 对数据密集型系统可靠性典型的基本期望：\n执行期望的功能 可以容忍错误的使用方法 性能可以应对典型场景 可以防止未授权的访问 fault 指部分的功能不符合预期，反之即为 fault-tolerence 或 resilient；failure 指整个系统的完全不可用。可靠性意味着，我们希望部分的 fault 不会造成整个系统的 failure。\n系统的可靠性需要涵盖的方面：\n硬件故障 软件错误 人为失误 以最小出错的方式来设计系统界面 将容易出错的地方分离，如上线的测试环境 充分测试 快速回滚 监控子系统 管理流程 可扩展性 Scalability # 在评价可扩展性之前，需要先能够描述系统的负载和性能。\n描述负载的指标通常包括 QPS、写入数据比例、同时活动用户数、缓存命中率、扇出数等，这些指标可以是均值、峰值、分位数。\n描述性能的指标包括延迟、响应时间、吞吐量等。其中，响应时间是端到端的全链路延迟，而 latency 一般只指用在处理请求上的时间。同样地，经常观测其中位数或高分位数。\n数据密集型系统的挑战通常来自于负载增加。需要考虑的问题：\n维持性能，负载增加，需要增加多少资源 维持资源，负载增加，性能会如何变化 通常使用的解决方案可以大致分为两类：\nScale up 垂直扩展 Scale out 水平扩展 可维护性 Maintainability # 可维护性通常包括：\n可运维性 简单性，即系统本身理解的复杂程度 可演化性，即系统迭代、改变设计的难度 运维团队的职责：\n监视系统健康状况，进行快速恢复 追踪异常的原因（如系统故障或性能下降） 保持更新（如安全补丁） 了解不同系统之间的相互有影响，避免破坏性操作 预测可能的问题并解决（如扩容规划） 建立部署和配置的良好实践和 util 执行复杂的运维任务，如集群迁移 修改配置时维护系统正常 制定规范操作流程，保持生产环境稳定 保持相关知识的传承 数据模型与查询语言 # 关系模型与文档模型 # NoSQL # 从发展历史来看，最早的数据库是层次模型，所有数据都在一棵树上，可以良好地支持一对多关系，但不能支持多对多，且不支持 JOIN。\n网络模型是层次模式的一个改进，相当于一个全部用指针链接的图，可以良好地支持多对多。然而，需要手动编写查询代码（跟随指针，性能较好但比较复杂）。\n关系模型只使用二维表作为数据结构，提供索引和 JOIN ，执行由查询优化器自动生成。\nNoSQL 使用文档模型或图模型。 文档数据库类似层次模型，但可以通过文档引用一定程度上支持 JOIN 。\n文档数据库和图数据库看起来似乎和层次模型、网络模型类似，但它们提供的使用方式并不相同。\n相对于 RDBMS，NoSQL 通常具备：\n更好的可扩展性，主要是超大数据集或超高吞吐 偏向免费开源而非商业数据库 关系模型不适应的查询操作 摆脱关系模型的一些限制 对象-关系失配 # RDBMS 使用的是行模型，应用程序通常使用的是面向对象模型，二者存在的不匹配需要用 ORM 来解决。如 LinkedIn 简历的文档结构中，就存在大量一对多关系。可能的方案包括：\n外键连接 复杂数据结构字段 JSON 或 XML 编码后直接存在文本字段中，无法索引 多对一与多对多关系 # 无论文档还是关系，在引用外部对象时，都经常使用 id 引用而非直接存储对象。优势包括：\n保持枚举值一致 避免歧义 易于更新（改变 id 对应的值） 本地化支持（id 对应多值） 更好的搜索支持（便于利用搜索词之间的关系） 关系数据库与文档数据库现状 # 文档不是完全的无模式，而是读时模式，相对应地关系性数据库是写时模式，类似于动态类型和静态类型的区别。在进行数据结构升级后，关系型数据库需要修改数据库内容执行全库 UPDATE，而文档数据库只要在应用代码中判断即可。当数据的结构不可控地经常改变，或由许多不同的类型组成，文档更适合。\n文档模型把相关的数据放在一起（查询时将整个文档整个读取到内存中），因此当读取一个文档的大部分内容时，比关系性数据库有优势，能够利用数据的局部性。类似的概念包括 Cassandra 和 HBase 中的列族，Spanner 和 Oracle 也有把相关数据临近存储的功能。\n数据查询语言 # 声明式查询语言的重要优势是，可以通过查询优化器的改变无感地提升查询性能，且适合并发。CSS Selector、XPath 都可以看做声明式的数据查询语言。\nMongoDB 和 CouchDB 支持有限的 MR 只读查询。\n图数据模型 # 模型 1：属性图 # 可以看做有两个关系表，一个表示顶点，一个表示边。\n顶点表的字段包括 id、出入边 id 集合，加上顶点的属性。\n边表的字段包括 id、头尾顶点 id、描述两个顶点间关系的标签，加上边的其他属性。一般在头尾顶点上建立索引。\n语言 1：CYPHER # CYPHER 最早为 neo4j 创建，采用声明式的语法。\nCREATE (NAmerica:Location {name:\u0026#39;North America\u0026#39;, type:\u0026#39;continent\u0026#39;}), (USA:Location {name:\u0026#39;United States\u0026#39;, type:\u0026#39;country\u0026#39;}), (Idaho:Location {name:\u0026#39;Idaho\u0026#39;, type:\u0026#39;state\u0026#39;}), (Lucy:Person {name:\u0026#39;Lucy\u0026#39;}), (Idaho) -[:WITHIN]-\u0026gt; (USA) -[:WITHIN]-\u0026gt; (NAmerica) (Lucy) -[:BORN_IN]-\u0026gt; (Idaho) MATCH (person) -[:BORN_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (us:Location {name:\u0026#39;United States\u0026#39;}), (person) -[:LIVE_IN]-\u0026gt; () -[:WITHIN*0..]-\u0026gt; (eu:Location {name:\u0026#39;Europe\u0026#39;}) RETURN person.name SQL 中的图查询 # 如上，图数据可以用关系模型来表达。但是，图数据查询的主要问题是，因为递归的次数是不一定的，相应 JOIN 的次数也是不一定的 / 无限多的。在上面的 Cypher 查询中，WITHIN*0.. 表示可以以 0 次以上的 WITHIN 关系找到后面的节点。也就是说，查询的 Person 的 LIVE_IN 关系可以指向某个街道、城市、国家或欧洲，都能被查询到，只要他们之间有 WITHIN 关系。SQL 引入了 WITH RECURSIVE 语法来表达这个查询，但仍然不太优雅。\nWITH RECURSIVE in_europe(vertex_id) AS ( SELECT vertex_id FROM vertices WHERE properties-\u0026gt;\u0026gt;\u0026#39;name\u0026#39; = \u0026#39;Europe\u0026#39; UNION SELECT edges.tail_vertex FROM edges JOIN in_europe ON edges.head_vertex = in_europe.vertex_id WHERE edges.label = \u0026#39;within\u0026#39; ) 模型 2：三元存储 # 三元存储可以看做，将上面的属全部改成图的边，只保留边表。得到的关系表只有三个字段：主体，客体，谓词。上面的模型相当于：\n_:namerica a :Location _:namerica :name \u0026#34;North America\u0026#34; _:namerica :type \u0026#34;continent\u0026#34; _:usa a :Location _:usa :name \u0026#34;United States\u0026#34; _:usa :type \u0026#34;country\u0026#34; _:usa :witin :namerica 注意数据库中并不存储第一列的节点名字，这个名字只出现在定义语言中。\n语言 2：SPARQL # 上面的查询改成 SPARQL：\nSELECT ?personName WHERE { ?person :name ?personName. ?person :bornIn / :within* / :name \u0026#34;United States\u0026#34;. ?person :liveIn / :within* / :name \u0026#34;Europe\u0026#34; } 可以看出这里的 :within* 类似于 Cypher 中的 -[:WITHIN*0..]-\u0026gt; 。\n图模型与网络模型的区别 # 图模型并不同于早期的网络模型。主要区别包括：\n图模型没有模式，而网络模型有一个模式来规定哪些节点之间可以怎样连接 网络模型需要通过访问路径来找到节点，图模型可以直接用唯一 id 网络模型中的边是有序的，给存储修改带来困难 网络模型的查询语言是命令式的 语言 3：Datalog # Datalog 的定义部分采用三元组：\nname(namerica, \u0026#39;North America\u0026#39;) type(namerica, \u0026#39;continent\u0026#39;) name(usa, \u0026#39;United States\u0026#39;) type(usa, \u0026#39;country\u0026#39;) within(usa, namerica) 查询部分则是采用规则，Datalog 是 Prolog 的子集，采用逻辑规则编程的方式：\nwithin_recursive(Location, name) :- name(Location, name). within_recursive(Location, name) :- within(Location, Via), within_recursive(Via, Name). in_europe(Name, LivingIn) :- name(Person, Name), live_in(Person, LiveLoc), within_recursive(LiveLoc, LivingIn). ?- live_in_europe(Who, \u0026#39;Europe\u0026#39;) 此时 Who 的值就是所有住在欧洲的人名字。\n"},{"id":4,"href":"/notes/programming-scala/2/","title":"2. Functions","section":"Programming in Scala","content":" 函数和闭包 # 局部函数 # 通常，对于小的 \u0026ldquo;工具函数\u0026rdquo;，我们会使用私有函数来处理：\nobject Util { def a(): Unit = { val a = 1; // sth b(a) } private def b(i: Int): Unit { // do sth print(i) } } 通过私有函数，我们避免了 b 函数对整个 Util 对象的调用者的污染。不过，对于 Util 本身的编写者来说，如果 b 函数没有别的用处，仍然有些污染视线。因此，可以：\nobject Util { def a(): Unit = { val a = 1; // do sth def b(): Unit = { // do sth print(a) } b() } } 与上面的实现的区别是，因为作用域共享，我们无需为 b 设定参数，它可以直接访问外面的 a 变量。\n简化函数字面量 # (x: Int) =\u0026gt; x+1 // Int =\u0026gt; Int //因为前面的对象是 List[Int], x 的类型被推断为 Int (1 :: 2 :: Nil).filter(x =\u0026gt; x \u0026gt; 1) // List[Int] = List(2) // 常见的 PlaceHolder 语法 (1::2::Nil).filter(_ \u0026gt; 1) // List[Int] = List(2) val f = (_: Int) + (_: Int) // (Int, Int) =\u0026gt; Int 显然，在 _+_ 这种语法中，每个参数只能出现一次。\n部分应用函数 Partially Applied Function # 下划线也可以用来一次代替多个参数：\ndef sum(a: Int, b: Int) = a + b; val f = sum _ f(1, 2) 这时，实际上我们就定义了一个部分应用函数（虽然这里并不\u0026quot;部分\u0026quot;）。总之，部分应用函数意味着你并不提供所有的参数，而是提供几个，或者不提供参数（如上）。上面的 sum 函数也可以被这样使用：\nval plus3 = sum(3, _: Int) plus3(4) 进一步，如果我们在不提供任何参数的情况下再去掉下划线，就得到了：\nList(1, 2).foreach(println) 这就相当于我们直接把函数 println 传入。\n不过，这种省略下划线的形式要求 foreach 的参数本来就是一个函数类型。考虑这样的情况：\nList(List(1, 2, 3), List(4, 5, 6)).map(_.tail) List(List(1, 2, 3), List(4, 5, 6)).map(_.drop(1)) List(List(1, 2, 3), List(4, 5, 6)).map(_.drop) // error: missing argument list for method drop in class List // Unapplied methods are only converted to functions when a function type is expected. // You can make this conversion explicit by writing `drop _` or `drop(_)` instead of `drop`. 在\u0026quot;纯函数式语言\u0026quot;，如 ML 和 Haskell 中，或者 Python 这样追求简单和统一的语言中，第三种写法通常是有效的。不过，这样做的结果通常只是打印出一串 \u0026lt;function\u0026gt;，这是 drop 这个函数对象的类型。正因为这个原因，所以有：\nsum // error: missing argument list for method sum // Unapplied methods are only converted to functions when a function type is expected. // You can make this conversion explicit by writing `sum _` or `sum(_,_)` instead of `sum`. // sum // ^ sum _ // (Int, Int) =\u0026gt; Int 闭包 Closure # var c = 1; val f = (x: Int) =\u0026gt; x + c f(1) // 2 c = 2 f(1) // 3 与 Java 8 不同，Scala 允许函数访问外部的可变的变量，这也带来了一些复杂的问题，例如：\nval more = 3 // won\u0026#39;t be used def f(x: Int): Int =\u0026gt; Int = { val more = 1 (y: Int) =\u0026gt; y + x + more } val fun = f(2) fun(3) // 6 这个例子有点复杂，其核心内容是，函数 f 中的变量 x 和 more 在调用 fun 时都已经离开了作用域，因为 f 函数已经返回。好在，Scala 帮助我们在闭包里保留了正确的值。\n变长参数，命名参数，默认参数 # def print(args: String*) = args.foreach(println) print(\u0026#34;abc\u0026#34;, \u0026#34;bcd\u0026#34;) print(Array(\u0026#34;abc\u0026#34;, \u0026#34;bcd\u0026#34;)) // error: type mismatch; // found : Array[String] // required: String // print(Array(\u0026#34;abc\u0026#34;, \u0026#34;bcd\u0026#34;)) // ^ print(Array(\u0026#34;abc\u0026#34;, \u0026#34;bcd\u0026#34;): _*) def sum(a: Int, b: Int = 3, c: Int = 4) = a+b+c sum(1, c = 5) 尾递归 # Scala 会把尾递归优化为一个跳回函数开头的指令。例如，下面两种写法是等价的：\ndef find5(num: Int): Int = { var n = num while (num != 5) n++ } def find5(num: Int): Int = if num != 5 find5(n+1) else num 这样，尾递归在内存和CPU的代价上都和循环一样，却避免了 var 的出现。不过，这样做会让 debug 时的堆栈看起来不太一样。可以使用 -g:notailcalls 来关闭尾递归。\n限于 JVM 的能力，Scala 进行的尾递归比较有限，并没有对间接的尾递归进行优化。例如两个函数交替调用的情况：\ndef isEven(x: Int): Boolean = if (x == 0) true else isOdd(x - 1) def isOdd(x: Int): Boolean = if (x == 0) false else isEven(x - 1) 柯里化 Currying # def oldSum(x: Int, y: Int) = x + y def sum(x: Int)(y: Int) = x + y def curried(x: Int) = { (y: Int) =\u0026gt; x + y } val f = curried(1) val plusOne = sum(1) _ f(2) // 3 plusOne(2) // 3 在 curried 函数的定义中，为了视觉上更加清楚，加上了大括号。\n控制抽象 # 减少代码重复 # 上一章的内容，有些让人觉得 Scala 中传递函数略显麻烦，但接下来我们就能看到这样做的好处。在 Python 这类动态语言中，我们常常会将一个函数作为参数传入，以便复用代码。作为静态语言，Scala 不能随意把函数直接传入，而是通过上一章的那些隐式转换达到同样的效果。在 Python 中，我们可能会这样做：\ndef files_maching(query, method): for f in get_files(): if method(f.get_name, query): yield f 这里的 method 可以是字符串的 contain regex_match 之类的函数。总之，到了 Scala，我们通常会这样做：\ndef filesMatching(query: String, matcher: (String, String) =\u0026gt; Boolean) = { for (file \u0026lt;- files; if matcher(file.getName, query)) yield file } // caller def filesEnding(query: String) = filesMatching(query, _.endsWith(_)) // \u0026#39;\u0026#39;\u0026#39;_.endsWith(_)\u0026#39;\u0026#39;\u0026#39; is equivalent with // \u0026#39;\u0026#39;\u0026#39;(fileName: String, query: String) =\u0026gt; filename.endsWith(query)\u0026#39;\u0026#39;\u0026#39; 仔细观察一下的话，我们还可以进一步简化：\ndef filesMatching(matcher: String =\u0026gt; Boolean) = { for (file \u0026lt;- files; if matcher(file)) yield file } def filesEnding(query: String) = filesMatching(_.endsWith(query)) def filesContaining(query: String) = filesMathcing(_.contains(query)) 这类高阶函数在 Scala 类库中的应用，比如 List 类型的 exists 方法：\ndef containsOdd(nums: List[Int]) = nums.exists(_ % 2 == 1) def containsNeg(nums: List[Int]) = nums.exists(_ \u0026lt; 0) 柯里化 # 现在我们可以回头再看看柯里化了。柯里化常被用在与高阶函数相关的场景，使我们自己创建的函数看起来就像语言提供的特性一样。对上面的例子来说，filesMatching 函数还能够明显地看出我们编写的影子，filesEnding 函数使用起来就非常简单了。也就是说，柯里化通过将部分参数预先定义，让我们的 API 看起来更加简洁。对于 Haskell 这样每一个函数只允许一个参数的语言来说，几乎所有的函数都是柯里化的。\n编写控制结构 \u0026amp; 传名参数（by-name parameter） # 先来考虑一个简单的情况，下面这个结构连续执行一个操作两次：\ndef twice(op: Double =\u0026gt; Double, x: Double) = op(op(x)) twice(_ + 1, 5) // returns 7 理解了这个例子之后，我们再来考虑一个常见的实际场景： try-with-resources。我们很自然地会写出这样的代码：\ndef withPrintWriter(file: File, op: PrintWriter =\u0026gt; Unit) = { val writer = new PrintWriter(file) try { op(writer) } finally { writer.close() } } withPrintWriter(new File(\u0026#34;path/to/file\u0026#34;), writer =\u0026gt; writer.println(\u0026#34;something\u0026#34;)) 用我们目前为止得到的思维模式来考虑，将参数拆分成多个参数列表，代码和上面基本相同。单个参数的参数列表和多参数最大的区别是，我们就得以使用大括号来调用：\ndef withPrintWriter(file: File)(op: PrintWriter =\u0026gt; Unit) = { val writer = new PrintWriter(file) try { op(writer) } finally { writer.close() } } withPrintWriter(new File(\u0026#34;path/to/file\u0026#34;)) { writer =\u0026gt; writer.println(\u0026#34;something\u0026#34;) } 这样的控制结构给了我们一种完全不同的思路。\n不过，这样的结构和我们熟悉的 try-catch 块值类相比，多了一个参数 writer，看起来不太像原生的语言特性。如果我们并不需要在这里传入参数，能不能把参数部分也省略掉呢？考虑一个断言的实现：\ndef myAssert(predicate: () =\u0026gt; Boolean) = if (assertionEnabled \u0026amp;\u0026amp; !predicate()) throw new AssertionException myAssert(() =\u0026gt; a \u0026gt; b) myAssert(a \u0026gt; b) // won\u0026#39;t compile 在这里，显然第二种调用方式更优雅一些，但在这样的实现中没有办法实现。因此，我们使用传名参数：\ndef myAssert(predicate: =\u0026gt; Boolean) = if (assertionEnabled \u0026amp;\u0026amp; !predicate) throw new AssertionException myAssert(a \u0026gt; b) 当然，实际上还有另外一种实现方式：\ndef myAssert(predicate: Boolean) = ... 这两种实现方式的区别在于断言条件被计算的时刻。如果直接实现为 Boolean，那么无论断言是否开启（assersionEnabled），a \u0026gt; b 都会被执行。例如：\nvar assertionEnabled = false def myAssert(predicate: =\u0026gt; Boolean) = if (assertionEnabled \u0026amp;\u0026amp; predicate) throw new AssertionError def boolAssert(predicate: Boolean) = if (assertionEnabled \u0026amp;\u0026amp; predicate) throw new AssertionError def pred(): Boolean = throw new RuntimeException myAssert(pred()) // no problem boolAssert(pred()) // java.lang.RuntimeException 在断言关闭的情况下，如果断言条件抛出了异常，那么在使用 predicate: =\u0026gt; Boolean 作为参数的实现中，pred() 将不会被执行，异常不会抛出。\n组合与继承 # 在这一部分的例子中，我们将最终实现这样一个布局库：\nval column1 = elem(\u0026#34;hello\u0026#34;) above elem(\u0026#34;***\u0026#34;) val column2 = elem(\u0026#34;***\u0026#34;) above elem(\u0026#34;world\u0026#34;) column1 beside column2 // hello *** // *** world 抽象类 # abstract class Element { def contents: Array[String] def height: Int = contents.length def width: Int = if (height == 0) 0 else contents(0).length } 首先我们定义了一个元素的抽象类。这里的三个方法都没有参数括号：这样，调用方也必须不加括号才能访问。\nScala 世界的通常做法是，对于那些不改变对象本身的方法调用，不加括号，使其看起来更像是一个成员变量。这样做的目的是，使得这个方法看起来很像一个成员变量。我们在解释 Java 为什么要使用 getter 时举了许多次的例子：\nclass A { public int length; } // version 1.0 class A { public int length( ... ); } // version 2.0 这个问题在这里直接得到了解决，因为一个名为 length 的变量和一个名为 length 的方法对调用方来说没有任何区别：\nclass A { var length } // version 1.0 class A { def length: Int = ... } // version 2.0 从代码风格来讲，虽然所有的空括号都可以被省略，对于那些有副作用的方法，例如执行 IO、修改变量，总之涉及到 mutable 对象的方法，最好还是加上括号。典型的例子如 println()。\n将字段和方法统一对待的这种方式被称为 Scala 的统一访问原则（the Uniform Access Principle）。\n扩展一个类 # class ArrayElement(conts: Array[String]) extends Element { def contents: Array[String] = conts } 由于上面我们提到的统一访问原则，你甚至可以用字段来 override 一个方法，这两种结构位于同一个命名空间内。出于同样的原因，这样的代码在 Java 里可行，在 Scala 里则不行：\nclass A { int i = 0; public int i( return 0; ); } class A { val i = 0; def i = 1 } // won\u0026#39;t compile Java 有四个命名空间：字段，方法，类型和包。而 Scala 只有两个：值，包括字段，方法，包和单例对象，以及类型命名空间，包括类和特质（Trait）。\n作为一个有经验的程序员，当我们意识到上面的变量名 conts 是在试图表达和 contents 一样的内容而在躲避变量名冲突时，就应该考虑是不是该重构一下这个片段了。这时适合使用参数化字段。而且，参数化字段和普通的字段一样可以使用 override protected private 来修饰：\nclass Cat { val dangerous = false } class Tiger ( override val dangerous = true, private var age: Int ) extends Cat 需要调用父类的构造方法时：\nclass ArrayElement(val contents: Array[String]) extends Element class LineElement(s: String) extends ArrayElement(Array(s)) { override def width = s.length override def height = 1 } Scala 在重写方法时强制使用 override 修饰符。考虑这样的情况：你想在你的类库里添加一个新的方法，但用户在他们的代码里已经继承了这个类并重写了相同名字的方法，在 Java 中这种情况是相当麻烦的。这种我们不愿见到的重载被称为脆基类。不过如果你和下游调用者的代码都是使用 Scala 编写的，由于缺少 override 修饰符，下游代码编译时将会报错。虽然这样的解决方式仍然不怎么优雅，至少比 Java 的情况要好。\n最后，和 Java 一样，可以给方法或类打上 final 修饰符以防止继承。\n滥用继承是一个常见的问题，毕竟，脆基类的问题只会出现在继承而不会出现在组合中。在使用继承的时候，最好确认：首先，二者必须是一个 is-a 的关系。其次，考虑用户是否真的想将子类当做一个父类对象来使用。比如，上面的 LineElement 继承 ArrayElement 就比较奇怪。让它直接继承 Element 可能是更好的选择。\n完善类库：实现方法，定义工厂 # 简单起见，我们先假设参数和被调用的 Element 宽或高相同。那么，方法可以这样实现：\nabstract class Element { def above(that: Element): Element = new ArrayElement(this.contents ++ that.contents) def beside(that: Element): Element = new ArrayElement ( for ( (line1, line2) \u0026lt;- this.contents zip that.contents ) yield line1 + line2 ) override def toString = contents mkString \u0026#34;\\n\u0026#34; } 实现了方法之后，我们希望使用工厂来对调用者暴露接口，而不是直接把继承层级告诉用户。毕竟，上面而我们已经修改了一次 LineElement 的层级了。同时，我们的 above 这些方法也可以转而调用工厂方法：\nobject Element { def elem(contents: Array[String]): Element = new ArrayElement(contents) def elem(line: String): Element = new LineElement(line) } import Elements.elem abstract class Element { def above(that: Element): Element = elem(this.contents ++ that.contents) def beside(that: Element): Element = elem( for ((line1, line2) \u0026lt;- this.contents zip that.contents ) yield line1 + line2 ) } 此外，我们还发现 ArrayElement 本身也完全不需要暴露给用户了。所以，可以将它们全部转移成私有的：\nobject Element { private class ArrayElement( val contents: Array[String]) extends Element private class LineElement { ... } def elem(contents: Array[String]): Element = new ArrayElement(contents) def elem(line: String): Element = new LineElement(line) } "},{"id":5,"href":"/notes/core-java-impatient/2/","title":"2. Interface, Lambda","section":"Core Java for Impatients","content":" 接口 # 在 cast 之前，先使用 instanceof 进行检查。 继承接口 # public interface Closable { void close(); } public interface Channel extends Closable { boolean isOpen(); } 那么，实现 Channel 接口的类必须实现两个方法。\n静态方法和默认方法 # 接口中的静态方法是后来才加入到 Java 中的。在这种语法出现之前，这些方法被放到伴随类中。例如，Collection 接口和 Collections 类。接口中的静态方法，意味着这个方法是属于这个接口类型的。因此，必须提供实现。调用时，使用 Interface.method。\n可以为方法提供默认的实现，在方法前加上 default 关键字即可。除了提供默认实现之外，这种方式还提供了不同版本间接口变化的可能性。例如，Collection 接口现在新加入了一个有默认方法体的 Stream 方法。如果引入的 class 文件是以前编译的，即其不包含 Stream 方法，将抛出 AbstractMethodError 异常。\n在实现多个接口时，可能存在默认方法的冲突。（如果两个接口都不提供默认方法体，因为我们只会实现一个，所以不存在冲突。如果有一个默认方法体，即使另一个方法不提供默认的，也属于冲突情况，因为非默认的方法可能被默认的覆盖。）这时，编译器会报错。我们需要自己解决冲突的情况。\npublic class Employee implements Person, Identified { public int getId() { // 调用父类型的方法 return Identified.super.getId(); } } 常用的接口 # Comparable # public interface Comparable\u0026lt;T\u0026gt; { int compareTo(T other); } 在实现这个方法时，如果打算返回两个值的差，最好使用 Integer.compare() 和 Double.compare() 方法，这样可以避免溢出的情况，并正确地处理 NaN 和无穷。对于实现了这个接口的类，可以使用 Arrays.sort 方法来进行排序。\nComparator # class LengthComparator implements Comparator\u0026lt;String\u0026gt; { public int compare(String o1, String o2) { return o1.length() - o2.length(); } } Comparator\u0026lt;String\u0026gt; comp = new LengthComparator(); Arrays.sort(arr, comp); 更方便的使用这个接口的方法是 lambda 表达式。\nRunnable # public interface Runnable { public void run(); } public interface Callable\u0026lt;T\u0026gt; { public T call() throws Exception; } class Task implements Runnable { public void run() {} } new Thread(new Task).start(); 任务将在新的线程里运行。这个接口同样适合使用 lambda 表达式。Callable\u0026lt;T\u0026gt; 的作用类似，有返回值，并可以抛出异常。\nlambda 表达式和方法引用 # lambda 表达式的核心目的是延迟执行。例如，将比较操作延迟到排序算法中，将操作延迟到遍历过程中，将一个调用延迟到另一个线程中，绑定回调函数等。总之，lambda 表达式的书写者不再需要考虑其调用。\nlambda 表达式 # lambda 表达式被用来代替函数式接口，即只有一个抽象方法的接口。在 Java 中，lambda 表达式是这样实现的：表达式 (Class param) -\u0026gt; {...} 相当于一个继承了函数式接口的对象的实例。例如，可以使用这样的用法：\nFunction\u0026lt;Integer, Integer\u0026gt; times2 = e -\u0026gt; e * 2; 如果要编写自己的函数式接口，建议使用注解 @FunctionalInterface 提示编译器进行检查。\n通用的函数式接口 # 接口 参数 返回值 方法名 其他方法 Runnable none void run Supplier\u0026lt;T\u0026gt; none T get Consumer\u0026lt;T\u0026gt; T void accept andThen BiConsumer\u0026lt;T, U\u0026gt; T, U void accept andThen Function\u0026lt;T, R\u0026gt; T R apply compose andThen identity BiFunction\u0026lt;T, U, R\u0026gt; T, U R apply andThen UnaryOperator\u0026lt;T\u0026gt; T T apply compose andThen identity BinaryOperator\u0026lt;T\u0026gt; T, T T apply andThen maxBy minBy Predicate\u0026lt;T\u0026gt; T boolean test and or negate isEqual BiPredicate\u0026lt;T\u0026gt; T, U boolean test and or negate 这里的\u0026quot;其他方法\u0026quot;，是一类用来组装函数的非抽象方法。形如：\npublic interface Predicate\u0026lt;T\u0026gt; { boolean test(T t); // 这个方法接收两个Predicate类型，返回一个新的，其效果相当于二者取布尔和。 default Predicate\u0026lt;T\u0026gt; and( Predicate\u0026lt;? super T\u0026gt; another) { ... } } 在这里，compose() 和 andThen() 都接收另一个参数为 ? super T 的函数，将二者以不同的顺序组合起来。and or negate 的形式与之类似。\n还有些方法是静态的。identity 返回一个函数，这个函数将其输入原封不动地返回出去。因此其返回值为 Function\u0026lt;T, T\u0026gt; 类型。maxBy minBy 接受一个 Comparator\u0026lt;? super T\u0026gt; 参数，返回一个用于比较的函数。isEqual 返回一个 null 安全的判断相等的函数。\n此外，还有一些用于未包装类型的接口，如 IntPredicate DoubleConsumer ToIntFunction\u0026lt;T\u0026gt; 等。\nlambda 表达式和变量作用域 # lambda 表达式和嵌套代码块有着相同的作用域。因此，这样的代码是不合法的：\nint first = 0; Comparator\u0026lt;String\u0026gt; comp = (first, second) -\u0026gt; ...; // DO NOT compile 由于作用域的共享，在 lambda 表达式中调用 this 得到的结果将和直接在方法中调用一样。\npublic class Application { public void doSth() { // Calls Application.this.toString(), NOT!! runner.toString() Runnable runner = () -\u0026gt; {this.toString()} } } 闭包 # 考虑一个 lambda 表达式：\npublic static void repeatMessage(String text, int count) { new Thread(() -\u0026gt; { for (int i = 0; i \u0026lt; 1000; i++) { System.out.println(text); } }).start() } 在这里，lambda 表达式除了参数和本身的代码块之外，还使用了外部的其他变量，即这里的 text。这种变量叫做自由变量，这个 lambda 表达式是一个闭包。\n由于 lambda 表达式是延迟执行的。例如，这里的 println 操作在另一个线程里结束之前，这个方法所在的对象很可能已经不被引用而 GC 掉了。所以，lambda 表达式需要在运行之前捕获这些变量，保存在这个实现接口（在这里是 Runnable）的匿名类对象里。如果这些变量是可变的，就无法进行正确的捕获。例如：\nfor (int i = 0; i \u0026lt; 1000; i++) { // Error, DO NOT compile new Thread(() -\u0026gt; System.out.println(i)).start(); } 同样的规则也适用于局部内部类。总之，被引用的自由变量必须可以被声明为 final，或者叫做有效 final（effectively final）。\n不过，在增强的 for 循环中，变量可以在 lambda 表达式中使用。在传统的 for 循环中，变量 i 的作用域是整个 for 循环。而在以下的代码中，变量 s 的作用域只有其所在的唯一一次迭代：\n//start Variable Scope of variable \u0026#34;i\u0026#34; for (String s : args) { //start variable scope of variable \u0026#34;s\u0026#34; new Thread(() -\u0026gt; System.out.println(s)); //end \u0026#34;s\u0026#34; scope } //end \u0026#34;i\u0026#34; scope 正由于捕获的过程，lambda 表达式不能改变被捕获的自由变量。当然，这样也可以避免一些多线程冲突的问题。\n如果真的需要使用可变的对象，可以通过数组来规避：\nint[] counter = new int[]; bunnton.setOnAction(event -\u0026gt; counter[0]++); 这样，counter 指向的数组没有变，而其中存储的数据变化了。当然，这样的代码不是线程安全的。\n方法引用 # 当 lambda 表达式的内容已经在其他方法中被实现，就可以使用方法引用来代替。\n形式 # 方法引用有三种形式：\nClass::instanceMethod\n如，String::compareTo = (x, y) -\u0026gt; x.compareTo(y)，是在对象 x 上调用，并将其他参数，在这里是 y，作为方法的参数。\nClass::staticMethod\n如，Object::isNull = x -\u0026gt; Objects.isNull(x)，是在类 Objects 上调用，将表达式的所有参数作为方法的参数。\nInstance::instanceMethod\n如，System.out::print = x -\u0026gt; System.out.println(x)，是在给出的对象 out 上调用方法，并把参数传入。\n构造函数的引用 # class Employee { public Employee(String name) {} } Stream\u0026lt;Employee\u0026gt; stream = names.map(Employee::new); 使用数组类型的构造函数引用，可以避免对数组类型的 cast（由于 Java 中没有泛型数组）：\nObject[] employees = stream.toArray(); // Use constructor reference instead: Employee[] buttons = stream.toArray(Employee::new); 高阶函数 # 此前我们已经见过高阶函数，即通用函数式接口里的那些 default 方法。高阶函数可以用来产生新的函数或修改一个函数：\n// Return a function public static Comparator\u0026lt;String\u0026gt; compareInDirection(int direction) { return (x, y) -\u0026gt; direction * x.compareTo(y); } // Modify a function public static Comparator\u0026lt;String\u0026gt; reverse(Comparator\u0026lt;String\u0026gt; comp) { return (x, y) -\u0026gt; comp.compare(x, y); } Comparator 方法 # // Sort by name Arrays.sort(people, Comaprator.comparing(Person::getName)); // Sort by name, with specified comparator Arrays.sort(people, Comparator.comparing(Person::getName, (s, t) -\u0026gt; s.length() - t.length()); // Sort by last name, then first name Arrays.sort(people, Comparator .comparing(Person::getLastName) .thenComparing(Person::getFirstName)); // For basic types: Arrays.sort(people, Comparator.comparingInt(Person::getId)) 在这里被传入的方法引用可以称为 key 提取器，这意味着这个方法把对象中的 key 提取出来以供比较。如果 key 提取器可能返回 null，则可以使用两个装饰器方法 nullFirst nullLast：\n// 将comparing方法静态导入进来 import java.util.Comparator.* Arrays.sort(people, comparing( Person::getMiddleName, nullsFirst(natualOrder()))) // or reverseOrder() "},{"id":6,"href":"/notes/intro-algo/2/","title":"2. Sorting, Order Statistic","section":"Introduction to Algorithms","content":" 排序算法 # 原地排序 (in place) ：仅需要常数的额外存储空间\n堆排序：$O(n\\log_2n)$ 的原地排序算法\n快速排序：期望为 $\\Theta(n\\log_2n)$，最坏情况为 $\\Theta(n^2)$，实际应用中通常比堆排序快。同时，其常数系数很小，是排序大数组时的常用算法。\n比较排序：通过对元素进行比较来决定，快排、归并、堆排序都是比较排序。比较排序的代价下界为 $\\Omega(n\\log_2n)$。\n线性时间排序：计数排序、基数排序、桶排序，在一定条件下，可以取得线性时间代价。\n算法 最坏情况代价 代价期望 插入排序 $\\Theta(n^2)$ $\\Theta(n^2)$ 归并排序 $\\Theta(n\\log_2n)$ $\\Theta(n\\log_2n)$ 堆排序 $O(n\\log_2n)$ 快速排序 $\\Theta(n^2)$ $\\Theta(n\\log_2n)$ 计数排序 $\\Theta(k+n)$ $\\Theta(k+n)$ 基数排序 $\\Theta(d(k+n))$ $\\Theta(d(k+n))$ 桶排序 $\\Theta(n^2)$ $\\Theta(n)$ 堆排序 # 复杂度为 $O(n\\log_2n)$，常数个额外空间（原地排序）。\n最大堆结构 # 最大堆是一个完全二叉树，且对于每一个结点，其子结点都比这个结点的值更小。通常使用数组来存储。这样，如果数组的下标从 1 开始，那么任意一个结点 $n$ 的左子结点为 $2n$，右子结点为 $2n+1$，父结点为 $\\lfloor i/2\\rfloor$（向下取整）。这样，我们可以轻松地利用移位指令来取得结点下标，获得比较高的性能。显然，二叉树的高度为 $\\Theta(\\log_2n)$。于是，我们能得到一些堆上的基本操作的复杂度：\n最大堆化（Max-heapify），复杂度为 $\\Theta(\\log_2n)$。\n构建最大堆（Build-max-heap），线性时间复杂度，将无序数据转化为最大堆。\n堆排序（Heapsort），复杂度为 $O(n\\log_2n)$，对一个数组进行原地排序。\n插入（Max-Heap-Insert）、删除最大（Heap-Extract-Max）、增长 key（Heap-Increase-Key）、取得最大（Heap-Maximum），时间复杂度为 $O(\\log_2n)$，功能是利用堆实现一个优先队列。\n维护堆的性质：最大堆化 # 最大堆化（Max-heapify）的输入是一个数组 $A$ 和一个下标 $i$。其中，$i$ 结点的左右子树都是已经构建完成的最大堆，而 $A[i]$ 不一定是。然后，我们通过 “逐级下降” 过程，将 $A[i]$ 插入到适当的位置，使得以 $A[i]$ 为根结点的子树是一个最大堆。\n算法如下：在程序的每一步，判断当前结点和其左右子结点的大小关系。如果父结点最大，显然最大堆已经完成。否则，将父结点与更大的一个子结点交换，并下降到交换的位置，重复这个流程。通过这样的不断与子结点交换的过程，目标将下降到适当的位置，使整个二叉树成为一个最大堆。\n这个过程中的代价包括：判断和调整三个结点的过程，代价为 $\\Theta(1)$，加上递归调用的代价。每个子树的大小最多为 $\\frac{2n}3$（当树的最底层半满时），那么我们可以用递归式 $T(n) \\leqslant T(\\frac{2n}3 + \\Theta(1))$ 来描述这个递归调用。使用主定理解得 $T(n) = O(\\log_2n)$。也就是说，对于一个树高为 $h$ 的结点，最大堆化的时间复杂度为 $O(h)$。\n建堆 # for i = A.length/2 downto 1 MAX-HEAPIFY(A,i) 将数组当成一个二叉树，从第一个非叶子结点开始，直到堆顶，不断进行最大堆化。在这个过程中，二叉树底部的结点，实行最大堆化的代价越小，而数量越多。结合上一部分得到的最大堆化的时间复杂度为 $O(h)$ 的结论，这个过程的代价为：\n$$ \\sum_{h=0}^{\\lfloor \\log_2n \\rfloor} \\lceil \\frac n {2^{k+1}} \\rceil O(h) = O(n) \\sum_{h=0}^{\\lfloor \\log_2n \\rfloor} \\frac h {2^h} $$\n又有公式：\n$$ \\sum_{k=0}^{\\infty} x^k = \\frac x {(1-x)^2} $$\n令 $x = \\frac12$ 可以得到：\n$$ \\sum_{k=0}^{\\infty} \\frac h {2^h} = \\frac {\\frac12} {(1-\\frac12)^2} = 2 $$\n于是：\n$$ O(n)\\sum_{h=0}^{\\lfloor \\log_2n \\rfloor} \\frac h{2^h} = O(n)\\sum_{h=0}^{\\infty} \\frac h {2^h} = O(n) $$\n因此，我们可以在线性时间内构建一个最大堆。\n堆排序 # 最大堆只能保证父子结点之间的大小关系，而不能保证同一层之间的关系，因此，排序尚未完成。为此，我们可以不断取出最大堆的根结点，并放置在堆后面。这样，所有元素将以升序摆放好。\n首先，将最大堆的根结点与最后一个结点互换。随后，对这个被换上来的结点执行最大堆化。在它被放置在正确的位置之后，整个序列中最大的值就被放置在了适当的位置，即数组的末尾，而其余值则仍然组成一个大小为 $n-1$ 的最大堆（这也是从最大堆中取出最大值的方法）。不断重复这个过程，缩小最大堆，就能够将所有元素摆放在适当的位置。\n优先队列 # 决定优先队列的顺序的值称为关键字（key）。一个最大优先队列可以：\n插入元素 取最大元素 删除最大元素 增大 key：将序列中一个元素的 key 修改为一个更大的值 最大优先队列可用于批处理式计算机系统的作业调度，用来寻找当前所有任务中优先级最高的开始执行。\n显然，取得最大元素的时间复杂度为 $\\Theta(1)$，删除最大元素的时间复杂度为 $O(\\log_2n)$（相当于堆排序的第一步）。增大某个元素的 key，可以在修改之后将其不断与其父结点比较并交换，直到这个结点小于其父结点，意味着它来到了适合的位置上。这个过程的复杂度为 $O(\\log_2n)$。在队列中增加结点，可以先在末尾加入一个 key 为 $-\\infty$ 的结点（此时二叉树仍然是一个最大堆），再将其 key 增加为需要的值，并应用上面的算法。显然，复杂度为 $O(\\log_2n)$。\n快速排序 # 算法描述 # 分解：将数组划分为两个子数组和一个分界，并使所有左侧数组的值小于右侧数组。 解决：对两个子数组递归调用快速排序 合并：数组已经排序完成。 当数组长度为 1 时，可以直接认为这个子数组排序已完成。\n快速排序划分算法的一种实现是：以数组末尾为主元（pivot），在内存中维护两个指针，分别代表两个数组的分界和右侧数组的末尾，初始化到数组头部。向右扫描，如果当前元素大于 pivot，说明其应该被放置在右侧数组，于是增长右侧指针的值。如果小于，则将其与右侧数组的第一个元素交换，并增长左侧指针的值。如下：\nstep i j 1 2 8 7 1 4 -1 0 2 2 8 7 1 4 0 1 3 2 8 7 1 4 0 2 4 2 8 7 1 4 0 3 5 2 1 7 8 4 1 4 6 2 1 4 8 7 - - 在第 5 步，由于 j 遇到了应该被放在左侧数组的元素，所以进行一次交换。最后，将 pivot 交换到合适位置。\n数组划分的另一种实现，使用分别向左和向右的两个指针，交替进行扫描。取第一个元素为 pivot，开始从右向左扫描。当遇到小于 pivot，应该放在左侧的元素时，将其放在之前取 pivot 留下的空位，即左指针指向的位置。相应的，此时右侧指针指向的位置即为空位。并开始从左向右扫描，交替进行，直到两个指针重合（此时应同时指向空位），将 pivot 放置在空位上。过程如下：\nstep l r 1 2 8 7 1 4 0 4 2 - 8 7 1 4 0 4 3 - 8 7 1 4 0 3 4 1 8 7 - 4 1 3 5 1 - 7 8 4 1 2 6 1 2 7 8 4 1 1 这种情况下，两个指针时刻有一个执行扫描任务，一个指向空位，预备交换。\n性能分析 # 最坏情况下，对于每次划分，都把数组划分为一个单独的元素与一个长度为 $n-1$ 的数组。或者说，分解成一个长度为 0 的数组和另一个数组。此时，可以用递归式：\n$$ T(n) = T(n-1) + T(0) + \\Theta(n) = \\Theta(n^2) $$\n来表示算法的代价。\n最好情况下，所有的划分都是平均的。此时，代价的递归式为：\n$$ T(n) = 2T(\\frac n 2) + \\Theta(n) = \\Theta(n\\log_2n) $$\n可以证明，任何一种常数比例的划分，即使相当不均衡，都会得到 $\\Theta(n\\log_2n)$ 的结果。\n除此之外，我们还可以在划分中取随机的 pivot，并经过一次交换后再开始正常的流程。通过这种方式，可以在算法中引入随机性，避免最差情况的出现。\n快速排序的概率分析 # 快速排序的运行时间取决于划分过程中进行比较的次数。定义一个随机指示器变量 $X_{ij}=\\mathrm I\\{\\text{compare $z_i$ with $z_j$}\\}$，那么总的比较次数的期望为：\n$$ \\begin{aligned} \\mathrm E(X) \u0026amp; = \\mathrm E[ \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} X_{ij} ] \\cr \u0026amp; = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\mathrm E[X_{ij}] \\cr \u0026amp; = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\Pr\\{\\text{compare $z_i$ with $z_j$}\\} \\end{aligned} $$\n考虑划分过程中的情况：所有元素都要与主元比较，且划分得到的两个数组之间不会发生比较。于是：\n$$ \\begin{aligned} \u0026amp; \\Pr\\{\\text{compare $z_i$ with $z_j$}\\} \\cr = \u0026amp; \\Pr\\{\\text{$z_i$ is first pivot of set $Z_{ij}$}\\} + \\Pr\\{\\text{$z_j$ if first pivot of set $Z_{ij}$}\\} \\cr = \u0026amp; \\frac 2 {j-i+1} \\end{aligned} $$\n代入得到：\n$$ \\begin{aligned} \\mathrm E(X) \u0026amp; = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac 2 {j-i+1} \\cr \u0026amp; = \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac2{k+1} \\cr \u0026amp; \u0026lt; \\sum_{i=1}^{n-1} \\sum_{j=i+1}^{n} \\frac2k \\cr \u0026amp; = O(n\\log_2n) \\end{aligned} $$\n快速排序的主要课题是防止最坏情况攻击。一种常用的方法是随机取三个数，并以其中位数为 pivot。\n非比较排序 # 比较排序的下界 # 之前介绍的归并排序、堆排序、快速排序都属于比较排序。比较排序在最坏情况下都为 $\\Omega(n\\log_2n)$ 的。因此，复杂度为 $\\Theta(n\\log_2n)$ 的归并排序和堆排序是渐进最优的，其他所有算法都只是在常数项上比它们更优。\n一个比较排序可以抽象为一颗决策树。如，对于序列 $a,b$ 的决策树为，根节点判断两个元素的大小，左叶子节点是 $a\\leqslant b$ 的结果，右叶子节点是 $a\u0026gt;b$ 的结果。相应地，对于长度为 3 的序列，树将有三层，六个叶子节点。决策树的最大层数既是我们在得到结果之前必须做的比较的次数。\n对于一个长度为 $n$ 的待排序序列，让决策树有 $l$ 个可达节点，即这些个可能的排列，树的高度为 $h$。那么将有：\n$$ n! \\leqslant l \\leqslant 2^h $$\n两边取对数得到：\n$$ h \\geqslant \\log_2(n!) = \\Omega(n\\log_2n) $$\n计数排序 # 计数排序的基本思想是：对每一个元素，数出有多少个元素小于它，则这个元素应该被放在相应数量后面的下一个位置。假设 $n$ 个输入元素均为 $0$ 到 $k$ 区间内的一个整数，那么当 $k = O(n)$ 时，排序的运行时间为 $\\Theta(n)$。\n计数排序的实现是这样的：初始化一个长度为 $k$ 的数组 $C$ 用来计数，扫描原数组，对每个元素，如果值为 $i$，就在 $C[i]$ 位置加一。扫描完成之后，就得到了每个值的元素数量。随后，在 $C$ 数组上进行一次 $C[i] = C[i] + C[i-1]$ 的扫描，这时 $C$ 数组中储存的就是小于该下标值的元素的数量。最后，扫描原数组，根据 $C$ 数组中的数据，将这些元素放置在另一个数组中合适的位置。\n显然，整个算法经历了一次 $\\Theta(n)$ 的扫描，一次 $\\Theta(k)$ 的累加和一次 $\\Theta(n)$ 的复制，整个算法的复杂度为 $\\Theta(n+k)$。当 $k = O(n)$ 时，复杂度就是 $\\Theta(n)$。\n在复制过程中，如果存在两个元素的值相等，就放到下一个位置，之前的技过程已经为它留计数了位置。因此，计数排序是稳定的。下面我们将看到，这个性质在其作为基数排序的一部分时，相当有用。\n基数排序 # 算法描述 # 基数排序（radix sort）来源于卡片排序机。卡片计算机一次只能判断一列孔，即只能判断一位数字。直觉上，我们倾向于从最高位开始，递归向下地进行排序。但是，这种方法在卡片计算机上使用时将会需要临时保存好另外 9 种数字的卡片，在数字位数较多时使用的容器和标签就会相当可观。\n因此，实际上我们是从最低位开始进行排序的。在排序好最低位之后，可以直接把所有卡片按顺序叠在一起，开始进行第二位的排序。这样就不需要把卡片另行保存，直接排序完成就能使用。\n所以，基数排序实际上是从低到高按位进行其他排序方法的算法。由于低位在之前的循环中已经排好，使用的其他排序方法必须是稳定的，否则低位的顺序就会被打乱。除了按十进制的位之外，还可以对日期按日 - 月 - 年进行排序等很多类似方式。\n代价分析 # 给定 $n$ 个 $d$ 位数，其中每个位有 $k$ 个可能的取值，由于 $k$ 有限且不大，计数排序十分合适，复杂度为 $\\Theta(n+k)$。显然，总的复杂度为 $\\Theta(d(n+k))$。\n接下来讨论 “划分” 位的策略。如果把子排序中使用的位数从 1 位扩展到 $r$ 位，而 整个数字一共有 $b$ 位。那么，上式的 $d$ 就是 $b/r$，$k$ 就是 $2^r-1$。例如，对于一个 32 位的整数，我们将其分解为 4 个字节，于是 $b = 32$，$r = 8$，对每个字节进行计数排序。在相似的条件下，排序的复杂度就变成了 $\\Theta(\\frac b r (n+2^r))$。\n如果 $b \u0026lt; \\lfloor \\log_2n \\rfloor$，那么对于任何 $r \\leqslant b$，都有 $(n+2^r) = \\Theta(n)$，算法的复杂度取决于 $b/r$。显然，当 $r = b$，复杂度取得最小值 $\\Theta(n)$。\n如果 $b \\geqslant \\lfloor \\log_2n \\rfloor$，对 $r$ 的值分情况来讨论。如果 $r = \\lfloor \\log_2n \\rfloor$，可以得到最优时间代价 $\\Theta(bn/\\log_2n)$。如果 $r\u0026lt;\\lfloor\\log_2n\\rfloor$，值越小，$b/r$ 项的值就越大，$(n+2^r)$ 仍然为 $\\Theta(n)$。反之，$r$ 越大，分子中的 $2^r$ 项比分母中的 $r$ 项增长得更快。因此，时间代价为 $\\Omega(bn/\\log_2n)$。\n与比较排序的对比 # 虽然基数排序的复杂度比较小，其中的常数项相对于比较排序来说哪一个更大，要取决于机器具体的实现和数据的特点。例如，通常来说快排可以更加有效地利用机器的缓存。另外，计数排序是一个非原地排序。当机器主存紧缺的时候，原地排序的优势就更大一些。\n桶排序 # 桶排序同样有一个假设：数据服从均匀分布，且有一定的范围。桶排序的流程是：将区间 $[0,1)$ 划分为若干个桶，将数据投入到各个桶中（实现上，可以用整数除法等操作实现）。在数据分布比较平均的情况下，各个桶里数据的量也比较平均。随后，对每个桶进行排序。最后，把所有的桶拼合起来即可。如果桶内的排序是稳定的，这个算法也是稳定的。\n现在来分析时间代价。假设使用插入排序，那么时间代价 $T(n) = \\Theta(n) + \\sum_{i=0}^{n-1} O(n_i^2)$。对这个式子两边取期望，得到：\n$$ \\mathrm E[T(n)] = \\mathrm[ \\Theta(n) + \\sum_{i=0}^{n-1} O(n_i^2) ] = \\Theta(n) + \\sum_{i=0}^{n-1} O(\\mathrm E[n_i^2]) $$\n我们断言 $\\mathrm E[n_i^2] = 2 - \\frac 1 n$，这个值可以利用随机指示器证明。最终，得到桶排序的时间复杂度：$\\Theta(n) + n \\cdot O(2-\\frac 1 n) = \\Theta(n)$。\n顺序统计量 # 顺序统计量 # 一个 $n$ 个数的集合的第 $i$ 个顺序统计量表示集合中第 $i$ 小的数。寻找顺序统计量的算法称为选择算法。显然，可以通过排序以 $O(n\\log_2n)$ 的代价寻找到该统计量。\n最大值和最小值 # 显然，最大值和最小值都属于顺序统计量的一种，可以通过代价为线性时间的一次扫描获得。一种同时取得最小值和最大值的方法是，对于每两个元素，先对这两个元素互相进行比较。随后，将较大者和最大值比较，较小者和最小值比较。这样，每两个元素只需要进行三次比较。比较次数从 $2n-2$ 次减少到 $\\frac 3 2 n$ 次。\n选择算法 # 期望为线性时间的分治选择算法 # randomized-select 算法是以快速排序为原型的。区别在于，作为选择算法，这个算法只需要对划分的一边进行递归处理。randomized 的意思是，它采用了和随机化快排中使用的相同的划分算法：随机挑选一个值作为主元 pivot。随后，我们从目标所在的那个分区继续递归调用，寻找顺序统计量。显然，这个算法的最坏情况代价和快速排序一样，为 $\\Theta(n^2)$。当所有元素都是互异的，这个算法的期望复杂度能够达到线性水平。证明过程比较繁复，按下不表。\n最坏情况下代价为线性时间的选择算法 # 这种选择算法是这样的：将输入按照每五个一组分组，并通过插入排序找到每一组的中位数，随后，递归调用本算法，找到中位数的中位数。如果这个中位数就是目标，此时就可以返回。然后，以这个中位数为 pivot 进行划分。随后，对目标所在的子数组再次递归调用。\n显然，分组、插入排序和划分都是线性代价的。递归寻找中位数的中位数这个过程的代价是 $T(\\frac n5)$，递归进行最后一步的代价最多是 $T(\\frac 7{10}n+6)$。于是有递归式：\n$$ T(n) \\leqslant \\begin{cases} O(1) \u0026amp; \\text{if } n\u0026lt;140 \\cr T(\\frac n5) + T(\\frac 7{10}n) + O(n) \u0026amp; \\text{if } n \\geqslant 140 \\end{cases} $$\n结果为 $O(n)$。不过，这个算法更加具有理论性质，它的常数项过大，大部分情况下都不适用。\n扩展：其他常见排序方法 # 希尔排序是这样一种算法：规定一个步长，从第一个元素开始，把所有间隔这个步长的元素作为一个子数组，对所有子数组进行原地的插入排序。随后缩小步长，继续进行类似的操作。直到步长为 1，经过插入排序的一遍扫描之后，序列即被排序好。\n希尔排序的核心思想是通过步长和分组使前期排序的 $n$ 变小，而后期排序的序列处于 “基本排好” 的状态，以避免插入排序中面临的大量元素移动操作。一次典型的希尔排序过程如下：\nstep 49 38 65 97 76 131 27 49 55 04 5 13 27 49 55 04 49 38 65 97 76 3 13 04 49 38 27 49 55 65 97 76 1 04 13 27 38 49 49 55 65 76 97 - 如，在算法的第一轮，以 5 为步长，则 (49, 13) 为一组，进行插入排序后二者被交换。第二轮以 3 为步长，则 (13, 55, 38, 76) 为一组，进行插入排序。\n朴素的希尔排序最差情况下的复杂度与插入排序相同，为 $O(n^2)$。不过，在精心设计的步长序列下，希尔排序在小数组上的效率甚至可能比快排更好。与冒泡排序比较，希尔排序相当于使用步长这个特点来使元素一次跳过比较长的距离。由于希尔排序的各轮之间是独立的，这个算法是不稳定的。\n鸡尾酒排序 # 原始的冒泡排序是这样的：每进行完一轮扫描，就回到序列头部重新开始。而鸡尾酒排序在进行完一次从左到右的排序后，继续进行从右到左的排序。鸡尾酒排序的双向冒泡可能会带来更好的性能，例如对于序列 (2, 3, 4, 5, 1)，鸡尾酒排序只需要一个来回，而冒泡排序需要 4 轮。但大部分情况下提升不大。平均时间代价仍然为 $O(n^2)$。\n梳排序 # 梳排序的想法有些类似于希尔排序：同样是使用一个步长，不过这个步长不用来划分出数组，而是仍然从头扫描，两两比较交换。如对于序列 (a, b, c, d, e)，取步长为 3，将会对 a 和 d 比较交换，再对 b 和 e 比较交换。缩小步长为 2，则对 (a, c)、(b, d)、(c, e) 比较和交换。因为每一轮不是完整的排序过程，所以步长只能以 1 为单位递减。梳排序的最差复杂度为 $O(n^2)$，期望为 $\\Theta(n\\log_2n)$。\n其他排序方法 # 除此之外，还可以使用二叉搜索树构造有序序列，这种结构非常适合插入和查找一定大小的值，复杂度为 $O(n\\log_2n)$。选择排序是不断取顺序统计量，也就是不断从后部分序列中寻找最小者并放在头部的算法，复杂度为 $O(n^2)$。内省排序是快排的一种改进，当递归达到一定的深度之后改用堆排序，以兼有二者的优势，将最差代价控制在 $O(n\\log_2n)$。\n"},{"id":7,"href":"/notes/ddia/2/","title":"2. Storage, Query, Encoding","section":"Designing Data-Intensive Applications","content":" 数据存储与检索 # 数据结构 # 最基本的数据结构：线性的 k-v 对，增加/更新时直接 append，查询时搜索整个日志找到最晚的。\n哈希索引 # 在上面的日志基础上，增加一个 hashmap，记录每个 key 的最晚位置。插入更新仍然是线性的，查找的速度也接近线性。需要所有 key 能够放在内存中，适合所有 key 都经常更新的情况。\n为避免用尽磁盘，将日志文件切分为段。对每个已经写完的文件段，可以进行压缩合并，即仅保留其中同一个 key 最晚的记录。这个过程可以异步，不影响正在进行的读写。注意，压缩后每个 key 的 offset 会变化，所以对每个压缩后的段需要保存新的 hashmap。读数据时，从新到旧依次查找每一个 hashmap。这是 Riak 中的 BitCask 的默认做法。\n文件存储：使用二进制格式 删除记录：使用特殊的已删除标记代替 value，进行插入 崩溃恢复：可以从文件中直接还原出 hashmap，可能较慢；也可以在磁盘上保留 hashmap 的快照，减少还原时间 写入时崩溃：使用校验位，确保不会认可不完整的数据 并发控制：单线程追加，多线程读 优点：写入快，并发和崩溃恢复简单，并发能力强 缺点：哈希表需要全部放在内存，区间查询效率差（WHERE BETWEEN） SSTable 和 LSM-Tree # 在上述基础上，对于压缩合并后的段文件，对 key 进行排序；对正在写入的文件，使用平衡二叉树。LevelDB、RocksDB、HBase、Cassandra 都是基于 SSTable。SSTable术语来自 BigTable 论文。整个方法也称 LSM-Tree（Log-Structured Merge Tree)\n合并段变成归并，更加高效 段文件的 hashmap 可以是稀疏的，因为可以通过排序来查找，稀疏程度参考文件块，文件块可以进行通用压缩（区分于上述的取最晚操作，而是 gzip 等通用压缩） 平衡二叉树变成已排序段文件（SS-Table）的效率较高（中序遍历） 为防止崩溃时正在写入的文件丢失，可以双写到二叉树和日志，日志用于恢复，二叉树用于查询。 当 key 完全不存在时，需要访问所有的 hashmap，可能有多次磁盘 IO；为此使用 bloomfilter 做预过滤 压缩合并的方式：LevelDB 和 RocksDB 使用分层压缩，旧数据采用更高的压缩等级；HBase 使用大小分级压缩，较新的段文件较小，被合并到较旧、较大的段文件去。Cassandra 两种都支持。TODO B 树 # 关系型数据库的标准实现，思路是 1. 每个节点是一个磁盘块 2. 让树尽量平衡且矮。\nB 树的可靠性保证比较复杂，因为它是直接替换磁盘上的文件，而非简单的追加的。如果是在节点分裂的过程中发生崩溃就更加危险。一般采用 WAL（Write-Ahead Log）先记录每一次 B 树的修改，再进行真正的改动。崩溃后可以从 WAL 恢复 B 树的状态。\n有些引擎不使用 WAL，而是用 copy-on-write 的方式更新节点。这样的缺点是，除了节点本身之外还要更新一系列指针；优点是崩溃恢复更容易，且比较容易支持并发事务。 对于比较高层的节点并不需要存储完整的 key，而是给出能够描述子节点范围的部分就可以，有利于提高分叉数 考虑到范围查询的需要，很多引擎尽量把相邻的节点在磁盘上相邻存储。但这一点随着数据量变大更难以保持，LSM-Tree 则比较容易（非原地归并） 考虑到范围查询，可以增加兄弟节点指针 一些变体，如分形树，借鉴了日志结构的思路来减少寻道 B 树和 LSM 树对比 # 整体来看，LSM 树写入更快，B 树读取更快。\nB 树的每一次写都需要至少修改 WAL 和整个页，写放大一般比 LSM 大，且随机读写较多。 LSM 树的数据结构更紧凑、压缩率更高，且碎片空间较少。B 树可能有很多页只存储了少量数据，造成浪费。 现代 SSD 可能将随机读写在内部转化为顺序读写，但写放大和碎片的问题不能被完全抹除。 LSM 树的后台压缩合并过程可能占用磁盘带宽，从而影响同时进行的读写操作，造成 LSM 树的高分位数读写延迟较高，而 B 树的性能比较稳定 LSM 树存储的数据量越大，压缩合并的成本就越高，用于实时读写的磁盘带宽就越少。 如果写入速度超过了压缩的速度，LSM 树的段文件就会越来越多。一般 LSM 树的数据库并不限制写入速度，因此需要额外监控。 B 树比较容易加锁，事务隔离可以直接实现到 B 树上。LSM 的写入有多副本，不容易隔离。 其他索引 # 上述只考虑了主键索引。除此之外，两种索引都可以用来实现二级索引。和主键索引不同，二级索引的 key 经常重复，这时可以用拉链法或在 key 上加入其他标记，然后用范围查询来实现。\n聚集索引 # 无论主键还是二级索引，索引的 key 是要查找的值，value 可以是行本身，或者指向实际行的指针，实际行位于堆文件中。堆文件可以是追加的，也可以是替换的（记录下被删除的位置，下次再填入）。这样，当存在二级索引时，就不需要复制数据。在 update 数据时，如果新值的大小不大于旧值，就可以原地替换，非常高效；如果大于旧值，就需要进行一系列移动并更新所有索引，或者直接在旧位置留下一个指向新位置的指针。\n如果索引的 value 是行本身而不是指向堆文件的指针，就是聚集索引。在 InnoDB 中，主键永远是聚集索引，二级索引引用的是主键，而非直接指向堆文件。在另一些实现中，可能主键和二级索引都不是聚集的，而是把数据存在散乱的堆文件中。\n也可以只在索引中保存部分列，称为覆盖索引，介于聚集索引和非聚集索引之间。如果查询的列是这些列的子集，就可以只利用索引返回结果，称索引覆盖了这个查询。例如，对于多个字段的索引，将前面的字段放在索引内，后面的字段只保存指针。\n当然，聚集和覆盖索引加快了查询速度，提高了数据冗余，占用更多空间，且提高了事务的复杂性。\n多列索引 # 上面的索引都只有一个 key 对应一个 value。但如果查询包括了多个列的范围查询，就不能满足，常见的是地理数据中需要同时过滤经度和纬度。一种选择是使用空间填充曲线（Space-filling curve）将两个列转化为单个列，或者使用专门适用于空间的索引结构，如 R 树。\n全文搜索和模糊索引 # Lucene 的做法是搜索在某个编辑距离内的文本，使用类似 SSTable 的方法，内存中的索引不再是稀疏的 key，而是一系列 DFA，类似字典树。这个自动机可以转化为 Levenshtein 自动机。\n在内存中保存所有内容 # 典型例子是 Memcached。也有在内存中实现的关系数据库。重启后也可以从磁盘中恢复状态，但运行时读取完全靠内存服务。写入磁盘一般是追加日志，除了持久化以外还方便备份、分析等。\n需要注意的是，内存数据库的性能优势并不来自内存读写，因为当数据较小时，磁盘数据库也可以几乎不访问磁盘。其优势主要来自序列化的开销。\n此外，内存数据库可以提供一些磁盘难以实现的功能，如 Redis 的优先队列和 Set。\n内存数据库也可以反缓存到磁盘，和磁盘数据库的区别是，被保存到磁盘的单位可以是行，而非页，因此也可能有一定性能优势。\nOptane 等非易失性存储（Non-volatile Memory）为内存数据库带来更多的想象空间。\n事务处理与分析处理 # 事务这个名词并不等同于 ACID，它实际上指的是 OLTP 的典型行为，即少量数据的低延迟读取和写入。与之对应的是分析处理，即 OLAP。\nOLAP 数据一般都是关系模型，因为 SQL 是分析的强力工具。\n星型模式 / 维度建模 / 雪花模式 # 在维度建模中，通常有一个事实表加上多个维度表，每个维度表达了 5W1H 信息。日期和时间也被建立维度表，从而进行日期维度的分析，如区分假期和平日的区别。\n在星型模式的每个维度表基础上再增加维度表就是雪花模式，例如产品表可以外键到另一个品牌维度表等。\n列存 # OLTP 和文档数据库中，每一行通常被相邻存放。在 OLAP 维度建模的场景下，每一个表通常非常宽，但每次只访问其中的几个列。这个模型和关系数据库相匹配，但非关系数据也可以使用，例如 Parquet 也支持文档数据存储结构。\n因为相同的列被放在一起，它们的数据相似性也变高，所以压缩率也大于行存。\n列压缩 # 列压缩的一个办法是，把枚举值变成 bitmap，那么当计算 IN (a, b, c) 的查询时，只需要按位与再和 0 比较即可。\n对于有 n 个枚举值的数据，每一个单元格的数据大小就是 n 个 bit。显然，仅仅这样存储的数据中会有很多 0。此时对于每个枚举值，再使用 run-length code 压缩，从而获得很高的压缩率和查询性能。\nA B C D 0 1 1 1 2 1 3 1 4 1 5 1 上述数据，按照对每个枚举值从上到下 run-length code 的方式编码。因为对每个枚举值，取值只有 0 和 1，所以可以省略，只记录每一段的长度：\nA, 0, 1, 5; B, 4, 1, 1; C, 1 3 2; D, 5 1\n内存带宽和矢量化 # 对大数据量的高速处理，需要考虑内存带宽、缓存命中率和 SIMD 指令的利用率。\n列压缩使得数据可以被放在 L1 缓存中用循环来处理，而避免了条件跳转和函数调用；使用按位操作运算符，使得 SIMD 可以比较容易使用。\n列存与排序 # 虽然数据是按列存储的，仍然需要按行排序，否则无法和主键相关联。排序的依据可以是多个列，例如把日期作为第一排序，对日期相同的可以用其他列来排序。当然，过多排序列将失去排序的作用。\n排序也使得 run-length code 的压缩率更高。\n另一种思路是，既然分布式数据库需要将数据备份存储，就可以在不同的副本中使用不同的排序列。当数据没有丢失时，就可以在不同的列上得到性能优势。\n列存的写操作 # OLAP 常见的是只读操作，但上面的优化都会使得写操作更加复杂。一个简单的改进办法是 LSM-Tree。\n聚合：数据 CUBE 与物化视图 # 将常见的聚合操作预先计算缓存起来，即物化视图。视图是指将计算逻辑固化，物化是指其将数据缓存了起来，而不仅仅保存了逻辑。\n当数据发生变化时，物化视图也需要随之更新，降低写入性能。所以总的来说是否提升性能需要考虑到实际的查询情况。\n数据 CUBE 是不同维度聚合的物化视图的集合。一个简单的二维数据 CUBE 就是数据透视表。事实表一般有不止两个维度，因此数据是一个高维超立方体。\n数据编码与演化 # 数据编码格式 # 通常数据同时存在于内存中和文件/网络中，需要在这两种形式间互相转化，即编解码。\nLanguage-Specific formats # Java 有 Serializable，Ruby 有 Marshal，Python 有 Pickle，此外还有第三方的 Kryo 等。但它们的问题是：\n仅适用于一个语言 经常包含能够生成一个类的实例的功能，可能带来安全问题 前后兼容能力一般不好 性能效率通常不好 JSON、XML、CSV 与其二进制变体 # XML 和 CSV 无法区分数字和数字字符串，遇到浮点数边界时问题尤其明显 不支持二进制 XML 有一些 Schema 规范，但都很复杂，正确实现其解码比较困难 CSV 没有模式，在格式变化时的前后兼容很难实现，很多实现也没有很好处理转义 XML 和 JSON 都有一些二进制变体，如 BSON，MessagePack，其中一些扩展了其数字和二进制的能力，但都没有规定模式，因此字段名仍然需要保存在编码后的每一条数据中。\nThrift 与 ProtoBuf # Thrift 和 ProtoBuf 的 IDL 和二进制都比较相似。共同特点是二进制数据中有字段类型、字段编号和字段长度，但没有字段名，字段名由 IDL 和基于 IDL 生成的各种语言的编解码代码确定。字段都区分 Optional 和 Required，通过字段编号来支持前后兼容和 Schema 变化。\n当旧代码读取新数据时，会直接忽略新的字段。当新代码读取旧数据时，不能缺少 required 字段，但可以缺少 optional 字段。\n当数据类型变化时，可能造成精度损失。\nThrift 支持列表和列表嵌套。\nProtobuf 不支持列表，只能将字段标为 Repeated。如果一个字段从非 Repeated 变成了 Repeated，那么读取旧数据的新代码只能看到一个数据，读取新数据的旧代码只能看到最后一个数据。\nAvro # Avro 解决的是 Thrift 和 Protobuf 不适合 Hadoop 使用的问题，IDL 可以用 JSON 表示以方便机器读取。Avro 没有字段类型和编号的标记，完全由读写模式确定。读写模式不必完全一样，只需要互相兼容。读取数据时，同时提供读模式和写模式，用写模式将数据取出，再转换为读模式，二者之间用字段名关联，从而允许字段位置变化或增减字段。\n{ \u0026#34;type\u0026#34; : \u0026#34;record\u0026#34;, \u0026#34;name\u0026#34; : \u0026#34;userInfo\u0026#34;, \u0026#34;namespace\u0026#34; : \u0026#34;my.example\u0026#34;, \u0026#34;fields\u0026#34; : [{\u0026#34;name\u0026#34; : \u0026#34;username\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34; : \u0026#34;NONE\u0026#34;}, {\u0026#34;name\u0026#34; : \u0026#34;age\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;int\u0026#34;, \u0026#34;default\u0026#34; : -1}, {\u0026#34;name\u0026#34; : \u0026#34;state_province\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34; : \u0026#34;NONE\u0026#34;}, {\u0026#34;name\u0026#34; : \u0026#34;country\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34; : \u0026#34;NONE\u0026#34;}, {\u0026#34;name\u0026#34; : \u0026#34;zip\u0026#34;, \u0026#34;type\u0026#34; : \u0026#34;string\u0026#34;, \u0026#34;default\u0026#34; : \u0026#34;NONE\u0026#34;}] } 如果读模式里有某个字段，写模式中没有（即要读取的数据中没有），则要求读模式中的这个字段有默认值。可以在 schema 中指定，也可以把字段定义为 union{null, long} 从而用 NULL 当默认值。因此 Avro 的字段无需区分 Required 和 Optional。\n总的来看：\n如果新的 reader 增加了一个没有默认值的字段，就无法读取旧 writer 写的数据 如果新的 writer 删除了一个没有默认值的字段，就无用旧 reader 读这个数据 如果新的 writer 给 union 类型增加了分枝，就无法用旧 reader 读 如果新 reader 更改了字段名，可以通过 alias 解决；但如果新 writer 更改了字段名，旧 reader 并不知道新的字段名是什么，所以无法读取 可以看到，Avro 在读取数据的时候，需要额外知道 writer 使用的 schema 是什么。注意到，很多时候 schema 定义可能比一行数据本身还要大。因此，Avro 适用于：\nHadoop 等场景，有数百万行 schema 相同的数据，schema 本身的大小变得不值一提，可以放在数据头部 在每一行数据开头带上 writer 的版本号，reader 侧另外记录每个版本号对应的 schema。 建立连接时先确定 schema，之后都用这个 schema 来传输大量文件。 Avro 同时支持代码生成和动态解析两种方式。\n动态模式 / 模式的优点 # Avro 不通过字段的编号来确定字段，而是把 schema 直接带上。这样的好处是，当数据的结构发生变化时，使用 Thrift 或 ProtoBuf 需要手动根据编号来改动 schema、再生成编解码代码、再编译，而 Avro 直接生成一个新的 schema（JSON 格式，可以比较方便地用机器生成）。并且，只要不改变字段名，就可以比较好地前后兼容。\n相应地，使用代码生成再编译的方式，产生的编解码代码通常性能更好，而且可以在编译期进行检查，确保某个数据只要能够被解码，就可以正确地操作。\nXML 和 JSON 有一些 schema 实现，但通常都更加复杂。例如，其中经常包括要求某个字符串字段符合某个正则表达式、要求某个整数字段在某个范围内的限制。相比之下，Thrift 这类二进制编码在这方面要简单得多。另外，数据库的 ODBC 或 JDBC Driver 经常使用自己的数据编解码格式。二进制编码的优势：\n通常比二进制变体的 JSON 更紧凑 Schema 本身是有价值的文档 可以在部署之前检查兼容性 对静态类型语言+代码生成的情况，可以进行类型检查。 数据流格式 # 通过数据库 # 访问同一个数据库的不同进程可能正在运行不同版本 schema 的代码，如果已经用新 schema 写入过，旧代码再写入同一行数据，对于旧代码中不存在的字段，理想的行为是保持不变。在使用 ORM 的时候需要注意这一点。\n在数据库中，数据比代码更加长久，经常可能存在数年前的数据。大部分关系数据库支持以常数代价增加一个全为空的新列，Linkedin 的文档数据库 Espresso 使用 Avro 存储并支持其演化规则。因此，虽然数据库底层可能存在各个版本的数据（例如，数年前的数据可能比现在的数据少很多个列），但从用户视角它们的模型是一样的。\n归档存储时，无论数据编码是何时的形式，在归档中通常都用最新的编码。\n通过 REST / RPC # 在 SOA（Service-Oriented Architecture）或称微服务的架构下，预期每个服务可以由单独的团队维护，因此新旧版本的代码经常需要一起运行。\nWeb 服务不仅用于 Web：\n客户端，浏览器，JS Web 应用程序 同一组织的内部调用（中间件） 不同组织互相调用（如支付） REST 的基本思路是，使用 URL 来标识资源，用 HTTP 的身份认证、缓存控制、内容协商等功能，通常基于 JSON。\nSOAP 是基于 XML 的协议，避免使用 HTTP 功能，而是使用 ws- 系列功能。因为使用 XML，所以严重依赖代码生成、IDE 等，且不同厂商的实现之间兼容性不一致。\nRPC 曾有多种历史实现：Enterprise JavaBeans 和 RMI 仅限于 Java，DCOM 仅限于微软平台，CORBA 过于复杂且没有前后兼容能力。本质上，网络调用和本地调用本就存在一定的差异：\n本地函数是可预测的，而远程调用可能由于网络故障、远端运算速度都受影响，需要有重试、超时等机制 本地函数要么返回结果、要么抛异常、要么陷入阻塞；而远程调用超时，我们无法确定远端是否收到并处理了请求，只是回复没有收到。再考虑到重试，需要提供幂等性。 每次调用本地函数的时间比较稳定，远程调用则波动很大 对于大对象，本地调用可以传递指针共用内存，远程调用需要编解码 RPC 可以用不同语言来实现，可能带来复杂的实现，例如不同语言对大整数的处理不尽相同。 REST 其中一个吸引人之处在于，它并不试图隐藏自己是网络调用的事实。尽管如此，很多 RPC 框架仍然尝试在 REST 之上实现。\n上面的例子中，Thrift 和 Avro 有 RPC 实现，ProtoBuf 有 gRPC 等。新一代的 RPC 框架一般不再试图和本地函数调用混淆，例如使用 Futures 来包装返回结果。gRPC 还支持流，一些框架还提供了服务发现。\n尽管如此，因为 REST 更方便调试，常见的方式是在组织内部使用 RPC，对外 API 使用 REST。\n在 RPC 的数据编码演化中，一个简化的假设是所有服务端先更新，客户端后更新。于是，请求需要向后兼容，相应需要向前兼容。对提供给外部的 API，很难强迫调用方升级，经常直接维护多个版本。标记版本可以基于 API Token+管理平台，或者在 URL 和 header 里说明。\n通过消息队列 # 和直接 RPC 相比，消息队列的优势：\n当接收方不可用或过载，可以缓冲请求 可以自动重试，避免请求丢失 避免依赖固定 IP 可以将一条消息发给多个接收方 在逻辑上将发送方和接收方分离 相应地，消息队列一般是单向的。消费者可以把数据发送到其他队列或回复队列。消息队列本身对数据 schema 一般不会强制，可以使用任意的编解码方式。\nActor 模型原本是用于单个进程并发的模型，通过封装 Actor 和消息传递避免竞争条件、锁和死锁等问题。在分布式 Actor 中，不同的 Actor 部署在不同的节点上。无论两个 Actor 是否在同一个节点上，都采用相同的编解码方式来传递消息，模型已经假定了消息丢失的情况，无需额外考虑。不过，对 Actor 的滚动升级仍然需要考虑前后兼容性。\nAkka 使用 Java 的内置序列化，不提供前后兼容性，但可以用 ProtoBuf 等来替代从而获得兼容 Orleans 同理，除了使用其他的序列化方式之外，也可以新建一个集群，将流量逐渐切换。 Erlang OTP 中，滚动升级很难实现。 "},{"id":8,"href":"/notes/programming-scala/3/","title":"3 .Inheritance, Package, Assertion","section":"Programming in Scala","content":" Scala 的层级 # Scala 继承层级 # Any 类定义了以下的方法：\nfinal def ==(that: Any): Boolean final def !=(that: Any): Boolean def equals(that: Any): Boolean def hashCode: Int def toString: String 其中 == != 方法是 final 的，它们的取值取决于 equals 方法。因此，Scala 中可以使用 == 来比较 Integer String 和其他对象。\nAny 有两个子类：AnyVal AnyRef。其中 AnyVal 有九个子类，包括 Java 的八种基本类型和 Unit。这些类都不能用 new 来创建，而必须使用字面量。实际上，这些类都是 abstract final 的，所以无法使用 new。Unit 则只有一个值，写作 ()。\nAnyVal 的子类被称为值类型，它们之间可以隐式地互相转换。之前提到过，它们还可以隐式地转换为对应的 Rich 类以支持 until range max 等更多操作。值类型在编译之后将会变成基本类型而不是他们对应的装箱类型，这样做能够带来一些性能提升。Scala 在这里做的事情和 Java 5 的自动装箱很相似。另外一个类 AnyRef 实际上就是 java.lang.Object。\n在整个继承树的底端是 scala.Nothing 和 scala.Null。Null 类有一个实例，即 null，也就是空引用，而 Nothing 则没有值。这样做的目的是为类型推导提供方便。二者的主要区别是，Null 仅包括了 AnyRef 的子类（某种意义上的引用类型），Nothing 包括值类型。以及，Null 有一个实例。\n{ x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else throw new Exception } // Int =\u0026gt; String { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else null } // Int =\u0026gt; String { x: Int =\u0026gt; if (x == 0) \u0026#34;zero\u0026#34; else 1 } // Int =\u0026gt; Any 自定义值类型 # class Dollars(val amount: Int) extends AnyVal { override def toString() = \u0026#34;$\u0026#34; + amount } 值类型可以让代码更加清晰，减少错误。考虑这样一段关于 HTML 的代码：\ndef title(text: String, anchor: String, style: String): String = s\u0026#34;\u0026lt;a id=\u0026#39;$anchor\u0026#39;\u0026gt;\u0026lt;h1 class=\u0026#39;$style\u0026#39;\u0026gt;$text\u0026lt;/h1\u0026gt;\u0026lt;/a\u0026gt;\u0026#34; 四个参数都是 String，有人称这种代码为 Stringly Typed。因为字符串之间没有区别，这段代码实际上和弱类型语言并没有本质区别，编译器也不能帮我们检查错误。如果使用短小的值类型，就能解决这种问题：\nclass Anchor(val value: String) extends AnyVal class Style(val value: String) extends AnyVal class Text(val value: String) extends AnyVal class Html(val value: String) extends AnyVal def title(text: Text, anchor: Anchor, style: Style): Html = new Html( s\u0026#34;\u0026lt;a id=\u0026#39;${anchor.value}\u0026#39;\u0026gt;\u0026lt;h1 class=\u0026#39;${style.value}\u0026#39;\u0026gt;text.value\u0026lt;/h1\u0026gt;\u0026lt;/a\u0026gt;\u0026#34; ) 相等性 # 因为 Scala 将 == 和 equals 统一起来，所以 AnyRef 定义了 eq 方法用于两个引用的直接比较。类似地，还有一个相反的方法名为 ne。有关相等性在 30 章还会有更多讨论。\n\u0026#34;abc\u0026#34; == new String(\u0026#34;abc\u0026#34;) // false \u0026#34;abc\u0026#34;.equals(new String(\u0026#34;abc\u0026#34;)) // true new String(\u0026#34;abc\u0026#34;) == new String(\u0026#34;abc\u0026#34;) // true new String(\u0026#34;abc\u0026#34;) eq new String(\u0026#34;abc\u0026#34;) // false // warning: comparing a fresh object using `eq\u0026#39; will always yield false // new String(\u0026#34;abc\u0026#34;) eq new String(\u0026#34;abc\u0026#34;) // ^ scala \u0026gt; \u0026#34;abc\u0026#34; eq \u0026#34;abc\u0026#34; // true, because of hash consing 这个原则的唯一例外是 Java 装箱类型。在 Java 中，能够互相转换的对象的 equals 结果仍然是 false。\njava.lang.Integer.valueOf(1) equals java.lang.Long.valueOf(1) // false java.lang.Integer.valueOf(1) == java.lang.Long.valueOf(1) // true java.lang.Integer.valueOf(1) eq java.lang.Long.valueOf(1) // false java.lang.Integer.valueOf(1) equals new java.lang.Integer(1) // true java.lang.Integer.valueOf(1) == new java.lang.Integer(1) // true java.lang.Integer.valueOf(1) eq new java.lang.Integer(1) // false 而对于 Int 来说，因为是值类型，不能进行 eq 比较。\n特质 Trait # Trait # Trait 和 Java 的接口的最大差别是我们混入（mix in）Trait 而不是实现接口。混入一个 trait 有两种方式：\nclass A trait T extends A object O extends T val v: A = O 如果使用 extends 关键字，那么新的对象就会继承于 trait 的父类。如果 trait 没有显式继承，那么这个对象理所当然地会继承于 AnyRef。\ntrait Tr object Obj extends A with T with Tr val v: T = Obj val v: A = Obj val v: Tr = Obj 瘦接口 VS 胖接口 # 在 Java 8 之前，Java 世界通常使用的是瘦接口。例如，虽然 String 的大部分方法都可以用于任何字符序列，CharSequence 接口提供的仍然很少。在没有默认方法的情况下，实现一个胖接口是一件十分累人的工作。在 Java 8 引入默认方法之前，这个接口只有四个方法：\npublic interface CharSequence { int length(); char charAt(int index); CharSequence subSequence(int start, int end); public String toString(); public default IntStream chars() {...} public default IntStream codePoints() {...} } 由于历史原因，即使到了 Java 8，为了与旧库的兼容，我们也不敢再贸然加入太多的新默认方法了。Java 世界的常规做法是定义一个抽象类来加入 Java 8 的默认方法所做的事情，再继承，但这样又失去了定义接口的意义。在 Scala 中，你只需要实现 compare，就能得到一系列方法：\ntrait Ordered[A] extends Any with java.lang.Comparable[A] { def compare(that: A): Int def \u0026lt; (that: A): Boolean = (this compare that) \u0026lt; 0 def \u0026gt; (that: A): Boolean = (this compare that) \u0026gt; 0 def \u0026lt;= (that: A): Boolean = (this compare that) \u0026lt;= 0 def \u0026gt;= (that: A): Boolean = (this compare that) \u0026gt;= 0 def compareTo(that: A): Int = compare(that) } 混入 # class A { def f(s: String) = print(s + \u0026#34; from class A\u0026#34;) } class B trait C extends A { override def f(s: String) = super.f(s + \u0026#34; from trait C\u0026#34;) } 这样编写特质之后，我们发现的第一件事是，因为我们在 C 的定义中明确了 A，所以 B 不能混入 C，因为它不是 A 的子类。\nclass D extends B with C // error: illegal inheritance; class D extends A with C 其次，值得注意的是 C 中定义了一个包含 super 的方法。这个定义在传统的面向对象模式中比较奇怪，如果是一个类，我们可以明确地知道 super 指向哪一个类的哪一个方法，而对于 interface 来说则不能。这体现了 Scala 中 trait 可堆叠的特性。类的 super 是静态绑定的，而 trait 中的 super 则是动态绑定的。这也是为什么这个方法的标签为 override。另外，我们也可以直接在定义 object 时混入 trait，甚至在 new 一个对象时混入：\nobject o extends A with C o f \u0026#34;abc\u0026#34; // abc from trait C from class A (new A with C) f \u0026#34;abc\u0026#34; // abc from trait C from class A 接下来我们来堆叠 trait。\ntrait D extends A { abstract override def f(s: String) = super.f(s + \u0026#34; from trait D\u0026#34;) } (new A with C with D) f \u0026#34;abc\u0026#34; // abc from trait D from trait C from class A (new A with D with C) f \u0026#34;abc\u0026#34; // abc from trait C from trait D from class A 基本上来说，super 的顺序取决于混入的顺序。换句话说，在处理多重继承问题时，混入的顺序就决定了方法动态绑定时线性化的顺序。在多重继承这个问题上，Java 的做法是，类优先于接口的默认方法，两个接口的默认方法的冲突则会报错。在 Scala 中，两个 trait，或一个 trait 和一个类中的普通方法相遇时也会报错，而对于有父类的 trait 中的 abstract override 方法，则能够通过这种方式来扩展功能。举个这样的例子：\nabstract class IntItem { def get: Int; def put(x: Int) } trait DoubleItem extends IntItem { abstract override def put(x: Int) = super.put(2 * x) } trait PlusOneItem extends IntItem { abstract override def put(x: Int) = super.put(1 + x) } class BasicIntItem extends IntItem { private var i: Int = 0; def get = i; def put(x: Int) = {i = x} } val a = new BasicIntItem with DoubleItem with PlusOneItem a.put(2) a.get // 6 val b = new BasicIntItem with PlusOneItem with DoubleItem b.put(2) b.get // 5 在这个例子里，因为父类 IntItem 是一个抽象类，所以 trait 中的类也要一起声明为抽象类（因为其中的 super.put 方法还是抽象的）。然后，我们提供了一个实现 BasicIntItem。这样，就能体现出 trait 中方法的动态绑定，super.put 最终被绑定到了 BasicIntItem.put 上去。然后，可以看到由于混入顺序的不同，方法的调用顺序不同，最终产生的逻辑也不同。如果我们在方法中加入的功能是可以堆叠的，就应该考虑这类方法。\n细节和使用 # 首先来讨论一下什么时候应该使用特质。首先，如果一个特性可以出现在一些互相关系并不大的类的类中时，特质通常是一个比抽象类更好地选择。这和\u0026quot;抽象类描述本质而接口描述功能\u0026quot;的想法是统一的。另外，在分发代码或者和 Java 协作是也要考虑。Java 可以自如地继承 Scala 类，继承特质就比较麻烦。例外是，完全抽象没有实现体的特质会直接被翻译成接口，所以这种没有问题。此外，在分发代码时，如果特质发生了改变，其下游的类需要跟随进行重新编译。所以如果一个接口可能被下游继承实现，抽象类可能是更好的选择。\n堆叠这种方式是解决多重继承的其中一个解决办法，可以参考 Python 使用的 C3 线性化来更好地理解它。由于 Scala使用了单继承 + trait 的方式，类的线性化方式比 C3 要简单。在 Java 中遇到方法冲突时，或者是 Scala 中的非 override 方法发生冲突时，我们会采用这样的方式：\ninterface B { public default void f(String s) { System.out.println(s + \u0026#34; from interface B\u0026#34;); } } interface C { public default void f(String s) { System.out.println(s + \u0026#34; from interface C\u0026#34;); } } class D implements B, C {} // ERROR, method conflict, won\u0026#39;t compile class D implements B, C { public void f(String s) { System.out.println(s + \u0026#34; from class D\u0026#34;); } } new D().f(\u0026#34;abc\u0026#34;) // abc from class D 或者在 Scala 里：\ntrait A {def f = 1} trait B {def f = 2} class C new C with A with B { override def f = 3 }.f // 3 和堆叠 trait 相比，这样的方式很难同时继承两个接口方法的功能。除非你能够把两个方法都重新实现一次，如果我们写出这样的方法（伪代码）：\nclass A { def f(s: String) = println(s + \u0026#34; from A\u0026#34;) } trait B extends A { override def f(s: String = super(s + \u0026#34; from B\u0026#34;)) } trait C extends A { override def f(s: String = super(s + \u0026#34; from C\u0026#34;)) } object o extends A with B with C ( override def f(s: String) = { B.super(s); C.super(s)}) o.f(\u0026#34;abc\u0026#34;) // abc from B from A // abc from C from A 由于这里的菱形继承关系，父类 A 的方法会被调用两次，这是我们不想看到的。\n线性化规则 # 最后我们来谈谈线性化的规则。相比 Python 或 C++ 这种支持多继承的语言，单继承 + 接口的语言的线性化要简单一些。基本的原则是，一个类型的线性化的后半部分是其父类的线性化。\nclass Animal trait Furry extends Animal trait HasLegs extends Animal trait FourLegged extends HasLegs class Cat extends Animal with Furry with FourLegged 于是根据这个规则我们有：\nAnimal、Furry 和 Four 的线性化显而易见。对于 Cats，按照 trait 混入的顺序。Furry 第一个被混入，因此也第一个线性化，得到 Furry -\u0026gt; Animal -\u0026gt; AnyRef -\u0026gt; Any。然后混入 FourLegged，得到 FourLegged -\u0026gt; HasLegs -\u0026gt; Furry -\u0026gt; Animal -\u0026gt; AnyRef -\u0026gt; Any，最后线性化 Cats 本身，这个顺序也就是 super 的查找顺序。\n包和访问 # 包 # 可以在一个文件里使用多个包：\npackage com package example package a { class A } package b { class B(val v: a.A) class C(val v: D) } class D 上面的代码生成了四个类：com.example.D com.example.a.A com.example.b.B com.example.b.C。而且，在同一个包里的类可以简略地互相访问。只有嵌套起来的包才可以这样去访问，这是符合直觉的，比如：\npackage a { class A } package a.b { class B(val v: A) } // Syntax Error, Won\u0026#39;t Compile 如果不规定的话，最外层的包名为 __root__。\nobject A // __root__.A package a { object A } // a.A 除此之外，还可以定义包对象，通常用于定义一些 Util 函数。这里的函数在整个包都能访问到。\n// in file com/example/package.scala package object example { def fun = ... } // another file import com.example.fun ... 引入 # import com.example.a // only the package import com.example.a.{A, B} // two objects import com.example.a.A._ // all in the object A import com.example.a._ // all objects in package a import com.example.a.{A =\u0026gt; C, B} // rename A to C import Laptops._ import Fruits.{Apple =\u0026gt; _, _} // all in the Laptops, and all except Apple in Fruits Scala 环境默认引入了一些变量，后引入的会覆盖先引入的。例如，scala.StringBuilder 会覆盖掉 java.lang.StringBuilder。\nimport java.lang._ import scala._ import Predef._ 访问控制 # class Outer { class Inner { private def f = 1 } new Inner().f // works in Java, not in Scala } package example { class Super { protected def fun = 1 } class Another{ def fun = { new Super().fun } } // works in Java(protected in same package), not in Scala } Scala 的成员是默认公共的。除了上述两条比 Java 更合理的限制之外，Scala 还提供了更细致的访问控制。\npackage com package example class Clazz { private[com] def fun = 0 // accessible for all in com and subpackages of com private[example] def fun = 0 // same as default in java private[Clazz] def fun = 2 // same as private in java class Inner { private[Inner] def fun = 3 // same as private in scala private[this] def fun = 4 // only in this object } } 最后一个问题是伴生对象。理论上来说，伴生对象和它伴生的类是两个不同的类型（type）。不过显然这两者应该共享访问控制才合适，Scala 也是这么做的。对单例对象来说 protected 没有意义。\n断言 # 断言有两种形式，一种是随时加入的传统断言，另一种是在返回结果之前进行的检查。断言接收第二个 Any 类型的参数，调用 toString 将结果作为错误信息显示。\ndef f(a: Int, b: Int) = { assert(a \u0026gt; 0) a + b } ensuring(a \u0026lt; _, \u0026#34;abc\u0026#34;) f(-1, 1) //java.lang.AssertionError: assertion failed f(1, -1) // java.lang.AssertionError: assertion failed: abc f(1, 1) ensuring 的实现大致是这样的：\n@elidable(ASSERTION) def assert(assertion: Boolean): Unit = { if (!assertion) throw new java.lang.AssertionError(\u0026#34;assertion failed\u0026#34;) } @elidable(ASSERTION) @inline final def assert(assertion: Boolean, message: =\u0026gt; Any): Unit = { if (!assertion) throw new java.lang.AssertionError(\u0026#34;assertion failed: \u0026#34;+ message) } implicit final class Ensuring[A](private val self: A) extends AnyVal { def ensuring(cond: Boolean): A = { assert(cond); self } def ensuring(cond: Boolean, msg: =\u0026gt; Any): A = { assert(cond, msg); self } def ensuring(cond: A =\u0026gt; Boolean): A = { assert(cond(self)); self } def ensuring(cond: A =\u0026gt; Boolean, msg: =\u0026gt; Any): A = { assert(cond(self), msg); self } } 也就是说，返回值被隐式转换为了一个 Ensuring 对象，这个对象有一个 ensuring 方法来接收这些参数，并进一步调用 assert。\n"},{"id":9,"href":"/notes/core-java-impatient/3/","title":"3. Inheritance, Reflection","section":"Core Java for Impatients","content":" 继承 # Java 中的 super 不是另一个对象的引用，而是绕过动态查找方法的指令。相比之下，this 是一个指向本身的引用。 在方法引用中可以使用 super。new Thread(super::work).start(); 重载方法不能改变参数。可以使用 @Override 让编译器进行检查。 子类无法访问父类的 private 成员。因此，应该在构造时调用父类的构造函数。super(params); 如果一个类的父类中有一个实例方法，实现的接口中有一个同名的默认方法。与接口冲突的情况不同，在这里，父类方法永远先于接口的实现。这是为了与旧版本代码的兼容。 final 方法不能被覆盖，final 类不能被继承。 里氏代换原则 # 里氏代换原则是指，一个父类对象可以出现的位置，也可以放置一个子类对象。\n因此，子类重载的方法可以返回父类方法返回值的子类。这被叫做协变返回类型。\nJava 的数组同样是协变的。也就是说，一个父类数组引用可以指向一个一个子类数组对象。因此，可能存在这样的问题：\nSon[] sons = new Son[0]; Father fathers = sons; fathers[0] = new Father(...) // Exception: ArrayStoreException 在这里，fathers 指向的实际上是一个 Son 类的数组，其中只能存储 Son 类的对象。因此，这里出现了一个编译时无法发现的错误。为了避免这种问题，应当限制协变数组的作用域。\n抽象类 # 抽象类的某些方法被声明为抽象的：\nabstract class Person { private String id; public Person(String name); public abstract int getId(); } 和接口的区别是，抽象类可以拥有实例变量和构造函数。实现上，我们认为，抽象类是一个可以被具体化的模型，而接口代表的是某一个功能，二者的意义是有区别的。因此，类是单继承的，接口是可以多继承的。\n当然，抽象类中可以不包含抽象方法，虽然这种情况很少见。但是，包含抽象方法的类必须被声明为抽象的。\n访问权限 # 子类的重载方法的可见性必须等于或者高于父类。例如，protected 父类方法的重载只能是 protected 和 public。这里可以看到，protected 是比默认的包访问权限更开放的。换句话说，同一个包内的其他类可以访问 protected 方法。\n不过，即使 B 继承了 A 类，B 的方法也只能访问 A 对象中的 A 方法，而不能访问其他的 A 对象。这样，就不会存在仅为了访问 protected 方法而编写的类。\n因此，我们可以看到，开放的方法不能被封闭起来，因此要慎重使用 protected 关键字。\n匿名子类 # 同样可以使用匿名子类，用法和接口的匿名类十分类似。例如：\nArrayList\u0026lt;String\u0026gt; names = new ArrayList\u0026lt;String\u0026gt;(100) { @Override public void add(int index, String element) { super.add(index, element); System.out.println(\u0026#34;added elem\u0026#34;); } } 同样地，如果给匿名子类使用一个初始化块，将得到这样的结果：\nnew ArrayList\u0026lt;String\u0026gt;() {{ add(\u0026#34;abc\u0026#34;); add(\u0026#34;abc\u0026#34;); }}; 这里，外面的大括号是类的大括号，里面的是初始化块的大括号。当然，这种技巧并不推荐。因为得到的并不是一个真正的父类对象，在某些实现的 equals() 比较中可能会出现问题。\nObject 类 # toString() # 对于一个普通的对象，习惯上使用这样的 toString() 格式：\njava.awt.Point[x=10,y=20]\n在 Object 类中，toString() 将给出这样的格式：\njava.io.PrintStream@2f6684\n对于一个 int[] 对象，打印出来的结果形如：\n[I@1a46e30\n其中的 [ 代表数组类型，I 表示 int 类型。对于一般的对象则会打印出包含包名的类名，形如：\n[Ljava.lang.String@123456\n更好的方法是使用 Arrays.toString(arr)，将会得到：\n[2, 3, 4]\n如果是多维数组，则要使用 Arrays.deepToString()。\n当对象与字符串连接时，会自动调用 toString() 方法。\nequals() # equals() 应当是 null 安全的。 一般认为返回真的两个对象是完全相等的，且检测代价很小。 由于需要在方法内进行 cast，之前一定要用 instaceof 或 getClass() 进行检查。 对于基本类型可以使用 == 操作符，对于 double，建议使用 Double.equals()，对无穷和 NaN 安全。 最好使用调用反转，或者使用 Objects.equals() 方法，以保证 null 安全。 子类中的 equals() 方法要先调用父类方法。 hashCode() # hashCode() 和 equals() 必须兼容。如果 equals 为真，hashCode 必须相等。因此，如果重写了 equals()，就必须重写 hashCode()。 最简单的实现方式是使用 Objects 类提供的方法：\nClass Item { public int hashCode() { return Objects.hash(field1, field2...); } } clone() # 如果需要深拷贝，就必须实现一个没有方法的标签接口：Clonable。否则，会抛出 CloneNotSupportedException。这是一个 Checked Exception。\nArrayList 实现的是一个浅拷贝的 clone 方法。因此，如果存储的不是 String 等 Immutable 的对象，得到的两个 ArrayList 将指向相同的元素。而且，这个方法返回 Object 对象，需要经过 cast 来使用。相比之下，数组不需要额外的判断。\n方法的重写也可以借助父类的方法。因此一个完整的实现应该是：\npublic Clazz clone() { Clazz clonedClazz = (Clazz) super.clone(); @SuppressWarnings(\u0026#34;unchecked\u0026#34;) ArrayList\u0026lt;String\u0026gt; cloned = (ArrayList\u0026lt;String\u0026gt;) original.clone(); clonedClazz.original = cloned; return cloned; } 枚举 # 枚举类型 # public enum Size { SMALL, LARGE }; 枚举类型是单例的，可以直接用 == 比较。类型已经包含了 toString 和 valueOf 方法。如果给定的字符串没有枚举类型，会抛出异常。可以获取所有的枚举实例：\nSize[] allVals = Size.values(); ordinal 方法用来获取枚举类型的序号。\nSize.SMALL.ordinal() == 0; //True 枚举类型还自动实现了 Comparable\u0026lt;E\u0026gt;。技术上来说，每个枚举类型都继承了 Enum\u0026lt;E\u0026gt;，并从中得到了以上方法，及 compareTo getDeclaringClass（用于有实现体的枚举）方法。\n枚举类型的实现 # 构造方法 # 枚举类型中的每一个枚举常量，如 SMALL，都是该类型的一个实例。因此，如果为类型定义构造函数，就需要为每个枚举提供参数。\npublic enum Size { SMALL(\u0026#34;S\u0026#34;), LARGE(\u0026#34;L\u0026#34;); private String abbr; Size(String abbr) { this.abbr = abbr; } public String getAbbr() { return abbr; } } 枚举的方法体 # 也可以为每个枚举常量实现方法，只要在类中进行了定义。\npublic enum Operation { ADD { public int eval (int arg1, int args) { return args1 + arg2; } }, MINUS { public int eval (int arg1, int args) { return args1 - arg2; } }; public abstract int eval(int arg1, int arg2); } 这里，实际上每个枚举常量都是该类的一个匿名子类的对象。\n静态成员 # 枚举常量在静态成员构造之前构造。因此，枚举常量的构造函数中不能访问静态成员，它们此时还不存在。静态成员的初始化应当在静态块中完成。\n运行时类型信息 # Class 类 # Class\u0026lt;?\u0026gt; c1 = \u0026#34;\u0026#34;.getClass(); //最好不要省略\u0026lt;?\u0026gt; Class\u0026lt;?\u0026gt; c2 = Class.forName(\u0026#34;java.lang.String\u0026#34;); // throws ReflectiveOperationException Class\u0026lt;?\u0026gt; c3 = String.class; Class\u0026lt;?\u0026gt; c4 = Runnable.class; Class\u0026lt;?\u0026gt; c5 = int.class; Class\u0026lt;?\u0026gt; c6 = void.class; 可以看到，class 对象不仅可以表示通常的类，还可以表示接口、基本类型和 void。严格来说，\u0026ldquo;类型\u0026rdquo; 而非 \u0026ldquo;类\u0026rdquo; 是一个更合适的名字。\nforName 方法可以用来生成那些在编译期还不知道的对象。比如，可以吧类名存在外部文件中，读取后通过 forName 来生成对象，这样就构成了一个动态框架。\nClass.getName() 方法可以得到与之前我们见到的 toString 方法类似的格式：\nString.getClass().getName(); //java.lang.String int[].getClass().getName(); //[I String[].getClass().getName(); //[Ljava.lang.String String[].getClass().getCanonicalName(); //java.lang.String[] 但是，在使用 forName 方法时，只能使用传统的 [I 形式。\nClass 类的一些方法 # Class 对象拥有一系列的 getXxxName 方法和一系列的 isXxx 方法，用于得到类名及检测类是否为数组、枚举、注解、嵌套类等。此外还有：\nClass\u0026lt;? super T\u0026gt; getSuperClass(); Class\u0026lt;?\u0026gt;[] getInterfaces(); Package getPackage(); int getModifiers(); 其中，getModifiers 得到的返回值可以用 java.lang.reflace.Modifier 类中的 toString(int) 方法和 isXxx(int) 方法来处理。类似这样的关于类的属性的方法还有很多。\nClass 类的一个重要作用是定位资源。例如，将 config.txt 和 .class 文件放在同一个目录下，一起打包成 jar，就可以这样使用：\nInputStream stream = MyClass.class.getResourceAsStream(\u0026#34;config.txt\u0026#34;); 另外，还有 getResource 方法返回一个资源的 URL，用于一些遗留方法。\nClass Loader # Class Loader 读取字节码，并将它们转化成一个类型。通常，JVM 使用三个 Class Loader：\nbootstrap Class Loader 加载 Java 类库。 扩展 Class Loader 加载 \u0026ldquo;标准库扩展\u0026rdquo; 部分，在以前的版本中位于 jre/lib/ext。 系统 Class Loader 加载应用程序类，在 class path 中进行查找。 安全起见，bootstrap ClassLoader 不能在代码中被获取到。并且，只有这个 Class Loader 能够加载 java 包下的核心类。String.class.getClassLoader 返回 null。在 Oracle 实现中，另外两个 ClassLoader 都是用 Java 实现的，为 URLClassLoader 的实例对象。因此，可以使用：\n((URLClassLoader) Main.class.getClassLoader()).getURLs() 得到 class path 中内容的 URL。\n关于 Class Loader 的一些问题 # 考虑这样一种情况。有这样一个工具方法：\npublic class Util { Object createInstance(String className) { Class\u0026lt;?\u0026gt; c1 = Class.forName(className); ... } } 这个类由 Class Loader A 来加载。现在，我希望从另一个由 Class Loader B 来加载的类来调用这个方法，而我们想要加载的 jar 位于 B 的 class path 下，并不在 A 的 classpath 下。由于 Util.creatInstance 方法调用的是自己所在的 CLass Loader A，这个 jar 包将无法被找到。因此，需要使用：\npublic Object createInstance(String className, ClassLoader loader) { Class\u0026lt;?\u0026gt; c1 = Class.forName(className, true, loader); } 或者，如果调用此方法的过程位于另一个线程里，可以使用：\n// caller Thread.currentThread().setContextClassLoader(loader); // the instanciate method public class Util { public Object createInstance(String className) { Class\u0026lt;?\u0026gt; c1 = Class.forName(className, true, Thread.currentThread().getContextClassLoader); } } 这是因为每个线程持有自己的上下文加载器（Context Class Loader），默认的总是继承与创建线程，通常为系统 Class Loader。调用结束后，应该把 Class Loader 修改回来。\nService Loader # Service Loader 是一种为加载实现共同接口的不同插件提供方便的方式。例如，我们有一个接口和一个实现：\npackage com.test.crypt; public interface Cipher { byte[] encrypt(byte[] source byte[] key); } //--------------- package com.test.crypt.impl; public class AESCipher implements Cipher { ... } 然后，我们提供一个文本文件：META-INF/services/com.test.crypt.Cipher，其中包含了实现的类名：\ncom.test.crypt.impl.AESCipher 这样，就相当于为一种服务提供了一个实现。然后，在程序代码中可以这样使用：\n// 通过Cipher.class指示ServiceLoader去寻找提供的实现 public static ServiceLoader\u0026lt;Cipher\u0026gt; cipherLoader = ServiceLoader.load(Cipher.class); // 遍历所有配置中提供的实现 for (Cipher c : cipherLoader) { ... } 显然，ServiceLoader 只应该被初始化一次。仅需一次，需要的类就被加载进内存了。\n反射 # 反射可以在运行时检查任意对象的内容，通常用于对象关系映射和 GUI。需要注意的是，它运行起来要更慢。\n检查类和对象 # Class\u0026lt;?\u0026gt; c1 = Class.forName(className); // print all methods while (c1 != null) { for (Mehod m : c1.getDeclaredMethods()) { ... } c1 = c1.getSuperClass(); } // checkout all variables in an Object Object obj = ...; for (Field f : obj.getClass.getDeclaredFields()) { f.setAccessible(true); // may be denied by security manager, no by default Object val = f.get(obj); // Field f in obj f.setDouble(obj ,0.1); // Modify the field ... } // checkout methods Person p = ...; // for method \u0026#34;setName(String)\u0026#34; Method m = p.getClass().getMethod(\u0026#34;setName\u0026#34;, String.class); // check out Method[] like above is also possible // call the method m.invoke(obj, \u0026#34;...name\u0026#34;); // construct an object with no param Object obj = c1.newInstance(); // construck with param Cunstructor constr = c1.getConstructor(c1); Object obj = constr.newInctance(param); JavaBean 和数组 # 对于 Bean 对象，反射系统还提供了更具体的一系列方法\nBeanInfo info = Introspector.getBeanInfo(c1); PropertyDescriptor[] props = info.getPropertyDecsriptors(); for (PropertyDescriptor prop : props) { prop.getName()...; } 对于数组，同样有一类方法：\nc1.isArray(); // true c1.getCompanentType(); // content class 代理 Proxy # Proxy 类用于使用反射在运行时创建一个实现某个接口的类。由于我们不能在运行时直接编写新代码，需要一个调用处理器，即一个实现了 InvocationHandler 接口的对象。下面实现一个在 Comparable 接口基础上添加调试信息的过程。\npublic interface InvocationHandler { Object invoke(Object proxy, Method method, Object[] args); } Object value = ...; value = Proxy.newProxyInstance (null, value.getClass().getInterfaces(), //lambda expression that implements InvocationHandler (Object proxy, Method m, Object[] margs) -\u0026gt; { System.out.println(value + \u0026#39;.\u0026#39; + m.getName() + Arrays.toString(margs)); return m.invoke(value, margs); }); "},{"id":10,"href":"/notes/intro-algo/3/","title":"3. LinkedList, HashTable","section":"Introduction to Algorithms","content":" 链表 # 链表的哨兵结点 # 链表的哨兵结点表示 nil 值，其 prev 属性指向表尾，next 属性指向表头。这样，就可以省略掉 head 属性，并简化边界条件的处理。如果我们使用的是很多个很短的链表，哨兵结点就会造成比较严重的存储浪费。\n不使用指针的链表 # 多数组的实现：对于双向链表，至少使用 3 个数组，分别为 prev、key、next，其值即为所指向对象的数组下标。每个数组相同下标的值合起来是一个完整的结点对象。\n单数组的实现：以一整个数组连续存储对象，使用下标代表指针。当需要访问对象的成员时，在指针上加一个偏移量。相对于多数组的实现，这种方式就可以支持不同长度的对象构成的链表。\n自由表：未被使用的，可能是之前被释放的内存单元组成的链表。。数组表示中的每一个对象不是在链表中，就一定在自由表中。实现上，自由表常常是一个链表栈。刚刚被释放的空间，在下一次插入中就会被用来存储新的对象。显然，多个链表也可以共用同一个自由表。使用自由表的释放操作和插入操作运行代价仍然是 $O(1)$，因此非常实用。\n有根树的表示 # 对于分叉数量未知的树，我们难以使用数组来储存孩子结点的指针。或者，如果最大孩子数很大，那么使用相同数量的指针空间将会浪费大量的存储空间。因此，在这里引入左孩子右兄弟表示法。\n在这样表示的树中，每一个结点有三个指针：父结点，左孩子指针和一个兄弟指针，兄弟指针指向它右侧的具有同一个父结点的结点。同一个父结点的所有孩子结点实际上相当于构成一个链表。如果是最右子结点，就把兄弟指针设置为 nil。\n散列函数 # 散列函数所需要的最基本性质是，尽量让 key 进入各个槽的概率平均。除此之外，还可能需要一些其他的性质。比如，可能希望相接近的关键字的散列值差距较大，在开放寻址法进行线性探查时需要这种性质，而这种性质由全域散列提供。此外，还可能需要把其他种类的关键字，或负数、浮点数等转换成自然数等。\n除法散列 # 最简单的除法散列适用于平均分布的自然数序列。被除数选择不接近 2 的整数幂的较大的质数有利于散列。如对于一个预备存储 2000 个元素的散列表，可取 $h(k) = k \\mod 701$。\n乘法散列 # 用关键字乘一个常数，通常为一个无理数，取小数部分，再乘上一个值，取整变回自然数。如：\n$$ h(k) = \\lfloor m(kA \\mod 1) \\rfloor $$\n乘法散列对 $m$ 的值并不挑剔，一般取为一个 2 的幂，这样在计算机内部可以直接通过取一个数的高位来获得散列值。$A$ 的一个比较理想的值为 $\\sqrt5-1 \\approx 0.618033 \\cdots$。\n全域散列 # 全域散列的思想是通过随机选择散列函数，避免最坏情况的，即所有元素都放置在同一个槽的情况出现。\n完全散列 # 完全散列的最坏情况查找只需要 $O(1)$ 次访存。一种完全散列的方法是，使用两层散列表。通过精心设计第二层散列函数，使得在第一集中落到同一个槽中的元素在第二级不再出现冲突。为了达到这个目的，第二层的槽数需要为散列到该槽中的关键字数的平方。\n散列冲突的解决 # 链接法（拉链法） # 每个散列的槽对应的是一个链表，其中存储所有该散列值的元素。如果要不重复地插入元素，或者删除指定 key 的元素，显然需要搜索整个链表。因此，操作的时间代价取决于链表的长度。\n定义散列表的装载因子，即元素数与槽数的比 $\\frac n m$ 为 $\\alpha$。链表的查找时间取决于链表的长度，显然，最坏情况下整个散列表的查找时间为 $\\Theta(n)$。每个槽链表长度期望等于 $\\alpha$，时间代价期望为 $\\Theta(1+\\alpha)$。这是建立在散列完全均匀的假设下。\n开放寻址法 # 开放寻址法意味着散列表中不存在链表，表有可能被填满。当出现散列冲突时，就根据一定的原则继续寻找下一个可以存储的位置。常见的探查方法有三种：线性探查、二次探查和双重探查。在以下的表述中，我们将未加入探查功能的散列函数称为辅助散列函数，以 $h\u0026rsquo;$ 表示。\n线性探查 # $$ h(k,i) = (h\u0026rsquo;(k)+i) \\mod m $$\n式中 $i$ 为探查的次数。线性探查是在失败之后，线性地依次查看后面的槽位，直到找到空的槽为止。线性探查容易实现，但容易发生群集。即，被占用的槽很可能形成连续的长序列，当辅助函数落到这个序列的头部时，就需要相当长的探查序列。\n二次探查 # $$ h(k,i) = (h\u0026rsquo;(k) + c_1i + c_2i^2) \\mod m $$\n线性探查相当于这种方式在常数 $c_1=1,c_2=0$ 时的情况。由于探查位置二次依赖于 $c_2$，探查序列不容易过于群集。\n双重散列 # $$ h(k,i) = (h_1(k) + ih_2(k)) \\mod m $$\n双重散列是开放寻址的最好方法之一，需要注意的是 $h_2(k)$ 必须与 $m$ 互质。一个方法是，取 $m$ 为 2 的幂，并设计一个永远产生奇数的 $h_2$。另一种方式是，取 $m$ 为质数，并让 $h_2(k)$ 略小于 $m$。前两种方法可能的探查序列有 $\\theta(m)$ 种，而双重散列将其提升到了 $\\Theta(m^2)$ 种。\n开放寻址的性能分析 # 仍旧使用装载因子 $\\alpha$ 的概念。不过，在开放寻址中，$\\alpha$ 始终小于 1。和之前一样，$i$ 是探查的次数。于是，探查次数期望的上界：\n$$ \\mathrm E[X] = \\sum_{i=1}^\\infty \\Pr\\{X \\geqslant i\\} \\leqslant \\sum_{i=1}^\\infty \\alpha^{i-1} = \\frac 1 {1-\\alpha} $$\n实际上，一次成功查找的探查期望次数是：\n$$ \\frac 1\\alpha\\ln\\frac 1{1-\\alpha} $$\n如果散列表是半满的，探查的期望小于 1.387. 如果散列表 90% 满，期望小于 2.559.\n"},{"id":11,"href":"/notes/ddia/3/","title":"3. Replication and Partition","section":"Designing Data-Intensive Applications","content":" 数据复制 # 多副本的目的：\n扩展性，提高读写负载能力 容错与高可用 到端上的延迟，如CDN Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。\n另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。\n相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。\n将数据分布在多个节点上时，有两种方式：复制和分区。\n本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。\n主从复制 # 主节点接收所有写请求，再分发给从节点 从节点的写入顺序和主节点相同 读取时可以请求主节点或从节点 主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。\n同步复制和异步复制 # 主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。\n实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。\n全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。\n链式复制是其中一种折中方案。\n配置新的从节点 # 需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。\n处理节点失效 # 如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。\n如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。\n可能存在的问题：\n异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。 其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露） 脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。 如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。 复制日志的实现 # 基于语句复制 # 如果语句使用了 RAND() NOW() 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式\n如果语句的写数据依赖读数据（INSERT SELECT FROM，UPDATE WHERE 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。\n其他有副作用的语句，对外部的副作用可能不同\n基于 WAL 复制 # SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。\n造成数据和存储引擎紧密耦合（如 B-Tree 的 WAL 包括很多树节点粒度的操作，不能直接适配到其他数据结构）\n造成数据和存储引擎版本紧密耦合，需要停机升级\n基于行的复制 # 提供一种和 WAL 不同的逻辑日志，如 binlog\n方便外部处理（如 dump 到 Hive，或另外构建索引和缓存）\n基于触发器复制 # 主节点触发外部触发器，可以执行过滤、处理等，再同步到从节点。 通常比数据库内置的方案开销更好，错误风险更大，但更灵活 复制滞后问题 # 异步复制下，从从节点可能读到过期的数据。考虑到数据最终会追赶上，这种效应被称为最终一致性。然而，数据同步的延迟理论上并没有上限。\n读自己的写 # 写入后马上从从节点读取自己刚刚写入的数据，可能读到过期的数据。保证这种读写一致性（写后读一致性）的方案：\n对于可能被修改的内容，从主节点读取。例如，只有用户自己能够修改用户资料，所以读取自己的用户资料时从主节点读，读取他人的可以从从节点读。 确定一个时间上限，例如发生修改后一分钟内一直从主节点读，同时对滞后超过一分钟的从节点暂时停止访问 客户端保留写入时的时间戳，与读出的数据中时间戳比较。如果发现不一致，就重试。时间戳可以是数据库的日志序列号，也可以是真实时间（需要考虑不可靠的本地时间） 对于跨地区多数据中心，需要把需求路由到主节点所在的机房 用户还可能在 Web 上写，在移动端读，这时就不能依赖客户端的一致性（如时间戳）。\n单调读 Monotonic Read # 如果一个用户先从进度较快的节点读一次，又从进度较慢的节点读一次，就可能看到第二次读取的值反而更老。单调读一致性的简单实现是让同一个用户永远读同一个从节点，例如通过哈希。但如果这个节点失效，就只能路由到其他副本。\n前缀一致读 Consistent Prefix Read # 对一系列写请求，需要按照相同的顺序读取。例如，事件 A 依赖事件 B，但某个副本上 B 比 A 更早得到同步的不一致情况。避免这种情况发生的一致性保证是前缀一致。当数据存在分区而两个事件存在不同的分区时，这个情况就容易发生。直觉的解决办法是追踪事件之间的因果关系，但这一点的实现并不容易。\n复制滞后的解决方案 # 这些问题的解决有两种思路：\n在应用层解决，比如上面的仅从主节点读取自己的用户资料的做法。但这种做法不太通用，且比较复杂，容易出错。 在数据库侧做这样的保证，即事务。单机事务已经比较成熟，分布式数据库则很多放弃了支持事务。 多主节点复制 # 显然，主从复制中如果主节点故障，所有的写操作都会中断。如果允许多个主节点，主节点之间互相同步，就可以避免这种情况。\n适用场景 # 在单个机房内部使用多主节点的意义不大，因为其实现比较复杂而机房内故障率相对低。对于跨机房数据库，如果所有写操作都只能通过主节点，就失去了多机房部署就近访问的意义。\n因此，在每个机房设置一个主节点，各个机房之间进行主主同步，机房内部进行主从同步。这样做的优点是提升性能（本地访问，异步同步到其他机房），容忍机房级别的故障和跨机房专线级别的故障。当然，这种部署需要解决写冲突的问题，且和自增主键、触发器、完整性约束等功能共同使用时的陷阱还比较多。\n使用多主复制的数据库包括：MySQL 的 Tungsten，PostgreSQL 的 BDR 和 Oracle 的 GoldenGate。\n此外，一些应用设计也需要类似分布式数据库的同步。例如跨平台的日历应用，允许在没有网络时修改日程，此时就相当于有一个本地的主节点，需要再网络恢复时进行同步。文档的协作编辑也是一个典型的例子。\n处理写冲突 # 有些冲突比较显然，比如同一行在两个不同的节点上被写不同的值；有的冲突比较隐晦，例如同一个会议室被预定两次：在两个节点上分别预定时，两个节点都认为这个会议室是空的。\n避免冲突，例如上面提到过的，对同一个文档路由到同一个主节点，对单个文档就变成了主从复制模型。但是，这仍然要面临数据中心故障或用户离另一个数据中心更近，一定要路由到其他数据中心的情况。\n收敛到一致状态\n给每个写入分配唯一的 ID，如时间戳、哈希、UUID 等，选择最晚 / 最大的，即最后写入者获胜。这种方法比较普遍，且在很多场景下适用，但会造成写入较早的一方数据丢失 为每个节点分配一个 ID，取 ID 最大的节点的版本，问题同上，甚至更加无法和业务场景契合 合并冲突的值 保留冲突为一个特殊的结构，然后依靠应用层或通知用户来解决（想象异步的 Git Merge） 自定义冲突解决逻辑\n从实用角度来讲，最合适的方法仍然是在应用层解决冲突，让数据库的使用者来编写冲突解决逻辑。这个逻辑可以是在写时执行或读时执行 通常，这个解决逻辑的粒度是单个行（对于关系型数据库）或单个文档（对于文档型数据库），因此对于包含多个写的原子事务存在一定局限性 自定义冲突解决逻辑很难以编写，需要考虑不同的业务场景。有一些自动冲突解决的探索：\nCRDT (Conflict-free Replicated Data Types) ，是一些可以多个用户同时编辑的数据结构，如 map、list、counter 等。一些在 Riak 中已经被实现 Mergable Persistent Data，使用类似 Git 的方式跟踪变更历史，使用三向合并(Three-way merge function）。相比之下，CRDT 是双向合并。 Operational transformation，是 Google Doc 使用的冲突解决算法，适用于可同时编辑的有序列表。 拓扑结构 # 多个主节点的同步方向可以有不同的拓扑结构：星形，环形，全连接型。\n星形和环形的问题是，每个数据需要经过多个节点才能同步完成，且单个节点宕机仍然会影响同步，需要重新配置拓扑结构。\n全连接的问题是，每条链路的速度不一定一致，可能造成某一行的 UPDATE 操作比 INSERT 更早抵达之类的情况。\n一种解决方案是使用版本向量。总的来说，在许多多主系统中，冲突检测技术还很不完善。\n无主节点复制 # 节点失效时写入数据库 # Dynamo 风格数据库，Riak、Cassandra、Voldemort 都使用类似的风格。客户端或客户端直接访问的协调者将写请求同时发送到多个节点，通过节点间的同步交互来进行冲突处理。在读请求的时候，也是同时读多个节点，经过协调确认最终结果。\n一个失效的节点重新上线后，进行同步的方式：\n读修复：在读取时，从某些节点读到旧数据，另一些读到新数据，就把新数据写入到旧数据的节点里，相当于加快同步。 反熵过程：由后台进程不断查找副本间的差异并同步。这个过程并不保证数据同步的顺序，同步滞后可能很长 如果系统只实现了读修复而没有实现反熵，那么没有被读取到的数据就会长期保留旧数据的状态，相当于副本数偏少，节点故障的容忍能力下降。\n读写 Quorum # 如果有 n 个副本，写入需要 w 个节点确认，读取至少读 r 个节点，那么当 w + r \u0026gt; n，就可以确保读到最新值，这种方式称为仲裁读和仲裁写。\n通常设置 n 为一个奇数 3 或 5，w = r = (n+1)/2。注意这不代表集群只有 n 个节点。\n通常读写请求都是同时发送到全部 n 个副本，w 和 r 只是读取和写入成功的判断标准。\n当 w \u0026lt; n，在写入时可以容忍节点不可用 当 r \u0026lt; n，在读取时可以容忍节点不可用 当 n = 3, w = r = 2，可以容忍一个节点不可用 当 n = 5, w = r = 3，可以容忍两个节点不可用 Quorum 一致性的局限性 # 上面的 w + r \u0026gt; n 的限制可以保证一定可以读到最新值，同时也确定了对节点失效的容忍能力。如果放开这个限制，那么就有可能读到旧值，降低一致性，但同时也可以提升性能（更低的延迟）和可用性（更高的节点失效容忍能力）。\n即使约定 w + r \u0026gt; n，也可能存在一些局限：\nsloppy quorum 写冲突 如果在写入过程中进行读取，数据可能尚未抵达足够多的节点 如果一次写入的部分节点成功，而部分节点失败，总的成功数量没有达到 w，整个写请求被认为是回滚状态，那么那些已经写入完成的节点上就存在了异常的值 如果某个存有新值的节点失效，恢复时从存有旧值的节点同步数据，就打破了上面 w 个副本的要求 其他异常情况，参见第九章“可线性化与 Quorum” 整体来说，我们仍然把 Dynamo 风格的数据库看作最终一致性的数据库，而不认为上面这些归档能够有真正的数据一致保证。\n即使应用本身可以接受读到旧值，监控数据库的同步状态仍然有很大运维意义。对主从复制，因为同步存在顺序，可以直接通过偏移量监控。对于无主复制的系统，不仅没有同步顺序，如果只使用读修复，值的同步延迟就是无上限的。我们可以基于 w r n 的值对旧值的比例给出一个估计。\nSloppy Quorum # 如果一个大集群发生了网络分区，无法访问到 w 个节点，应该如何处理写请求？一个方案是，将写请求暂时写入到 n 个节点之外的可访问节点（其他分区的节点）上。这时我们认为写入和读取仍然需要 w 和 r 个节点，但不一定是原来的 n 个节点之内的。当网络得到恢复，就要把这些临时数据放回应该在的位置。\n显然，这样做会失去读到最新的保证，同时也大大提高了写入可用性。Riak 默认开启，Cassandra 和 Voldemort 默认关闭。\n跨数据中心的 Quorum # Cassandra 和 Voldemort 的做法是将副本数 n 分布到各个机房，写入时会同时向本地机房和远端机房发送写请求，但只等待本地机房的写入结果，从而同时获得容灾能力和低延迟。\nRiak 的做法是在每个机房单独配置集群，在各个集群之间采用类似多主复制的方式来同步。\n检测并发写 # 和多主复制一样，Quorum 机制不能解决写冲突的问题。\n在使用 LWW（Last Write Wins）的方法时，在多节点同时写的情况下，如果不确定一个规则，就可能永远无法达成一致。但我们事实上很难确定事件发生的自然顺序，因此人为指定一个排序即可。这样可以达到最终一致性，但带来数据丢失。\n当两个操作互相之间没有依赖关系（happens-before），就是并发的。\n先考虑一个简单的情况：一个单副本的购物车数据库，在两个客户端并发写入。于是数据库会维护两个版本的数据，客户端在每次读取时带上自己已知的上一个版本，从而让数据库知道应该覆盖哪一个版本的数据；同时数据库会将两个版本都返回给客户端，让客户端自己来处理两个版本的合并。\n这样，我们同时获得了最终一致性，也没有发生值的丢失。\n在确定合并逻辑时可以使用版本向量、软删除墓碑、CRDT 等方法。\n数据分区 # 数据分区与数据复制 # 一个节点可能同时既是某个分区的主副本，又是其他分区的从副本。\nKV 数据的分区 # range分区，手动或自动。存在查询热点问题，如大部分请求读的都是最新的数据。可以先按其他字段分区，再按时间分区。 hash分区 hash函数需要是一致的（反例：Java和ruby的hashcode在不同进程中不同），但不需要加密性很强 丧失区间查询能力 Cassandra 的折中：复合主键的第一个字段用哈希，剩下的排序用range，确定第一个主键后剩下的可以区间查询 一致性哈希（consistent hash）解决的问题：对朴素的mod n哈希分区，如果增加一个分区，那么有大量的数据会改变前后所在的节点，需要大量数据迁移或缓存失效；一致性哈希在扩容时不改变大部分分区的hash范围，仅变更扩容处的，解决这个问题；ddia称目前并不常用，实际效果不好。\n哈希仍有不能解决的倾斜，如明星热点事件，单个ID有大量访问。简单的办法是增加一个随机数拆分，查询完再合并。\n分区与二级索引 # HBase，Voldemort 等 kv 数据库不支持二级索引，Riak 等正在尝试增加；对于 Solr 和 Elastic Search 等，二级索引是其存在的根本意义。\n基于文档的二级索引：每个分区一个二级索引表，对每一条新纪录，插入时在二级索引中添加这条新记录的ID。需要确保二级索引和kv的一致性。 在二级索引上查询时，需要查询所有分区的二级索引，再合并，称分散/聚集（scatter/gather），写放大严重，延迟高 Mongo，Riak，Cassandra，ES，Solr，Volt等使用 基于词条的二级索引：全局使用同一个二级索引，二级索引本身也分区，索引本身可以使用range分区 不需要读取所有分区 写入慢且复杂 数据和索引一致性和写入延迟之间需要权衡。如果要同步索引，则一个事务需要至少读写两个分区（KV 和索引），因此目前数据库都不支持同步索引 分区再平衡 # 直接取模会遇到大量数据迁移\n固定数量分区：提前确定数量超多的分区，固定数量；节点负载增加时将分区移走但不需要改变哈希 Riak ES Couchbase Voldemont使用这种方式 分区数量固定，大小和数据量成正比 动态分区：允许分区合并拆分，类似b树 HBase 和 Rethink 使用这种方式 对 HBase，拆分的数据迁移依赖 HDFS 当数据库为空时只有一个分区，会造成流量集中在一个节点；解决办法是预分裂 分区大小有上下限（合并拆分），数量与数据量成正比 按节点比例分区：每个节点的分区数量一定 分区数量不和数据量成正比，而是和节点数量成正比 Cassandra 和 Ketama 使用这种方式 符合一致性哈希 再平衡可以是自动的或手动的。自动再平衡节省运维，但遇到故障容易出现负载失控；如果一个节点负载很高，可能将其认为失效，对其进行数据迁移反而进一步加重负载，甚至导致雪崩。\n请求路由 # 随机选择（普通的负载均衡），允许客户端连接到任意节点，再路由到分区所在节点 增加一个路由层，客户端连接到路由层 客户端感知节点分配，直接连接目标节点 共同问题：及时更新分区变更\n配置中心 Linkedin 的 Espresso 使用基于 ZK 的 Helix HBase，Solr，Kafka 使用 ZK Mongo 使用自己的配置中心 Gossip 协议，使用上述的路由方案1 Cassandra，Riak Couchbase不支持自动再平衡，使用路由层 moxi 向集群学习路由变化 使用路由层或随机选择时，仍然需要知道 IP 地址，因为变更不频繁，使用 DNS 就足够。\n"},{"id":12,"href":"/notes/intro-algo/4/","title":"4. BST, Balanced BSTs","section":"Introduction to Algorithms","content":" 二叉搜索树 # 对一个二叉搜索树，任何一个结点的左子结点不大于它本身，右子结点不小于它本身。这样，就可以简单地使用中序遍历查找元素。中序遍历打印出来的序列，就是已经排序完成的序列。中序遍历的时间代价为 $\\Theta(n)$。\n二叉搜索树的基本操作 # 在高 $h$ 的树上，以下操作的时间代价均为 $O(h)$。\n查找：比较和当前结点的大小，选择子树。 最大和最小：不断取左子结点或右子结点。 深度优先遍历的方式：\n前序遍历 中序遍历 后序遍历 后继前驱 # 如果关键字不重复，那么一个结点的中序遍历后继为大于这个结点的最小者，即升序序列中的下一个。如果结点的右子树非空，那么右树中的最左结点即为后继结点，不断向左寻找即可。\n如果右子树为空，说明这个结点是某个左子树的最右结点，而这个左子树的父结点即为后继结点。这意味着，遍历这个结点之后，这个左子树遍历完成，进入某个遍历过程的根结点部分。于是，不断向上寻找，如果当前结点不再是右结点，说明已经找到了这个根结点。如果找到了 $nil$，则说明没有后继，这个结点是整个树的最右结点。\n前驱和后继的过程对称，时间代价均为 $O(h)$。\n插入和删除 # 插入过程比较简单。寻找结点的关键字应该在的位置，并修改父结点的指针即可。\n删除结点可分为三种情况：\n没有子结点，直接删除并修改父结点的指针即可。 只有一个孩子，则用这个孩子来替代这个结点。 两个孩子，则使用后继结点来替代这个结点。由于被删除的这个后继结点是右子树的最左结点，其一定没有左子节点。因此，使用其右子节点代替它的位置，并用这个节点代替待删除的结点。于是，删除完成。 显然，这两种操作的时间代价也是 $O(h)$。\n随机构建二叉搜索树 # 二叉搜索树的构建由插入和删除操作完成。显然，实际情况中，这是一个随机过程。在最坏情况下，当元素严格升序或降序插入，二叉搜索树将成为一个链表。十分显而易见的是，在完全随机的情况下，元素均匀插入，二叉树接近完全，其高度的期望为 $O(\\log_2n)$。\n红黑树 # 性质 # 一个基于二叉搜索树增加一个颜色属性的树，保证没有任何路径会比另一条路径长出二倍的近似平衡树。\n每个结点是红色或黑色的 根结点是黑色的 每个叶子结点都是黑色的 如果一个结点是红色的，则其两个子结点都是黑色的 对每个结点，从该结点到其所有后代叶结点的简单路径上，均包含相同树木的黑色结点。 性质 5 是红黑树的核心：红黑树是一颗近似平衡二叉树。\n为了避免叶子结点的空间浪费，可以指定一个哨兵结点为 NIL，并使所有叶子都指向这一个结点。通常忽略叶子结点，因为其并不储存 key 值。\n定义任何一个结点的黑高为从该结点到其后代叶子结点的路径上黑色结点的数目。根据性质 5，任何一条简单路径的黑高都相同。一颗有 $n$ 个内部结点的红黑树的高度至多为 $2\\log_2(n+1)$。\n旋转操作 # 显然，红黑树的查找和一般的二叉搜索树一样。对于插入和删除，为了维护红黑树的性质，需要进行旋转操作。两种操作的时间代价均为 $O(\\log_2n)$。旋转操作分为左旋和右旋。旋转的目的是改变两棵子树的高度。旋转前后二叉搜索树的性质不变。可以看到，旋转前后，除 P、Q 两个结点之外的其他三个之间的左右顺序不变，这保持了二叉搜索树的性质。\n插入 # 将要插入的结点按照正常二叉搜索树插入，这时有可能破坏性质 4、5。我们把这个结点标记为红色，于是只可能违反性质 4。然后，调用一个过程，从这个新结点开始进行红黑树的调整。我们将所有情形概括为 5 种：\n如果新插入的结点是整棵树的根结点结点，显然直接将其涂为黑色即可。\n或者，新结点的父结点是黑色。这时，红黑树的任何性质都没有受到影响，无需任何操作。否则，为了保证被插入结点的父结点左右黑高相等，我们就需要进行旋转调整。旋转调整分为以下三种情形：\n父结点和叔父结点（父结点的兄弟结点）都是红色的。为了满足性质 4，将这两个结点都涂成黑色，并将祖父结点涂成红色。这时，新加入的结点和两个父辈结点都能保证符合红黑树的性质，而被涂红的祖父结点不一定。将祖父结点当成新加入的结点，递归地从情形 1 开始进行整个检查过程。\n父结点是红色，叔父结点为黑色或不存在（实际上是黑色的哨兵结点），并且，子结点方向不一致。形象地说，新结点、父结点、祖父结点不在同一条直线上，有一个拐弯。此时，首先进行一次旋转，将其与其父结点 P 对调。三代结点现在位于一条直线上，都是左子结点或右子结点。随后，把被调换到下面的 P 当做新加入的结点，进行情形 5 的操作。 父结点是红色，叔父结点为黑色或不存在，并且新结点 N 和其父结点 P 都是各自父结点的同一个方向的子结点。即，三代结点在一条直线上。对新结点的父结点和祖父结点进行一次旋转。即，将这条直线的中间一点变成高点。这个过程，实际上是由于无法直接通过着色来满足性质 4，于是选择通过旋转来改变新结点的父结点。 所有的情形，使用的都是尾递归。因此，整个算法是一个原地算法。算法的时间代价为 $O(\\log_2n)$，且最多发生两次旋转，即情形 4-5 的路径。\n删除操作 # 对任何一个结点，它本身有红色和黑色两种可能，子结点的情况有 0、1、2 三种可能，得到以下的这些情况：\n被删结点为红色，没有子结点：直接删除即可。\n被删结点为黑色，并有一个子结点。这时，这个子结点必然为红色。于是，交换这两个结点并改变颜色，就可以安全删除。\n被删结点为红色，并有一个子结点。这种情况不可能存在。\n被删结点有两个子结点。这时，首先使用与二叉搜索树一样的方法，用后继结点替代这个结点。在这里只对 key 进行替代即可。此时，实际上就变成了删除后继结点的情况，可以进行递归讨论。\n接下来讨论被删结点为黑色且没有子结点的情况。以下所有的图中，N 为被删除的结点的位置，P 是父结点，S 是兄弟结点。\n兄弟结点为黑色，且有一个与兄弟结点方向一致的红色子结点。这时，进行旋转操作。 兄弟结点为黑色，且兄弟结点有一个方向不一致的红色子结点。这时，先将兄弟结点进行一次旋转，转化为第 1 种情形。 兄弟结点为黑色，且兄弟结点没有红色子结点。\n若父结点为红，直接对兄弟结点和父结点重新着色即可。\n若父结点为黑色，则对兄弟结点标为黑色，并对父结点进行递归判断。\n兄弟结点为红色。此时，父结点一定是黑色。对父结点和兄弟结点进行一次旋转并重新上色。 AVL 树 # AVL 树是另一种二叉平衡树。由于这种树的维护代价比较高，在实际应用中并不常见。不过其操作的复杂度同样为 $O(\\log_2n)$。在 AVL 树中，两个分支的高度最多相差 1。AVL 树的旋转操作与红黑树类似，只是对于每个结点，不再有红黑属性值，而是变成了高度值（从一个布尔值变成一个整型）。另一种实现是存储两个子树高度的差，此时这个值的范围是 -1、0 或 1。\n由于所有路径的高度最多相差 1，从任何一个结点向上到根的路径上，最多有两个不平衡的结点（只有一个子结点）。在改变结点之后，如果路径上存在两个不平衡的结点，就需要调整。\n插入 # 我们把插入之后得到的情况分为 4 种，分别称为 LL、LR、RL、RR。通过旋转操作，将树重新平衡。在按照二叉搜索树的方法插入新结点后，要从新的结点一直回溯到根，逐个遍历，判断是否需要进行旋转。\n删除 # 与一般的二叉搜索树一样，将含有两个子结点的结点的删除操作转化为其前驱或后继结点的删除。随后，同样回溯到根，检查是否出现失衡。\nB 树与红黑树 # 2-3 树（3 阶 B 树） # 我们将结点分为两种，2 - 结点有一个 key，并有两个子结点。3 - 结点有两个 key，有三个子结点并保持类似于二叉搜索树的性质。注意，2-3 树中每一层都必须是完全的，也就是不存在 NIL。在向 3 - 结点进行插入时，结点分裂，挤出一个 key 给父结点，并向上回溯。2-3 树是一个绝对平衡的树，操作的复杂度为 $\\Theta(\\log_2n)$。\n2-3-4 树与红黑树 # 2-3-4 树的结构和 2-3 树类似，只是最大的结点变成了 4- 结点。可以看到，这类树的实现比较复杂。现在考虑：将红黑树的所有红色的边放平。并合并。于是，红黑树的黑高变成了 2-3-4 树的高，红色的结点与其父结点合并变成了 2-3-4 树的结点。于是，红黑树与 2-3-4 树是等价的。\n"},{"id":13,"href":"/notes/core-java-impatient/4/","title":"4. Exception, Logging","section":"Core Java for Impatients","content":" 异常 # 异常对象 # RuntimeException 和 CheckedException 继承于 Exception，Exception 和 Error 继承于 Throwable。Error 和 RuntimeException 均属于 Unchecked Exception。\nChecked Exception 和 Unchecked Exception 在意义上是存在区别的。例如，有关 IO 和类加载的异常大多是 Checked Exception，因为编写代码时无法判断该异常会不会发生。唯一的办法是捕获并处理它。相比之下，类似 NumberFormatException 这一类异常的发生是可以在代码中进行避免的。因此，属于 Unchecked Exception，需要在代码中进行预防而非捕获。\n如果调用可能抛出 Checked Exception，就需要在方法头声明，以便找到最终最适合处理的位置。对于 lambda 表达式也同理。如果 lambda 表达式可能抛出 Checked Exception，就无法被传给一个不抛出异常的函数式接口。好在，大多数情况下，抛出的异常都适合在 lambda 表达式里捕获处理。\n在编写自定义的异常类时，建议至少提供两个构造方法：一个无参的方法和一个接受消息字符串的方法。\n异常捕获 # 多种异常 # 对于需要处理多种异常的情况，可以分别或一起捕获：\ntry { ... } catch (ExceptionClass | ExceptionClazz ex) { ex.fun()... } catch (ExClass ex) { ... } try-with-resources # 对于临时需要的资源，使用 try-with-resources 语句，资源需要实现 AutoClosable 接口：\npublic interface AutoClosable { public void close() throws Exception; } public interface Closable implements AutoClosable { public void close() throws IOException; } //use try-with-resources try (PrintWriter out = new Printwriter(\u0026#34;output.txt\u0026#34;)) { ... } catch (Exception e) { //Catch statment is optional ... } 这样，无论是否发生了异常，资源都会被自动调用 close() 方法，以保证资源的释放。如果申请了多个资源，则会按照相反的顺序释放，因此资源之间可以互相依赖。\n当然，close() 方法也可能抛出异常。这个异常同样会在 catch 语句中被捕获。不过，如果 try 块内已经发生了异常，close() 又发生了一个异常，那么后者会被附加到原本的异常里。\ncatch (IOException ex) { Throwable[] closeExceptions = ex.getSuppressed(); } // manually set suppressed exception ex.addSuppressed(e); finally # try { ... } catch (Exception ex) { ... } finally { ... } 无论是否有异常，finally 子句都会在 try 之后被运行。\n由于 finally 语句一定会被运行（存在一些特殊情况，如线程被终止），所以必须避免在 finally 中抛出异常或者返回值。否则，这个异常和返回值就会覆盖掉 try 中的异常和返回值（这里没有上面的 Suppressed 机制）。很多时候，finally 都可以被 try-with resources 取代。仍然需要 finally 的时候，通常是当 try 块中出现了 return break continue 等情况。这时，即使跳出或者返回，finally 都能很好地完成清理工作。\n这篇文章比较详细地讲解了 finally 块的情况。\n异常处理 # 重抛 # 无法处理的异常应该被重抛出去：\npublic void fun() throws IOException try { ... } catch (Exception ex) { ... throw ex; } 链（chaining） # 或者如果你想要改变抛出异常的类型，把它封装起来：\ncatch (SQLException ex) { throw new ServletException(\u0026#34;info\u0026#34;, ex); // Exception that privides constructor } catch (OtherException ex) { Throwble e = new CruftyOldException(\u0026#34;info\u0026#34;); // Exception that don\u0026#39;t provide e.initCause(ex); throw e; } //outer caller catch (ServletException ex) { Throwable cause = ex.getCause(); // can\u0026#39;t figure out what kind of cause it is } 堆栈信息 # 默认，未被捕获的异常会在 System.err 流输出堆栈信息。如果要重定向这个信息，可以针对当前线程进行设置：\nThread.setDefaultUncaughtExceptionHandler((thread, ex) -\u0026gt; { ... }) 即使不知道该如何处理一个异常，至少要把堆栈信息打印出来：\nex.printStackTrace(); //--------------- ByteArrayOutputStream out = new ByteArrayOutputStream(); ex.printstackTrace(out); String description = out.toString(); requireNotNull() # Objects 类有一个 requireNotNull 方法，如果参数为 null，就抛出异常。这样会使 stack trace 中问题的定位更加简单一些。\n断言 # 断言是防御型编程的典型做法。和手动判断的区别是，在生产代码中不需要一个个地去除判断（带来可能的问题）。\nassert x \u0026gt;= 0; assert x \u0026gt;= 0: x; 第二种的 x 会转换为字符串作为错误消息。断言由 Class Loader 处理，所以不需要分别编译 debug 版本和 release 版本。在运行程序时，使用参数来启用断言：\njava -ea MainClass java -enableassertions MainClass # enable assertion for specific class or package java -ea:MyClass -ea:com.test.package MainClass # -disableassertions java -ea:com -da:com.StableClass MainClass # for \u0026#34;System Classes\u0026#34; that are not managed by class loader java -esa java -enablesystemassertions 或者在运行时处理断言的状态：\nvoid ClassLoader.setDefaultAssertionStatus(boolean enabled); void ClassLoader.setClassAssertionStatus(String className, boolean enabled); void ClassLoader.setPackageAssertionStatus(String packageName, boolean enabled); 日志 # 使用日志 # Java 现在引入了日志 API，使用 java.util.logging.Logger 类。\nLogger.getGlobal().info(\u0026#34;info\u0026#34; + filename + ...); Logger.getGlobal().log(Level.INFO, \u0026#34;info\u0026#34;); // prints Time, class, method, level and info // 有些情况难以得到准确的堆栈信息，例如方法被内联的时候 // 这时，可以手动指定信息 Logger.getGlobal.logp(Level l, String className, String methodName, String message); Logger.getGlobal.setLevel(Level.OFF); // create your own logger Logger logger = Logger.getLogger(\u0026#34;com.test.000\u0026#34;); 即使日志 Level 设置为更低，上面的字符串连接过程也要进行。如果要避免这个代价，可以使用 lambda 表达式：\nLogger.getGlobal().info(() -\u0026gt; \u0026#34;info\u0026#34; + filename + ...); 另外，为了更好地管理与包有关的日志，日志的父子之间会存在一些关系。例如，如果 com Logger 的 Level 设置为 OFF，com.test Logger 的 Level 未设置（默认是 null），那么 com.test Logger 也无法写日志。\n此外，还有一些预置的方便的方法：\nvoid entering(String className, String methodName); void entering(String className, String methodName, Object param); void entering(String className, String methodName, Object[] params); void exiting(String className, String methodName); void entering(String className, String methodName, Object result); 这些方法将会写出 FINER 级别的进入和离开方法的日志。很奇怪的是，这里没有被写成可变参数。\n对于有关异常的日志，可以使用：\nvoid log(Level l, String message, Throwable t); void throwing(String className, String methodName, Throwable t); 配置日志 # 配置等级 # # 指定有关日志的配置文件的位置： java -Djava.util.logging.config.file=configFile MainClass 在配置文件里，对日志属性进行设置：\ncom.test.level=FINE # for Handlers below java.util.logging.ConsoleHandler.level=FINE 配置 Handler # 日志的实际处理由 Handler 完成。如，ConsoleHandler 将日志从 System.err 打印出来。默认，根 Logger，即名为空字符串的 Logger 连接到一个 ConsoleHandler。\nLogger logger = Logger.getLogger(\u0026#34;com.test\u0026#34;); logger.setUseParentHandlers(false); // stop sending to parent logger.addHandler(new ConsolHandler); Java API 默认提供了另外两种 Handler。SocketHandler 将日志发送给网络，FileHandler 写到文件。默认，会写入一个 javax.log 文件，其中 x是一个用来确保唯一的整数，默认为 XML 格式。\nFileHandler 的配置项包括：level append limit（字节数）pattern count（循环的文件数）filter encoding formatter。\n在文件名 pattern 中，可以使用的模式包括：%h 家目录，%t 系统临时目录，%u 唯一编号，%% % 号。\nFilter 和 Formatter # Filter 是一个函数式接口：\npublic interface Filter { boolean isLoggable(LogRecord record); } 一次只能通过 setFilter() 使用一个 Filter。\nFormatter 类需要覆盖方法：\nString format(LogRecord record); // for xml formatters: String getHead(Handler h); String getTail(Handler h); 其中可能需要用到\nString formatMessage(LogRecord record); "},{"id":14,"href":"/notes/programming-scala/4/","title":"4. Pattern Matching, Collections","section":"Programming in Scala","content":" 模式匹配 # 样例类 case class # abstract class Expr case class Number(num: Double) extends Expr case class UnOp(operator: String, arg: Expr) extends Expr case class BinOp(operator: String, left: Expr, right: Expr) extends Expr Scala 为一个 case class 提供了包括：\n一个 apply 工厂方法，等同于 def apply(num: Double) = new Number(num) 一系列字段。等同于 val num: Double 正确实现的 toString equals hashCode 方法 一个 copy 方法，这个方法可以接收参数以产生部分不同的新对象。 最重要的是，样例类可以进行模式匹配。\n模式 # 通配模式 case _ 匹配任何对象，用于缺省捕获。 常量模式 case 1 仅匹配自己，也就是 equals 返回真值的对象。包括数字、字符串、单例对象、val 值等都可以。 变量模式 case e 变量模式也匹配任何对象，但这个变量名在后续的表达式中是有意义的，可以进行进一步处理。在区分常量模式和变量模式时，Scala 简单地使用首字母来判断。如果首字母是大写，就认为常量。必要时可以选择转义。 构造器模式 case BinOp(\u0026quot;+\u0026quot;, e, Number(0)) 构造器可以进行深度匹配，例如这里嵌套的 Number 对象。 序列模式 case List(0, a, _) 元组模式 case (0, a, _) 类型模式 case m: Map[_, _] 在 Scala 中推荐使用类型匹配而非 isInstanceOf[String] asInstanceOf[String] 来判断类型。 变量绑定 case BinOp(\u0026quot;-\u0026quot;, v @ Number(1), _) v 可以作为变量使用。这样可以在变量模式的基础上进行匹配。 常量和变量的匹配顺序规则如下：大写开头作为常量，小写开头作为变量，加转义则变回常量，这是考虑常量的值是作用域中某个变量的情况。大写开头的变量则不被支持。\nimport math.Pi val pi = 3.14 def f(n: Double) = n match { case Pi =\u0026gt; Pi case `pi` =\u0026gt; pi case pi =\u0026gt; pi } List(Pi, 3.14, 3) map f // List(3.141592653589793, 3.14, 3.0) 另一个问题是类型擦除。由于类型擦除，Scala 也没有办法准确地推断出一个泛型容器内部的类型。因此，一个匹配 Map 的模式匹配将能够接受所有的 Map 类型。\nval f = (n: Any) =\u0026gt; n match {case m: Map[Int, Int] =\u0026gt; true; case _ =\u0026gt; false} // warning: non-variable type argument Int in type pattern scala.collection.immutable.Map[Int,Int] (the underlying of Map[Int,Int]) is unchecked since it is eliminated by erasure // (n: Any) =\u0026gt; n match {case m: Map[Int, Int] =\u0026gt; true; case _ =\u0026gt; false} // ^ f(Map(1 -\u0026gt;2)) // true f(Map(1 -\u0026gt;\u0026#34;\u0026#34;)) // true 模式守卫与模式重叠 # 在模式匹配中，模式需要是线性的，一个变量模式只能出现一次。如果我们要判断两个位置的值相等，就需要这样做：\ndef same(s: Any) = s match { case (x, x) =\u0026gt; true; case _ =\u0026gt; false } // error: x is already defined as value x // def same(s: Any) = s match { case (x, x) =\u0026gt; true; case _ =\u0026gt; false } // ^ def same(s: Any) = s match { case (x, y) if x == y =\u0026gt; true; case _ =\u0026gt; false } 这种方式当然也能添加其他的条件。\n模式重叠（Pattern Overlaps）指的是，在模式匹配中，排在上面的模式所覆盖的范围应该小于下面的，否则下面的模式就是 Unreachable Code。\n封闭类 # 是否在模式匹配的最后使用 case _ =\u0026gt; 是一个选择。如果使用这样的语句，那么错误就有可能被隐藏起来难以发现。如果不使用，则会抛出 MatchError。但如果使用封闭类（sealed class），Scala 就能够判断出这个类的所有情况都已经被覆盖了。\nsealed abstract class A class B extends A class C extends A def f(a: A) = a match { case a: B =\u0026gt; true } // warning: match may not be exhaustive. // It would fail on the following input: C() // def f(a: A) = a match { case a: B =\u0026gt; true } // ^ def f(a: A) = a match { case a: B =\u0026gt; true; case a: C =\u0026gt; false } // no warning 当然，也可以选择使用 @unchecked 注解，但这样做通常并不合适。\ndef f(a: A) = (a: @unchecked) match { case a: B =\u0026gt; true } Sealed class 的最典型例子就是 Option。这个类只有两个子类，Some 和 None。Scala 的 Map 就使用了这个类：\nval m = Map(\u0026#34;a\u0026#34; -\u0026gt; 1, \u0026#34;b\u0026#34; -\u0026gt; 2) m(\u0026#34;c\u0026#34;) // java.util.NoSuchElementException: key not found: c // at scala.collection.immutable.Map$Map2.apply(Map.scala:135) // ... 28 elided m.get(\u0026#34;c\u0026#34;) // None m.get(\u0026#34;a\u0026#34;) // Some(1) 处理 Option 的常用方法也包括 map flatMap 和模式匹配。\n模式的更多应用 # val myTuple = (123, \u0026#34;abc\u0026#34;) val (number, string) = myTuple // number: Int = 123 // string: String = abc 类似地，所有的 case class 都可以用类似的方法来解析。\n花括号内的部分实际上就是一个函数字面量，或者说一个 lambda 表达式。\nval withDefault: Option[Int] =\u0026gt; Int = { case Some(x) =\u0026gt; x case None =\u0026gt; 0 } // withDefault: Option[Int] =\u0026gt; Int = \u0026lt;function1\u0026gt; 我们把 withDefault 定义为了一个接收 Option[Int]，返回 Int 的 lambda 表达式，这种语法在 Akka 中非常常用。\n如果一个模式匹配的最后没有 case _ 或 case v，那么当遇到未覆盖的值时会抛出 MatchError，这样的模式匹配属于一个偏函数。就是说，它不能完全处理整个定义域。\nval second: Function1[List[Int], Int] = { case x :: y :: _ =\u0026gt; y } // warning: match may not be exhaustive. // It would fail on the following inputs: List(_), Nil // val second: Function1[List[Int], Int] = { case x :: y :: _ =\u0026gt; y } // ^ val second: PartialFunction[List[Int], Int] = { case x :: y :: _ =\u0026gt; y } List(Nil, List(1, 2, 3)) map second.isDefinedAt // List(false, true) 在将一个 lambda 表达式隐式转换成一个 PartialFunction 对象时，这个类被这样定义：\nnew PartialFunction[List[Int], Int] { def apply(xs: List[Int]) = xs match { case x :: y :: _ =\u0026gt; y} def isDefinedAt(xs: List[Int]) = xs match { case x :: y :: _ =\u0026gt; true case _ =\u0026gt; false } } 相对于普通的 Function 类，PartialFunction 额外定义了一系列可以用于量多个偏函数连接起来的方法。如果不能匹配，两种函数都会抛出 MatchError。\nfor 表达式里也会出现模式匹配。通常 for 表达式都能够完美匹配，因为容器中只能保存一种对象。一个例外是 Option，这种情况下 None 会被抛弃。\nfor (Some(a) \u0026lt;- List(Some(1), None, Some(3))) print(a) // 13 for ((a, b) \u0026lt;- List((1, 2), (3, 4))) println(a + \u0026#34; \u0026#34; + b) // 1 2\\n3 4 列表 # 列表的形式 # 我们熟悉的列表语法：List(1, 2, 3) 是一个语法糖，它等价于 1 :: 2 :: 3 :: Nil，别忘了 :: 是右结合的。Nil 的类型是 List[Nothing]。因为 List 是 Immutable 的，它也被实现为协变的。这样，由于 Nothing 是所有类型的子类，Nil 也是所有 List 的子类，因此 Nil 可以作为任何 List 的空列表表示。\n我们已经知道在列表的前面增加元素是高效的，而在 ArrayBuffer 的后面增加元素是高效的。ArrayBuffer 能够向后扩容，而 List 的默认实现是一个链表。不严谨地说，List(1, 2 ,3) 这个对象的 head 属性是 1，而 tail 属性是 List(2, 3)，这样做比复制整个列表节省内存和时间。更重要的是，实现为链表有利于进行模式匹配。\n或者从另一个角度来考虑，列表的所有操作都可以被归纳为三种：head tail isEmpty。对于 List(1, 2, 3)(1)，大体上相当于 (1 :: 2 :: 3 :: Nil).tail.head。Scala 列表的这些特性和 Haskell 非常相似。\n列表与模式匹配 # 在模式匹配中，列表模式也可以使用 :: 来表达，上面已经出现了这样的形式。不过利用 :: 能够匹配得更加自由：\nval a :: b :: c = List(1, 2, 3, 4) // a: Int = 1 // b: Int = 2 // c: List[Int] = List(3, 4) val a :: b :: c = List(1, 2, 3) // a: Int = 1 // b: Int = 2 // c: List[Int] = List(3) val List(a, b, _) = List(1, 2, 3) // a: Int = 1 // b: Int = 2 val List(a, b, _) = List(1, 2, 3, 4) // scala.MatchError: List(1, 2, 3, 4) (of class scala.collection.immutable.$colon$colon) // ... 28 elided 相对于 List(a, b, c) 的形式，使用 :: 能够正确地处理不同长度的列表。这两种方式适用于不同的情景。\n有意思的是，这两种形式实际上都不符合我们之前对模式的定义。实际上，List(a, b) 是一个由开发库定义的 extractor 模式的实例（详细说明出现于书 24 章）。而另一种形式 a :: b 当出现在模式匹配中时，不再是调用的 c.:: 方法，而是 scala.:: 这个类，a :: b 等价于 ::(a, b)，其中 :: 是一个 Case Class。也就是说，这里的模式部分实际上是使用 a 和 b 作为两个参数生成的 :: 对象模式。\nfinal case class :: [+A](override val head: A, private[scala] var next: List[A @uncheckedVariance]) extends List[A] { override def isEmpty: Boolean = false override def headOption: Some[A] = Some(head) override def tail: List[A] = next } 用模式匹配来实现关于列表的功能就是一个非常类似于 Haskell 的过程了。我们来尝试使用递归和模式匹配来实现 ::: 的功能。为了避免和原有的方法冲突，我们将其命名为 +++:：\nimplicit final class AppendList[T](private val self: List[T]) extends AnyVal { def +++:(other: List[T]): List[T] = { other match { case Nil =\u0026gt; self case head :: tail =\u0026gt; head :: tail +++: self } } } List(1, 2) +++: List(3, 4) // List(1, 2, 3, 4) 可以先不去考虑这里的隐式转换。模式匹配的逻辑并不复杂：如果是一个空列表，那么只需要返回原来的列表就可以了。如果是一个有内容的列表，那么就变成其 head 与一个递归的 append 列表的连接。这里的主要部分在于列表的递归思想。当然，实际的代码要比这样效率更高些。\n由于列表的这种实现方式，取得元素的 head 和取得剩余列表的 tail 是 O(1) 的操作，而取得元素的 last 和取得前面一部分列表的 init 是 O(n) 的操作。\n然后，我们来尝试实现一个归并排序：\ndef msort[T](less: (T, T) =\u0026gt; Boolean)(xs: List[T]): List[T] = { def merge(xs: List[T], ys: List[T]): List[T] = { (xs, ys) match { case (Nil, _) =\u0026gt; ys case (_, Nil) =\u0026gt; xs case (x :: xsl, y :: ysl) =\u0026gt; if (less(x, y)) x :: merge(xsl, ys) else y :: merge(xs, ysl) } } val n = xs.length / 2 if (n == 0) xs else { val (ys, zs) = xs splitAt n merge(msort(less)(ys), msort(less)(zs)) } } msort((x: Int, y: Int) =\u0026gt; x \u0026lt; y)(List(1, 2, 3)) val intSort = msort((x: Int, y: Int) =\u0026gt; x \u0026lt; y) _ 这里也能看到柯里化的手法，通过柯里化让一个泛型函数变成了一个固定参数类型的函数，然后接受下一个参数来执行。\nList 相关的高阶方法 # 高阶方法接受或返回另一个函数。如果你和我一样熟悉 Spark，或者熟悉 Python 的推导式，那么 map filter flatMap foreach 这些函数应该用起来很自然。和 for 一样，Scala 会自动产生与之前相似的类型。\nList(1, 2, 3).map(_ + 1) // List(2, 3, 4) ArrayBuffer(1, 2, 3).map(_ + 1) // ArrayBuffer(2, 3, 4) Array(1, 2, 3).map(_ + 1) // Array(2, 3, 4) List(\u0026#34;abc\u0026#34;, \u0026#34;abcdge\u0026#34;).indices // scala.collection.immutable.Range = Range 0 until 2 val f = (l: List[Int]) =\u0026gt; List(l.partition _, l.takeWhile _, l.dropWhile _, l.span _, l.forall _, l.exists _) f(List(3, 4, 2, 1, 7, 5)) map {_{_ \u0026gt; 2}} foreach println // (List(3, 4, 7, 5),List(2, 1)) // List(3, 4) // List(2, 1, 7, 5) // (List(3, 4),List(2, 1, 7, 5)) // false // true 折叠 # def sum(xs: List[Int]): Int = (0 /: xs) (_ + _) 这里使用了左折叠的操作。一个折叠操作与三个值有关：(z /: xs) (op)，即开始值、列表和操作符。如果要在开头排除操作符的副作用，例如：\nval l = List(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;) (\u0026#34;\u0026#34; /: l)(_ + \u0026#34; \u0026#34; + _) // \u0026#34; a b c\u0026#34; (l.head /: l.tail)(_ + \u0026#34; \u0026#34; + _) // \u0026#34;a b c\u0026#34; l.reduce(_ + \u0026#34; \u0026#34; + _) // \u0026#34;a b c\u0026#34; 类似地，:\\ 操作符向右折叠，同时初始值和列表也要反过来。这样也遵循了之前定义右结合操作符时使用的 : 朝向被调用者的原则。也就是 (List(a, b, c) :\\ z)(op)。当然，也可以使用 foldLeft 和 foldRight。此外，还可以使用 reduceLeft 和 reduceRight，它们不接收初始值，直接使用开头或结尾作为初始值。相应地，如果列表为空，它们会抛出异常。\n考虑一个将 List[List[T]] 转换为 List[T] 的 flatten 操作。由于拼接列表这个操作满足结合律，有：\ndef flattenLeft[T](xss: List[List[T]]) = (List[T]() /: xss) (_ ::: _) def flattenRight[T](xss: List[List[T]]) = (xss :\\ List[T]()) (_ ::: _) 但这两种实现的性能有所不同。由于 ::: 的时间代价与前者的长度成正比，所以 flattenRight 的性能要比 flattenLeft 好得多。类似地，可以实现一个基于折叠的线性复杂度的 reverse 方法：\ndef reverseLeft[T](xs: List[T]) = (List[T]() /: xs) {(ys, y) =\u0026gt; y :: ys} 注意到在这两个函数中，都使用了 List[T]() 而非 Nil 来提供类型推断。\nList 对象的方法 # Range // scala.collection.immutable.Range Range(1, 10, 2) // scala.collection.immutable.Range List.range(1, 10, 2) // List(1, 3, 5, 7, 9) List.concat(List(1, 2), List(3, 4), List(5, 6)) // List(1, 2, 3, 4, 5, 6) 在目前的实现中，::: 是使用 ListBuffer 实现的，而 List.concat 继承自 scala.collection.StrictOptimizedIterableOps，是基于 Iterable 实现的。\n最后，我们提供一个直接进行 zip 操作的方法，以下两种方式是等价的：\n(List(1, 2, 3), List(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)).zipped.map{(i: Int, s: String) =\u0026gt; i + \u0026#34; \u0026#34; + s} // List(1 \u0026#34;a\u0026#34;, 2 \u0026#34;b\u0026#34;, 3 \u0026#34;c\u0026#34;) (List(1, 2, 3) zip List(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;)).map{ case (i: Int, s: String) =\u0026gt; i + \u0026#34; \u0026#34; + s} // List(1 \u0026#34;a\u0026#34;, 2 \u0026#34;b\u0026#34;, 3 \u0026#34;c\u0026#34;) 区别是，第一种的 map 接受的参数可以直接接收两个参数，而第二种中接收到的是一个 Tuple2[Int, String]。\nScala 的类型推断 # 比较两个排序函数：\nmsort((x: Char, y: Char) =\u0026gt; x \u0026gt; y)(list) list sortWith (_ \u0026gt; _) _ \u0026gt; _ 这样的简单写法适用于后者但并不能适用于前者，这里就涉及到了 Scala 的类型推断。Scala 的类型推断是基于程序流的。sortWith 是 list 对象的方法，所以我们能够知道 T 的类型，而 msort 不能。回忆 msort 的定义：\ndef msort[T](less: (T, T) =\u0026gt; Boolean)(xs: List[T]): List[T] = ??? 当我们传入 (x: Char, y: char) =\u0026gt; x \u0026gt; y 时，T 才被推断为 Char 类型。因此，可以这样调用：\nmsort[Char](_ \u0026gt; _)(list) 手动给 T 赋予值之后，就能够正常地进行推断了。另一种更好的方法是：\ndef msort[T](xs: List[T])(less: (T, T) =\u0026gt; Boolean): List[T] = ??? 这样，在第一个参数处，就可以直接得到类型参数，无需再手动指定了。不过，这样的结果是失去了柯里化的方便。因此我们得到了一个原则：在提供 API 时，尽量把数据结构放在前面，函数放在后面。\n然后我们回头来看上面的 flatten 函数，函数里使用了 xs :\\ List[T]() 而非 Nil 或等价的 List()。这是因为，如果使用 List()，那么折叠过程的第一步需要一个 (List[T], List[Nothing]) =\u0026gt; List[T] 的操作符，而之后的部分则需要 (List[T], List[T]) =\u0026gt; List[T] 类型，无法统一。\n集合 # Iterable 指代一个集合，而 Iterator 指代对这个集合的操作。实际上，Iterator 继承了 IterableOnce 这个 trait。所以 Iterator 只能遍历一次。\n序列、集合、映射 Seq Set Map # Scala 的数组和 Java 数组是对齐的，可以直接进行操作，虽然形式上 Scala 的数组是泛型的，这使得它们在变型（逆变和协变）问题上略有不同。\nListBuffer 是可变版本的 List。在循环中拼接一个 List 时，要么需要使用 var，要么需要进行递归。如果操作不是尾递归，就有必要使用 ListBuffer 来控制栈的深度。\nQueue 和 Stack 包括可变和不可变的版本，分别使用 enqueue dequeue 和 push pop 来操作。对于不可变的版本，取元素操作会返回一个 Tuple[T, Queue[T]]。\n字符串的隐式转换类 RichString 也是一个 Seq[Char]。\nSet 和 Map 都可以使用 + - 来增加或删除元素，使用 ++ -- 删除一个集合里的元素。对于可变版，还可以使用对应的 += -= 等。对 Map，删除时只需要传入 Key，增加时则是传入一个元组。用箭头来生成 Tuple2[A, B] 作为键值对的语法就适合这种场景。\n出于性能优化的原因，Scala 为 0~4 个元素的不可变集合与映射直接提供了类 scala.collection.immutable.Map.Map4 等一些，更大容量的则使用 HashSet 作为默认。这种小尺寸的集合比可变版本的更加紧凑，更加节省内存，访问时也通常更节省时间。\nScala 只提供了不可变版本的排序后的集合 TreeSet TreeMap，基于红黑树实现。其元素需要混入 Ordered 或能够隐式转换为 Ordered。\nimport scala.collection.immutable.TreeSet var t = TreeSet(1, 3, 4) t += 2 t // TreeSet(1, 2, 3, 4) 在这个例子中，虽然 t 指向一个不可变集合，但因为它是一个 var，所以 t 会指向一个新创建的对象。这个原则适用于所有 Immutable var，只要操作符以 = 结尾。\nSet 和 Map 都有 toArray 和 toList 方法，虽然会造成元素的拷贝。生成的顺序与 elements 方法的返回值一样。如果要从其它集合转换为 Set 和 Map，或者在可变与不可变之间转换，就需要使用 ++。\n元组 # 之前已经提到过模式匹配的一个特例：\nval (a, b) = (1, 2) // a: Int = 1 // b: Int = 2 val a, b = (1, 2) // a: (Int, Int) = (1,2) // b: (Int, Int) = (1,2) 第二种情况更类似 C 风格的传统代码。\nMutable 对象 # Mutable 类内部经常使用 var 进行定义，但这不是绝对的。例如，一个使用 var 引用进行缓存的类可以是纯函数式的，只要其行为对于相同的输入来说是始终相同的：\nclass A { def getValue: Int = ... } class CachedA extends A { private var value: Option[Int] = None override def getValue: Int = if (value.isDefined) value.get else super.getValue } Scala 对于类中非私有的 var 变量直接提供了 getter 和 setter，这样不仅为未来的修改留出空间，甚至可以让我们自己定义一个虚拟的变量出来。以下的类定义对调用者来说是等价的：\nclass A { var a = _ // 注意在 Scala 中必须赋一个初值，否则定义的是一个抽象变量 } class B { private[this] var v = 0 def a: Int = v def a_=(x: Int) = v = x } 当我们访问或修改对象中的 var 变量时，无论是否显式写出来，实际上都是调用了两个方法。这样，有必要的话我们就可以在 getter 和 setter 中添加逻辑，或者实现一些更复杂的逻辑。例如，定义一个温度类：\nclass Therometer { var celsius: Float = _ def fahrenheit = celsius * 9 / 5 + 32 def fahrenheit_= (f: Float) = celsius = (f - 32) * 5 / 9 override def toString = fahrenheit + \u0026#34;F/\u0026#34; + celcius + \u0026#34;C\u0026#34; "},{"id":15,"href":"/notes/core-java-impatient/5/","title":"5. Generics","section":"Core Java for Impatients","content":" 使用泛型 # 泛型方法 # public class Arrays { public static \u0026lt;T\u0026gt; void swap(T[] array int i, int j) { ... } } // call Arrays.swap(arr, 1 ,2); Arrays.\u0026lt;String\u0026gt; swap(arr, 1, 2); // for better error message 类型限定 # // for an arraylist public static \u0026lt;T extends AutoClosable\u0026gt; void closeAll(ArrayList\u0026lt;T\u0026gt; elems) throws Exception { for (T elem : elems) elem.close(); // an method in interface AutoClosable } //for an array, no need for generic public static void closeAll(AutoClosable[] elems) throws Exception { ... } 这是因为，PrintStream[] 是 AutoClosable[] 的子类，而 ArrayList\u0026lt;PrintStream\u0026gt; 并不是 ArrayList\u0026lt;AutoClosable\u0026gt; 的子类。也可以同时限定多个类型：\nT extends Runnable \u0026amp; AutoClosable 类型变异和通配符 # 上面我们看到，子类数组是父类数组的子类。数组随其元素而变化，这种行为称为协变性（covariance）。但 ArrayList 并不是协变的。这是为了防止之前提到过的异常。因此，需要使用通配符来限制类型，这种类型有时称为使用时变化（use-site variance）。\n父子类型 # public static void printNames(ArrayList\u0026lt;? extends Employee\u0026gt; staff) { //use Employee methods only Employee e = staff.get(i); // legal staff.add(new Employee()); // illegal. same with subtypes of employee } 这样，实际上这种写法是只读的。类似地：\npublic static void insert(ArrayList\u0026lt;? super employee\u0026gt; staff) { //a weird method, only for test //accept supertypes of Employee staff.add(new Employee()); // legal. for ? is supertype of employee Employee e = staff.get(0); //illegal. } 在这里，相当于集合变成了只能写入的。这叫做逆变。由于这个原因，一个口诀叫做 \u0026ldquo;PECS\u0026rdquo;，即 \u0026ldquo;Producer excents Comsumer super\u0026rdquo;。也是由于这个原因，C# 在这里使用了 in 和 out 关键字。\n带类型变量的通配符 # public Collections { public static \u0026lt;T extends Comparable\u0026lt;? super T\u0026gt;\u0026gt; void sort(List\u0026lt;T\u0026gt; list); } 仍然使用员工和经理的例子。设想 T 是 Manager 的情况。\nEmployee 实现了 Comparable\u0026lt;Employee\u0026gt;，Manager 继承了 Employee。这时，Manager 实现的是 Comparable\u0026lt;Employee\u0026gt; 接口，而非 Comparable\u0026lt;Manager\u0026gt; 接口。很容易理解，因为 Comparable\u0026lt;Manager\u0026gt; 并不是 Comaprable\u0026lt;Employee\u0026gt; 的子类。\n因此，这里使用的必须是 Comparable\u0026lt;? super T\u0026gt;，这样才能匹配到 Comparable\u0026lt;Employee\u0026gt;。然后，这个方法显然应该能够接受 Manager 的子类，因此最终的泛型为 T extends Comparable\u0026lt;? super T\u0026gt;。\nC# 和 Scala 等语言在这里使用了声明时协变和逆变的方法，比 Java 的方法更加方便直观，但不那么强大。\n通配符捕获 # 尝试写一个 swap 方法：\npublic static void swap(ArrayList\u0026lt;?\u0026gt; elements, int i, int j) { ? temp = ... // ???? } 除了使用 Object 之外，还有另一种方法来绕过这个问题：\npublic static void swap(ArrayList\u0026lt;?\u0026gt; elems, int i, int j) { swapHelper(elems, i, j); } public static \u0026lt;T\u0026gt; void swapHelper(ArrayList\u0026lt;T\u0026gt; elems, int i, int j) { T temp = ... } 虽然我们不知道 T 是什么，但我们知道它一定是一个类型。相当于我们\u0026quot;捕获\u0026quot;了这个通配符。这样做的目的是，暴露出一个容易理解的 \u0026lt;?\u0026gt; 的 API 接口，而不是令人困惑的泛型方法。\nJVM 中的泛型 # 类型擦除 # 对于 Entry\u0026lt;K, V\u0026gt;，会转变成：\npublic class Entry { private Object key; private Object value; ... } 类似地，对于 Entry\u0026lt;K entends Comparable\u0026lt;? super K\u0026gt; \u0026amp; Serializable, V extends Serializable\u0026gt; ：\npublic class Entry { private Comparable key; // 这里失去了一个接口信息，虽然对象本身仍然是同时实现二者的 private Serializable value; ... } 对于大部分的方法，由编译器解决了 cast 的问题。因此，仍然可以使用：\nString str = getValue(); 桥方法 # 桥方法是编译器在泛型类型和协变返回类型中做的一个 trick。例如，考虑：\npublic class WordList extends ArrayList\u0026lt;String\u0026gt; { public void add(String e) { ... } } ArrayList\u0026lt;String\u0026gt; strings = new WordList(...); strings.add(\u0026#34;abc\u0026#34;); 这时，理论情况下，就会调用 ArrayList\u0026lt;String\u0026gt; 的 add(Object) 方法，而这显然不是我们想要的。因此，Java 编译器偷偷生成了一个方法：\npublic void add(Object e) { add((String)e); } 对于返回泛型对象的情况，也是一样的处理。\nString get(int); Object get(int); 在 Java 语言中不能定义这样的两个方法，但在字节码中可以。总之，javac 通过这种方法让泛型类型的使用看起来更加自然。\n泛型约束 # 绝大部分是由于类型擦除机制，Java 的泛型使用存在一些限制。首先，泛型参数必须是对象，而非基本类型。\ncast 中的问题 # if (a instanceof ArrayList\u0026lt;String\u0026gt;); // Runtime Error, for no generic type ArrayList\u0026lt;String\u0026gt; list = (ArrayList\u0026lt;String\u0026gt;) result; // warning, only confirm is arryalist, but maybe not string // @SuppressWarnings(\u0026#34;unchecked\u0026#34;) // if wrong type, will throw ClassCastException ArrayList\u0026lt;String\u0026gt;.class; // ArrayList, no generic 实例化泛型变量 # 同样是由于类型擦除，我们无法使用 new T() 或 new T[] 这些表达式。这时，可以使用反射机制，使用方法引用，或者手动传入一个数组。\n// use method reference public static \u0026lt;T\u0026gt; T[] repeat(int n, T obj, IntFunction\u0026lt;T[]) constr) { T[] result = constr.apply(n); ... } // call String strs = repeat(10, \u0026#34;abc\u0026#34;, String::new); // use reflection public static \u0026lt;T\u0026gt; T[] repeat(int n, T obj, Class\u0026lt;T\u0026gt; cl) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T[] result = (T[]) java.lang.reflect.Array.newInstance(cl, n); // similar for objects ... } // call String[] strs = repeat(10, \u0026#34;abc\u0026#34;, String.class); // use array param public static \u0026lt;T\u0026gt; T[] repeat(int n, T obj, T[] array) { T[] result; if (array.length \u0026gt;= n) // enough space result = array; else { // use reflection like above // java.lang.reflect.Array.newInstance(array.getClass().getComponentType(), n) } ... } 当然，最简单的办法还是依旧返回一个泛型对象，推荐使用这种方法：\npublic static \u0026lt;T\u0026gt; ArrayList\u0026lt;T\u0026gt; repeat(int n, T obj); 如果需要在泛型类内使用戒；数组，那么直接使用 Object[] 就可以了。这也是 ArrayList 内部的实现方法。\n参数化类型的数组 # 由于泛型类型和数组的实现有所不同，二者相遇时就会需要一些技巧。例如，直接赋给一个包含泛型类型的数组是不可以的，回忆之前我们提到过的 ArrayStoreException。\nEntry\u0026lt;String. Integer\u0026gt;[] entries = new Entry\u0026lt;String, Integer\u0026gt;[100]; // Wrong Grammar 实际上，这个对象类型是正确的，只是初始化过程不合法。因此：\n@SuppressWarnings(\u0026#34;unchecked\u0026#34;) Entry\u0026lt;String, Integer\u0026gt;[] entries = (Entry\u0026lt;String, Integer\u0026gt;[]) new Entry\u0026lt;?, ?\u0026gt;[100]; 这里没有规定 Entry 的泛型参数，因为所有类型事实上都是一样的。然后，使用一个带泛型的引用来管理它。当然，直接使用支持泛型的 ArrayList 仍然简单得多。\n还有一种可以实例化泛型数组的位置是可变参数。\n@SafeVarargs public static \u0026lt;T\u0026gt; ArrayList\u0026lt;T\u0026gt; asList(T... elems); // call ArrayList\u0026lt;Entry\u0026lt;String, Integer\u0026gt;\u0026gt; entries = Lists.asList(entry1); // OK 静态上下文 # 再次考虑：泛型可以认为是由引用来处理的，各种不同泛型的同类对象都是相同的。因此，静态方法和变量不能接受泛型类型。或者说，静态方法是对于 ArrayList 只有一份，而不是对于各种 T 各有一份。\n方法冲突 # 例如，这样一个方法在类型擦除之后会和 Object.equals 冲突，难以发现这个方法事实上完全无效。\npublic interface Ordered\u0026lt;T\u0026gt; extends Comparable\u0026lt;T\u0026gt; { public default boolean equals(T value) { ... } } 还有一种更加隐蔽的可能：\npublic class Employee implements Comparable\u0026lt;Employee\u0026gt; { public int compareTo(Employee other) {...} } public class Manager extends Employee implements Comparable\u0026lt;Manager\u0026gt; { public int compareTo(Manager other) {...} } 这时，虽然这两个方法没有关系，但我们之前提到的桥方法发生了互相冲突。\n异常与泛型 # Throwable 的子类不能是泛型的。也就是说，不存在泛型异常。类似地，也不能 catch(T ex)。不过，可以在方法声明中声明一个类型参数：\npublic static \u0026lt;V, T\u0026gt; V fun(Callable\u0026lt;V\u0026gt; c, T ex) throws T { ... } 有可能通过泛型来绕开 Checked Exception 机制，因为泛型过程并不知道类型是 Checked Exception 还是 Unchecked Exception。\n反射与泛型 # 前面我们已经知道，Class 类是泛型的。这样做的目的是为 newInstance cast 等方法提供方便：\npublic class\u0026lt;T\u0026gt; { public T newInstance() throws ... {...} public Class\u0026lt;? super T\u0026gt; getSuperClass() {...} public \u0026lt;U\u0026gt; Class\u0026lt;? extends U\u0026gt; asSubClass(Class\u0026lt;U\u0026gt; clazz) {...} public T cast(Object obj) {...} public Constructor\u0026lt;T\u0026gt; getConstructor(Class\u0026lt;?\u0026gt;... parameterTypes) {...} public Constructor\u0026lt;T\u0026gt; getDeclaredConstructor(Class\u0026lt;?\u0026gt;... parameterTypes) {...} T[] getEnumConstants() {...} } 大多数情况下，我们直接使用 Class\u0026lt;?\u0026gt;，因为我们并不知道对象的实际类。\njava.lang.reflect 包中与泛型相关的接口包括：描述类型变量的 Typevariable，描述通配符的 WildcardType，描述泛型类和泛型接口的 ParameterizedType，描述泛型数组（T[]）的 GenericArrayType。\n"},{"id":16,"href":"/notes/programming-scala/5/","title":"5. Generics, Abstract, Implicits","section":"Programming in Scala","content":" 类型参数化 # 一个摊还 O(1) 复杂度的函数式队列 # 函数式数据结构通常期望使用递归来进行操作并避免状态的暴露，这让编程模型更优雅统一，但同时，与随机访问的数据结构相比，会将复杂度从 O(1) 提高到 O(n)。不过，通过一系列精妙的设计，函数式的数据结构同样可以具有高性能。虽然这方面的研究尚不完善，但 Scala 混合编程范式的特点让我们能够比较容易地做到这一点。\n首先我们简单地使用列表来实现一个队列。由于列表是前追加的数据结构，我们的第一反应是使用一个翻过来的列表：\nclass SlowHeadQueue[T](elems: List[T]) { def head = elems.last def tail = new SlowHeadQueue(elems.init) def enqueue(x: T) = new SlowHeadQueue(x :: elems) } 这个数据结构 enqueue 是 O(1) 的，而 head 和 tail 是 O(n) 的。但是，我们可以考虑将 head 操作和 tail 操作分开，即使用两个背对背的列表来处理。\nclass Queue[T](private val leading: List[T], private val trailing: List[T]) { private def mirror = if (leading.isEmpty) new Queue(trailing.reverse, Nil) else this def head = mirror.leading.head def tail = { val q = mirror new Queue(q.leading.tail, q.trailing) } def enqueue(x: T) = new Queue(leading, x :: trailing) } 现在，仅当 leading 为空时，才会发生 O(n) 的操作。由于要使得 leading 为空需要 O(n) 次的 tail 操作，所以这个数据结构的摊还成本是 O(1) 的。\n现在，剩下的问题是，构建这个队列看起来非常奇怪，需要传入两个队列。所以，我们需要另一个构造函数。实现方式有几种：\nclass Queue[T] private(private val leading: List[T], private val trailing: List[T]) { def this() = this(Nil, Nil) def this(elems: T*) = this(elems.toList, Null) } // you can only call this() from the outside now Queue Queue(1, 2 ,3) Queue(List(1, 2, 3): _*) 更好的办法是使用伴生对象：\nobject Queue { def apply[T](xs: T*) = new Queue[T](xs.toList, Nil) } 实际上，在 Scala 中，既然已经有了 apply 方法，我们就完全不再有必要把带有具体实现的 Queue 类暴露出来。所以，通常我们会这样做：\ntrait Queue[T] { def head: T def tail: Queue[T] def enqueue(x: T): Queue[T] } object Queue { def apply[T](xs: T*): Queue[T] = new QueueImpl[T](xs.toList, Nil) private class QueueImpl[T]( private val leading: List[T], private val trailing: List[T] ) extends Queue[T] { ... } } 泛型变型 # 变型 # Scala 中的泛型默认是不变的。[+T] 表示协变，[-T] 表示逆变。Scala 编译器会自动检查代码中类型参数被使用时的正确性。简单地理解，生产者是协变的，消费者是逆变的。典型的例子是 Scala 中的函数类型：\ntrait Function1[-T1, +R] extends AnyRef { ... } class A[+T] { def get: T = ??? } class A[-T] { def set(x: T) = ??? } class A[-T] { def get: T = ??? } // error: contravariant type T occurs in covariant position in type =\u0026gt; T of method get // class A[-T] { def get: T = ??? } // ^ class A[+T] { def set(x: T) = ??? } // error: covariant type T occurs in contravariant position in type T of value x // class A[+T] { def set(x: T) = ??? } // ^ Java 的数组默认是协变的，这是因为 Java 1.5 之前没有泛型时的历史原因，Scala 则默认不变型。当我们修改 Java 数组时，可能会得到 ArrayStoreException。而在 Scala 中，不能像 Java 一样直接赋值，只可能像 Java 的默认泛型一样 cast 数组（arr.asInstanceOf[Array[Object]]），然后才有可能发生 ArrayStoreException。也就是说，当能发生这个问题的情况时，你应该已经意识到了这个风险。\n下界和上界 # 对我们上面的队列的例子，如果队列定义为协变的，enqueue 方法就会产生矛盾，因为 setter 是逆变点（消费者）。例如，如果这里没有限制，就可能会有：\nval q: Queue[Fruit] = new Queue[Apple] q.enqueue(new Orange) 这显然是不合理的。好在我们可以利用下界（相当于 Java 中的类型参数通配符）来限制 enqueue 参数的类型的范围：\nclass Queue[+T] (private val leading: List[T], private val trailing: List[T]) { def enqueue[U \u0026gt;: T](x: U) = new Queue(leading, x :: trailing) } 现在，enqueue 的参数被限制在 T 的父类，其返回值也变成了 Queue[U] 而不是 Queue[T]。于是有：\nval q: Queue[Fruit] = new Queue[Apple] q.enqueue(new Orange) // won\u0026#39;t compile q.enqueue(new Fruit) // q.enqueue now only takes U \u0026gt;: Fruit, no Orange 有意思的是，这样的方式仅适用于符合函数式范式的 Immutable 的数据结构。而且，我们这里确定了正确的类型的同时，代码的逻辑也被确定了。也许我们一开始并没有想到这样安全地描述 enqueue 方法，但由于编译器的协变检查，我们必须要这样实现。这种方式也被叫做类型驱动设计（type-driven design）。在一些学术性质甚至比 Scala 更强的语言，如 Haskell 中，这件事体现得更加明显。\n这也解释了为什么 Scala 采用了\u0026quot;声明点型变\u0026quot;而不是 Java 的\u0026quot;使用点型变\u0026quot;。作为类的定义者，我们可以在这里解决变型问题而不是将这些复杂的问题交给使用者。实际上，许多使用基于 Scala 的成熟框架的开发者，比如 Spark 的用户，完全不需要知道这些有关变型的知识。\n另外，对象内私有的变量不需要进行这些限制，因为它并不会被对象外访问到。例如，上面的队列在 leading 为空时连续 head 的性能较差，因为每一次调用都要重新进行 trailing.reverse。可以在对象内部引入状态来解决：\nclass Queue[+T] private ( private[this] var leading: List[T], private[this] var trailing: List[T]) { private def mirror() = if (leading.isEmpty) { while (!trailing.isEmpty) { leading = trailing.head :: leading trailing = trailing.tail } } def head: T = { mirror(); leading.head } def tail: Queue[T] = { mirror(); new Queue(leading.tail, trailing) } def enqueue[U \u0026gt;: T](x: U) = new Queue[U](leading, x :: trailing) } 这里的 mirror 使用了指令式编程的方式，目的是体现出，虽然这个过程中出现了关于类型参数 T 的 get 和 set 参数（修改 leading 和 trailing 的值），但我们完全不需要考虑变型问题，因为对象内部的 T 是已经确定的。所以，private[this] 的变量不会进行编译期的变型检查，只会进行基本的类型检查。\n类似地，对于逆变的情况，也可以有上界。例如，对于一个排序函数，需要有：\ndef mergeSort[T \u0026lt;: Ordered[T]](xs: List[T]): List[T] = { ... } 这样，我们要求传入的列表中的对象必须是 Ordered[T] 的子类型。不过，这并不是使用 Ordered 特质的最佳方式。\n抽象成员 # 抽象成员的种类 # 一个包括各种抽象成员的例子：\ntrait Abstract { type T def transform(x: T): T val initial: T var current: T } class Concrete extends Abstract { type T = String def transform(x: String) = x + x val initial = \u0026#34;hi\u0026#34; var current = initial } 这里的抽象成员包括类型成员、函数、可变变量和不可变变量。\n首先，定义抽象类型的目的是形成一个别名（alias）。这样做的目的通常是隐藏一个复杂而含义不明显的类型，也可以使用 \u0026lt;: 或 \u0026gt;:。然后，在后面的类定义中就可以使用这个别名。\n之前我们已经知道，def val var 对调用者来说并没有本质的区别。但当继承这个定义的时候，val 的下游只能是 val，所以每一次重复调用它，返回值都应该是相同的。def 则无法做出这样的保证。也就是说，可以用 val 来定义抽象的 def，但不能反过来。\n抽象 val 的初始化时机 # 考虑之前出现过的有理数类：\ntrait RationalTrait { val numerArg: Int; val denomArg: Int } class Rational (val numerArg: Int, val denomArg: Int) extends RationalTrait new RationalTrait { val numerArg = 1; val denomArg = 2 } // $anon$1@1c00d406 new Rational(1, 2) // Rational@67cd84f9 这两种方式看起来似乎是一样的。但实际上，二者的参数初始化时间存在区别。使用类的情况下，1 和 2 是作为参数被传入，（在非传名参数的情况下）是先求值，再传入。而在使用 new trait 的情况下，则会先初始化 RationalTrait，再传入这两个值。如果我们加上一个能够检测这种情况的条件：\ntrait RationalTrait { val numerArg: Int; val denomArg: Int; require(numerArg \u0026gt; 0) } new RationalTrait { val numerArg = 1; val denomArg = 2 } // java.lang.IllegalArgumentException: requirement failed // at scala.Predef$.require(Predef.scala:268) // at RationalTrait.$init$(\u0026lt;console\u0026gt;:11) // ... 29 elided new { val numerArg = 1; val denomArg = 2 } with RationalTrait // $anon$1@2ba0b7cf 在进行 require 判断时，这两个变量的值还是默认值 0，所以抛出了异常。在 Trait 初始化完成之后，才会被赋上值。如果采用混入的方式，那么这两个变量会仙贝初始化，然后才会调用父类的构造方法，不会发生这个问题。\n当然，一种更优雅的方式是将 require 语句放在 lazy val 的初始化中去，这在逻辑上也更合理。\ntrait LazyRationalTrait { val numerArg: Int val denomArg: Int lazy val = numerArg / g lazy val = denomArg / g private lazy val g = { require(denomArg != 0) gcd(numberArg, denomArg) } private def gcd(a: Int, b: Int): Int = if (b == 0) a else gcd(b, a % b) } 显然，如果 lazy val 的初始化涉及到副作用，初始化时间的情况将会变得相当复杂。所以，这个特性和函数式数据结构结合得更加紧密。\n抽象类型的作用 # 考虑这样的情况：\nclass A class B extends A abstract class C { def consume(x: A) } class D extends C { override def consume(x: B) = print(x) } // \u0026lt;console\u0026gt;:14: error: class D needs to be abstract, since method consume in class C of type (x: A)Unit is not defined // (Note that A does not match B: class B is a subclass of class A, but method parameter types must match exactly.) // class D extends C { override def consume(x: B) = print(x) } // ^ // \u0026lt;console\u0026gt;:14: error: method consume overrides nothing. // Note: the super classes of class D contain the following, non final members named consume: // def consume(x: A): Unit // class D extends C { override def consume(x: B) = print(x) } 我们发现，consume(x: B) 无法重写 consume(x: A)，因为它们接收不同类型的参数。那么如果我们希望限制 D 类型中 consume 方法能够接收的参数类型呢？留下一个废弃的 consume(x: A) 显然不合适，允许重写更不合理。如果我们使用抽象类型：\nabstract class C { type T \u0026lt;: A // a type that is subclass of A def consume(x: T) } class D extends C { type T = B override def consume(x: B) = print(x) } (new D) consume (new B) // $line46.$read$$iw$$iw$B@43687885 (new D) consume (new A) // \u0026lt;console\u0026gt;:15: error: type mismatch; // found : A // required: B // (new D) consume (new A) // ^ 这样，我们就限制了子类中参数的类型。\n此外，这样定义的类型限制是路径依赖的。体现为，如果我们将一个 D 对象放在 C 引用中，那么抛出的 type mismatch 将会是：\nval d: C = new D // d: C = D@1eb3b8c0 d.consume(new A) // \u0026lt;console\u0026gt;:15: error: type mismatch; // found : A // required: d.T // d.consume(new A) // ^ (new { val d: C = new D }).d.consume(new A) // \u0026lt;console\u0026gt;:16: error: type mismatch; // found : A // required: _1.T where val _1: C // (new { val d: C = new D }).d.consume(new A) // ^ 可以看到，因为引用是 C 类型的，我们无法直接知道所需的 T 是哪一个实际类型，但是我们知道这个 T 是 d 对象中的，也就是说这个类型依赖于其（对象引用的）路径。具体来说，依赖于这些对象所属的类，有点类似于 Java 的内部类。不过，对于内部类的情况，Scala 使用 # 作为连接符。\n改良类型 refinement type # 抽象类型甚至让 Scala 具有了一定程度上类似于 duck type（没错）的能力。例如，在上面的例子中，如果我们想要一个类型，能够包括所有接收 B 类型的 C（例如，所有食草的动物），那么就有：\nval cThatTakesBs: List[C {type T = B}] = ... 这样我们就无需为一系列\u0026quot; T 为 B 的对象\u0026quot;定义一个麻烦的容易忘记的新 trait 了，也能够更加肆无忌惮地使用 new object with 等语法来创建匿名类了。与 duck typing 相比，Scala 的这种能力需要我们在父类中提前做好设计（又一次，类型驱动设计），但相比于 Python 的按方法名判断，这种方式显然要安全得多。鸭子类型和改良类型都是实现\u0026quot;结构子类型\u0026quot;的一种方式，即由对象的结构决定其类型，而非传统的反过来的\u0026quot;名义子类型\u0026quot;，只有显式继承的才算是子类。\n枚举 Enumeration # 由于路径依赖类型的存在，Scala 不需要像很多语言一样让编译器额外处理枚举类型（再一次体现了 Scala 简单的基本语法延伸出复杂的用法和功能）。只需要继承一个类即可：\nobject Color extends Enumeration { val Red = value val Green = value } object Color extends Enumration { val Red, Green = Value } import Color._ Enumeration 类的核心部分大致上是：\nabstract class Enumeration (initial: Int) extends Serializable { protected final def Value: Value = Value(nextId) protected final def Value(i: Int): Value = Value(i, nextNameOrNull) protected final def Value(name: String): Value = Value(nextId, name) protected final def Value(i: Int, name: String): Value = new Val(i, name) abstract class Value extends Ordered[Value] with Serializable {} protected class Val(i: Int, name: String) extends Value with Serializable {} } 所以，不同的 object extends Enumeration 里的 Value 类，由于路径依赖不能兼容，使得不同枚举类的元素之间不能互相兼容。\n隐式定义 # 隐式转换 # 在使用隐式转换时，编译器会首先尝试编译。如果类型不能匹配，编译器会在作用域中寻找合适的隐式转换。查找隐式转换的范围是，作用域中所有的\u0026quot;单个标识符\u0026quot;，也就是直接定义在作用域里，而不是某个对象里，再加上源类型与目标类型的伴生对象里。例如，当前作用域下一个变量 someVar.String2Int 不会被搜索，但 String 类和 Int 类对应的伴生对象中的方法都会被搜索。此外，Scala 的隐式转换只能进行一次，不会发生难以控制的链式多次转换的情况。也不会覆盖显式的定义，只要能通过类型检查，就不会调用隐式转换。\n例如，Predef 中定义了数字之间互相转换的函数。再一次，Scala 通过一个通用的语言特性解决了一种\u0026quot;特殊情况\u0026quot;。（虽然 Scala 编译器仍然进行了特殊情况的处理，生成了效率更高的字节码）。\n隐式定义会出现在三个地方：转换到预期的类型，选择接收端和隐式参数。分别对应这样的方式：\nimport scala.language.implicitConversions class A class B { def run = println(\u0026#34;B.run\u0026#34;) } implicit def a2b(x: A): B = new B implicit val y = new B def fun(x: B) = println(\u0026#34;obj B in fun\u0026#34;) def func(x: A)(implicit y: B) = y.run fun(new A) // converting to a expected type // obj B in fun (new A).run // converting the receiver // B.run func(new A) // implicit parameters // B.run 隐式转换适合用来创建 DSL。例如，用来创建 Map 的语法就是使用隐式转换制作的：\nMap(1 -\u0026gt; \u0026#34;one\u0026#34;, 2 -\u0026gt; \u0026#34;two\u0026#34;) package scala object Predef { implicit final class ArrowAssoc[A](private val self: A) extends Anyval { @inine def -\u0026gt; [B](y: B)Tuple2[A, B] = Tuple2(self, y) } } // or, seperated class and function object Predef { final class ArrowAssoc[A](private val self: A) extends Anyval { @inine def -\u0026gt; [B](y: B)Tuple2[A, B] = Tuple2(self, y) } implicit def any2ArrowAssoc[A](x: A): ArrowAssoc[A] = new ArrowAssoc } 其中上面一种方式称作隐式类，相当于一个类定义和一个以其构造方法为形式的隐式转换函数。显然其构造方法必须是单参数的。Scala 还限制隐式类必须存在于另一个对象、类或特质里，这样就一定程度上限制了隐式类的滥用。\n隐式参数 # 隐式参数常被用在提供一个多次用到的通用值的情形。例如定义一个命令提示符：\nclass PreferedPrompt(val preference: String) def printWithPrompt(str: String)(implicit prompt: PreferedPrompt) = println(prompt.preference + \u0026#34; \u0026#34; + str) implicit val prompt = new PreferedPrompt(\u0026#34;\u0026gt;\u0026#34;) printWithPrompt(\u0026#34;run\u0026#34;)(prompt) printWithPrompt(\u0026#34;run\u0026#34;) 常见的方式是将默认值放在一个对象中，再 import obj._。\n同时，我们在这里专门为隐式参数定义了一个类，而不是使用 String。这是为了避免不必要的额外匹配的风险，因为隐式转换只寻找类型而不判断变量名。隐式参数在 Scala 中最常见的场景是用于排序，排序函数的第二个参数列表里通常是一个 Ordering[T] 对象。显然，这里的功能只需要一个 (T, T) =\u0026gt; Boolean 类型的参数就能完成，但这样的类型太过泛化，风险比较高。\n考虑一个常见的排序函数的递归实现：\ndef sort[T](l: List[T])(implicit ordering: Ordering[T]) = { ... sort(...) if (ordering.gt(...)) ... } 和我们的直觉相符，Scala 会直接把外层的函数接收的 ordering 作为隐式参数继续使用，也可以直接去调用这个参数。那么，有没有可能不去显式地写 ordering 这个变量名呢？（毕竟它是隐式的）\ndef sort[T](l: List[T])(implicit ordering: Ordering[T]) = { ... if (implicitly[Ordering[T]].gt(...)) ... } implicitly 是 Scala 定义的一个用于查找一定类型的隐式参数的函数。现在我们发现，有了这种查找方式，我们实际上已经不再需要 ordering 这个变量名了。所以，进一步地，我们可以直接去修改类型参数：\ndef sort[T: Ordering](l: List[T]) = ... 这样定义意味着要求类型参数 T 必须有相应的 ordering。一个很好的性质是，通过这种方式，我们并不需要修改 T 类型。例如，一个外部库里有一个类型 A，当我们对 A 排序时，不需要修改 A 使其实现 Comparable，而只需要提供一个 Ordering[A]。\n另外一个需要解决的问题是隐式定义的冲突。在 Scala 2.8 之后，采用了和方法重载类似的方式：优先选择\u0026quot;更具体\u0026quot;的那一个。如果同样具体，就需要手动指定。在静态类型语言中，这并不是什么大问题：\nimplicit val v: AnyVal = 1 implicit val i: Int = 2 implicitly[Int] // 2 implicitly[AnyVal] // 2, which is the more specific one implicit val ii: Int = 3 implicitly[Int] // ambiguous implicit value 2 and 3 with same weight // \u0026lt;console\u0026gt;:15: error: ambiguous implicit values: // both value i of type =\u0026gt; Int // and value ii of type =\u0026gt; Int // match expected type Int // implicitly[Int] // ^ 可以使用 -Xprint:typer 参数来查看发生的隐式转换。\n隐式转换的优先级 # 并不是所有隐式转换都会显式发生冲突。例如，String 有两个隐式转换，一个转换成 WrappedString，其方法返回的仍然是 WrappedString。另一个是 StringOps，其返回值仍然是 String，第二个的优先级更高。这样，如果我们需要一个 Seq，会得到 WrappedString，否则仍然得到一个 String。这样的原因是 StringOps 的转换位于 Predef，WrappedString 的转换位于 scala.LowPriorityImplicits。Scala 选择隐式转换的原则是：\n更具体类型的隐式转换优先级更高。 如果 A extends B，那么 B 中的优先级更高。 考虑我们定义一个类继承另一个类，那么子类中的隐式很可能更加是我们想要的。\nimplicit val a: String = \u0026#34;a\u0026#34; implicit val b: CharSequence = \u0026#34;b\u0026#34; def f(implicit s: String) = s f // a class A { implicit val a: String = \u0026#34;a\u0026#34; } object B extends A { implicit val b: String = \u0026#34;b\u0026#34; } def f(implicit s: String) = s import B._ f // b "},{"id":17,"href":"/notes/intro-algo/5/","title":"5. Trie-Tree, Extending Data Structures","section":"Introduction to Algorithms","content":" Trie 树（前缀树） # 前缀树的结构是这样的：每个结点的一个分支代表一位数据。这里的一位不一定是一个 bit，也可以是一个字符等，因为前缀树经常被用在字符串处理，如输入时的提示。在这里，节点内部并不需要保存 key，因为其所有位都已经表示在了路径上。\n如果每一个分支不只保存一个位，将唯一子树与其父节点合并，就变成了基数树。基数树需要保存的路径信息变多了，但不再使用无用的结点。\nHuffman 编码 # Huffman 编码就是一种使用前缀的编码方法，其核心是前缀树的构建。Huffman 编码采用这样的基于概率的前缀构建：将所有可能的值作为叶子，并不断合并频率和最小的两个节点，最终构成一颗二叉前缀树。\n树构建完成之后，在每一个节点，以左侧为 0，右侧为 1，构建前缀编码。Huffman 编码是最优的前缀编码。类似地，自顶向下构建前缀树，在每一步尽量使两侧概率相等的编码方式称为 Shannon-Fano 编码，这种方法不一定总能得到最优编码。\n数据结构的扩展 # 在这里，数据结构的扩展指的是在原有数据结构上做出一些修改，使得其能够支持更多的特性。这些修改包括设计新的操作，以及增加可能的域。\n动态顺序统计 # 为了获得一个集合里每个元素的次序，在红黑树结点的基础上增加一个域 size，用于保存以这个节点为根的子树的总结点数。即顺序统计量。定义中序遍历的顺序为这个元素的秩。\n给定秩，寻找相应的元素。从根出发，左子树的 size+1 就是这个节点所在的位置。判断和输入的大小关系，选择子树继续寻找。\n给定元素寻找秩的过程与之相反。从这个节点回溯到根，将所有的左子树 size+1 加起来，就是这个结点的秩。两种查找的复杂度均为 $O(\\log_2n)$。\n为了维护 size 域的值，在每一次插入和删除时，都需要回溯至根来修改，复杂度为 $O(\\log_2n)$。此外，当发生旋转时，也要分别修改 size。修改的复杂度为 $O(1)$，所以插入和删除的复杂度不变，仍为 $O(\\log_2n)$。\n区间树 # 接下来，我们以红黑树为基础，构建一个可以保存区间对象的数据结构，以展示如何进行数据结构的扩展。\n基础数据结构：每个节点包含一个区间信息，并以区间的低端点作为其关键字。\n附加信息：每个节点额外维护一个值 max，它是以这个节点为根的子树中的所有区间端点的最大值。\n对信息的维护：每个结点的 max 为 max(x.int.high, x.left.max, x.right.max)。于是，更新这个属性的复杂度为 $O(\\log_2n)$。实际上，操作的复杂度只有 $O(1)$。\n设计新的操作：查找与指定区间相交的区间。在查找时，如果左子结点的 max 大于查找目标的左端点，说明左子树中有重叠的部分，则进入左子树继续查找。\n"},{"id":18,"href":"/notes/programming-scala/6/","title":"6. Collections, Extractor, etc","section":"Programming in Scala","content":" List 与 ListBuffer # Scala 的 List 是一个 sealed abstract class，有两个子类：:: 和 Nil。其中 Nil 是一个 case object。所以，不能直接使用 new List()，只能通过 List.apply() 来调用 :: 类。由于 Nil extends List[Nothing] 且 List 是协变的，它能兼容任何列表类型。\n::（cons, construct）类表示有元素的列表。它的构造接收两个参数，即列表的 head 元素和 tail 列表。然后，List 类中定义了这样的方法。要记得以冒号结尾的操作符是右结合的，所以：\ndef :: [B \u0026gt;: A](elem: B): List[B] = new ::(elem, this) 使得 1 :: 2 :: Nil 这样的构造方式得以实现。\n现在我们再来考虑类型问题。从结果上来说，一些不同类型的对象进行 :: 操作，最终得到的结果应当是一个以其公共父类为类型参数的列表。这里通过上面这个方法的类型参数得以实现。当：\napple :: List(orange) 这里的 :: 方法在 List(orange) 上被调用，那么上面的类型参数 A 是 Orange，B 则 应该是 A 的一个父类型。又因为接收的参数也是 B 类型，所以最终 B 被决定为 Fruit 类型。\n接下来我们讨论 ListBuffer。考虑一个简单的 map 函数，如果直接用列表递归的方式来实现，由于 :: 是右结合的，我们会得到一个糟糕的没有尾递归的函数：\ndef map[B](f: A =\u0026gt; B): List[B] = xs match { case Nil =\u0026gt; Nil case x :: xs1 =\u0026gt; f(x) :: xs1.map(f) } 面对这种情况，可以使用 ListBuffer，通过 += 来向尾部追加这些结果。ListBuffer 中 addOne 的实现是这样的：（Scala 中 List 的 map 没有直接使用 ListBuffer，但实质是一样的）：\ndef addOne(elem: A): this.type = { val last1 = new ::[A](elem, Nil) if (len == 0) first = last1 else last0.next = last1 last0 = last1 len += 1 this } 可以看到，其基本想法是，直接把列表末尾的那一个 :: 对象的 next 从原来的 Nil 修改为新的最后一个 :: 对象。这样，前后追加和转换成 List 的操作都是 O(1) 的。当然，如果在输出为列表之后还要进行追加，仍然要进行复制，不过这种情况很少见。ListBuffer。这样做之所以可行，是因为 :: 类的定义中，next 变量是一个 private[scala] var。这样，scala 包中的集合可以直接调用它，但对于用户来说，只有只读的 tail 可以访问，next 访问不到。于是，List 类仍然是 Immutable 的。\nfinal case class :: [+A](override val head: A, private[scala] var next: List[A @uncheckedVariance]) extends List[A] { override def isEmpty: Boolean = false override def headOption: Some[A] = Some(head) override def tail: List[A] = next } 总之，最终我们实现了一个 Immutable 的列表。让其对外部成为不变类的原因是，我们得以让许多列表共享中间的 :: 链表节点结构，还分别提供了 prepend 和 append 操作。通常，List 的 :: 更适合函数式、递归的分治，ListBuffer 的 += 更适合传统的命令式编程范式。\nfor 表达式 # for 表达式会被翻译成对应的 map flatmap filter foreach 的序列。也可以只实现其中的一部分：\n实现 map，允许单个生成器的 for 表达式 实现 map 和 flatMap，可以允许多个生成器的 for 表达式 实现 foreach，可以允许 for 循环 实现 filter，可以允许 for 表达式中的 if 条件 其中，通过将 flatMap 翻译为 for 表达式，实现了对 Monad 的访问的比较好的语法糖。\n以一系列在一个两层嵌套的列表 val ll = List(List(1), List(2)) 上的操作为例，对于这样的 for 表达式：\nfor { a \u0026lt;- ll } yield a.toArray for { a \u0026lt;- ll; b \u0026lt;- a if b % 2 == 0 } println(b) 等同于：\nll map { _.toArray } ll flatMap { _ filter (_ % 2 == 0) } foreach println 比较特殊的情况是，for 表达式中生成器的左边不是一个简单的变量名而是一个模式。为了避免抛出 MatchError，需要先对是否符合模式进行一次判断，因为在 for 表达式中，不能匹配的元素将会被直接抛弃。\nval ll = List(List(1, 2), List(2)) for { List(a, b) \u0026lt;- ll } yield a + b ll map { case List(a, b) =\u0026gt; a + b } // MatchError ll filter { case List(_, _) =\u0026gt; true; case _ =\u0026gt; false } map { case List(a, b) =\u0026gt; a + b } // Works as for expression 现在我们终于可以解释 for 表达式强大的适配能力和\u0026quot;自动选择合适的结果集合类型\u0026quot;的能力了。实际上，这些工作都是由 map flatMap filter 这些方法完成的。\nScala 集合系统 # 集合 Trait 提供的方法 # Traversable Iterable Seq IndexedSeq Vector ResizableArray GenericArray LinearSeq MutableList List Stream Buffer ListBuffer ArrayBuffer Set SortedSet TreeSet HashSet(immutable and mutable) LinkedHashSet BitSet EmptySet, Set1, Set2, Set3, Set4 Map SortedMap TreeMap HashMap(immutable and mutable) LinkedHashMap EmptyMap, Map1, Map2, Map3, Map4 Traversable 定义了我们用到的绝大部分方法：判断尺寸，折叠，foreach，map，mkString，++，groupBy，集合间的转换等等。Iterable 有两个重要的方法返回 Iterator ：grouped 和 sliding，分别提供元素分组和滑动窗口。zip 也由这个特质提供。\n下面的三个特质 Seq Set 和 Map 都继承了 PartialFunction，拥有 definedAt 方法，目的是让我们以 seq(index) 的方式进行下标访问，以 set(elem) 的方式测试是否存在，以及以 map(key) 的方式取得值。\nSeq 提供的有用方法包括 union diff intersect distinct sorted sortBy sortWith reverse update 等。其下的两个特质，LinearSeq 和 IndexedSeq 分别标记着擅长 prepend、append 操作的链表实现和擅长随机访问的数组实现。\nMap 的 get 方法返回的是一个 Option[V] 类型的对象，因此是安全的。\n集合之间的相等性判断是基于元素的，而无视类型。因此，可变集合不应当被用于 HashMap 的 Key。\n创建集合 # 除了熟悉的 apply empty 之外，几乎所有的集合类型的伴生对象还提供了一系列创建集合的其他方法，包括 concat fill tabulate range iterate 等，由于接收的参数都是表达式而非直接的变量，提供了一些高阶的使用方式。\nList.concat(Array(1, 2), Traversable(3, 4)) // List[Int] = List(1, 2, 3, 4) import scala.util.Random List.fill(3)(1) // List(1, 1, 1) List.fill(2, 3)(1) // List(List(1, 1, 1), List(1, 1, 1)) List.fill(2, 3)(Random.nextInt(10)) // List(List(1, 7, 4), List(1, 5, 3)) List.tabulate(5)(_ + 1) // List(1, 2, 3, 4, 5) List.tabulate(3, 3)(_ * 10 + _) // List(List(0, 1, 2), List(10, 11, 12), List(20, 21, 22)) List.range(10, 0, -1) // List(10, 9, 8, 7, 6, 5, 4, 3, 2, 1) List.iterate(5, 5)(_ + 1) // List(5, 6, 7, 8, 9) List.iterate(2, 5)(_ * 2) // List(2, 4, 8, 16, 32) collection.JavaConversions 中提供了 Iterable Iterator Buffer Map Set 转换到对应的 Java 对象的隐式转换。因为 Scala 集合的底层与 Java 集合兼容，所以转换的代价是 O(1) 的。甚至，两次转换后得到的其实还是原来的集合。\n具体的不可变集合类 # Stream # Stream 与列表类似，使用和列表相似的 #:: 和 Stream.empty 来构建，但流是惰性的。在没有访问的情况下，其整个 tail 都是尚未求值的。因此可以构建一个无限递归的无限流：\ndef fibForm(a: Int, b: Int): Stream[Int] = a #:: fibForm(b, a + b) val fibList = fibForm(1, 1).take(7).toList // List(1, 1, 2, 3, 5, 8, 13) Vector # Vector 是一个适合复杂访问的序列类型。其内部是一个宽而浅、每个节点能装下 32 个元素的树结构。虽然理论上其访问的时间是对数级的，但实际上五层的树结构已经足以装下 2^30 个元素了。因此，访问的时间可以认为事实上是常量级别时间的。基于同样的原因。当在向量中改变一个元素时，只需要复制从根部到这个节点的所有节点，也就是不超过五个，其需要复制的量级也是\u0026quot;事实上的常量级别\u0026quot;。\nMap \u0026amp; Set # Scala 中的 HashSet HashMap 实现和 Vector 类似，也是通过这种形式的一棵树，只不过改用了哈希前缀树的形式。TreeSet 和 TreeMap 则是使用红黑树来实现。\nBitSet 内部使用 Long 来二进制地存储整数。如果需要一系列几百量级以内的整数集合，使用这个数据结构的效率非常高，因为一个 Long 就能保存 64 个整数位置。\n最后，ListMap 以键值对列表的形式存储。仅当第一个元素被经常访问时，其效率才比较高，并不常用。\n具体的可变集合类 # DoubleLinkedList 是双向链表，在使用迭代器迭代时删除元素的效率是 O(1) 的（单向链表删除的效率是 O(n) 的）。\nMutableList 是 mutable.LinearSeq 的默认实现，包括一个单向链表和一个指向最后一个节点的引用。这样，在向列表追加时效率从 O(n) 提升到了 O(1)。\n可变版本的 Queue 用 += 替换了 enqueue，dequeue 也只是直接返回（因为队列本身发生了变化）。\n数组 # 使用数组 # Scala 中的 Array 是 Java 数组的简单包装，但这个类兼容于 Seq 引用，也支持 Seq 的操作。不过，这两件事并不是以同样的方式实现的。如果用一个 Seq 类型的引用接收数组，数组会被隐式转换为 WrappedArray。在这个类上调用方法，得到的仍然是 WrappedArray。但如果直接在数组上进行调用，会被隐式转换为 ArrayOps，得到的结果仍然是数组，ArrayOps 可以被回收，现代虚拟机甚至能把这个过程内联掉。\nval seq: Seq[Int] = a1 // Seq[Int] = WrappedArray(1, 2, 3) seq.reverse // Seq[Int] = WrappedArray(3, 2, 1) val ops: collection.mutable.ArrayOps[Int] = a1 // scala.collection.mutable.ArrayOps[Int] = [I(1, 2, 3) ops.reverse // Array[Int] = Array(3, 2, 1) 这种方式的实现方法是，转换为 ArrayOps 的隐式转换定义在 Predef，转换为 WrappedArray 的转换被定义在 scala.LowPriorityImplicits 中，这个类是 Predef 的超类，所以优先级更低。\n泛型数组 # 在 Java 中，由于历史原因，数组不是泛型的，所以有：\npublic \u0026lt;T\u0026gt; void fun() { System.out.println(new T[0]); } // 错误: // 创建泛型数组 // public \u0026lt;T\u0026gt; void fun() { System.out.println(new T[0]); } // ^------^ 由于 Scala 的数组是直接用 Java 数组表示的，这个问题同样存在。\ndef fun[T](t: T) = Array(t) // error: No ClassTag available for T // def fun[T](t: T) = Array(t) // ^ 这时我们需要为数组提供一个 ClassTag，作用类似于泛型的类型参数，用来表示被擦除的类型。\ndef fun[T: ClassTag](t: T) = Array(t) // [T](t: T)(implicit evidence$1: scala.reflect.ClassTag[T])Array[T] 大多数情况下，Scala 编译器能够自动推断出 ClassTag 的值。从上面的 shell 提示可以看到，这个写法的结果实际上是为 fun 增加了一个名为 evidence$1 的隐式参数而已。这个参数的类型是 ClassTag，位于反射包中，编译器在构建数组是会使用它。\n当然，Scala 编译器的推断能力也不是无限的。如果这个类型本身是另外一个类型参数，我们就无法在这里进行推断，需要把外围的函数也加上 ClassTag。\ndef f[U](x: U) = fun(x) // error: No ClassTag available for U // def f[U](x: U) = fun(x) // ^ def f[U: ClassTag](x: U) = fun(x) // [U](x: U)(implicit evidence$1: scala.reflect.ClassTag[U])Array[U] 集合性能总结 # head tail apply update prepend append insert immutable List O(1) O(1) O(n) O(n) O(1) O(n) - Stream O(1) O(1) O(n) O(n) O(1) O(n) - Vector 事实O(1) 事实O(1) 事实O(1) 事实O(1) 事实O(1) 事实O(1) - Stack O(1) O(1) O(n) O(n) O(1) O(n) - Queue 摊还O(1) 摊还O(1) O(n) O(n) O(n) O(1) - Range O(1) O(1) O(1) - - - - String O(1) O(n) O(1) O(n) O(n) O(n) - mutable ArrayBuffer O(1) O(n) O(1) O(1) O(n) 摊还O(1) O(n) ListBuffer O(1) O(n) O(n) O(n) O(1) O(1) O(n) StringBuilder O(1) O(n) O(1) O(1) O(n) 摊还O(1) O(n) MutableList O(1) O(n) O(n) O(n) O(1) O(1) O(n) Queue O(1) O(n) O(n) O(n) O(1) O(1) O(n) ArraySeq O(1) O(n) O(1) O(1) - - - Stack O(1) O(n) O(n) O(n) O(1) O(n) O(n) ArrayStack O(1) O(n) O(1) O(1) 摊还O(1) O(n) O(n) Array O(1) O(n) O(1) O(1) - - - lookup add remove min immutable HashSet/HashMap 事实O(1) 事实O(1) 事实O(1) O(n) TreeSet/TreeMap log(n) log(n) log(n) log(n) BitSet O(1) O(n) O(n) 事实O(1) ListMap O(n) O(n) O(n) O(n) mutable HashSet/HashMap 事实O(1) 事实O(1) 事实O(1) O(n) WeakHashMap 事实O(1) 事实O(1) 事实O(1) O(n) BitSet O(1) 摊还O(1) O(1) 事实O(1) 视图 View # 视图是实现惰性求值集合的方法。如果不使用视图，可以这样实现一个惰性的集合：\ndef lazyMap[T, U](l: List[T], f: Int =\u0026gt; U) = new Iterable[U] { def iterator = l.iterator map f } 有了 View，我们就可以交给 Scala 来实现一个惰性求值的，元素完全一样的集合：\nList(1, 2, 3).view.map(_ + 1).map(_ * 2).force // Seq[Int] = List(4, 6, 8) List(1, 2, 3).view.map(_ + 1).map(_ * 2).map(_ - 1).filter(_ \u0026gt; 5).slice(0, 2).reverse // scala.collection.SeqView[Int,Seq[_]] = SeqViewMMMFSR(...) 从得到的中间结果的类型可以看到，对于每一次操作，SeqView 的类型后面会多一个字母，表示封装了一定的操作。而当我们用 force 取得结果时，运算的中间结果并不会被创建，这非常重要。考虑这样一个情形：\naLargeCollection.take(100000).find(condition) aLargeCollection.view.take(100000).find(condition) 由于 View 的惰性求值，如果能够在集合的前半段就找到目标，那么长达 100000 的中间结果的大部分将完全不存在于内存中。\n另一个有意义的用法是，当我们想要修改可变集合中的一个窗口，也可以使用 View。这样，修改和切片两个操作就被很好地解耦。\nval a = ArrayBuffer(1, 2, 3, 4, 5, 6) val part = a.view.slice(1, 4) for (i \u0026lt;- 0 until part.length) part(i) += 1 a // scala.collection.mutable.ArrayBuffer[Int] = ArrayBuffer(1, 3, 4, 5, 5, 6) 不过这并不是一个非常好的例子，因为创建闭包和视图所消耗的 CPU 和内存几乎一定大于在这么小的集合上进行操作的消耗。如果代码是有副作用的，那么在惰性求值的情况下，事情会变得更加复杂。\n迭代器 Iterator # Iterator 提供了大部分 Seq Traversable Iterable 中提供的方法，但行为不太一样。例如，迭代器的 map 返回另一个迭代器，并且只包含迭代器后面的元素的结果，foreach 同理。而且 map 和 foreach 都会让迭代器到达集合的末尾，继续调用 next 则会抛出 NoSuchElementException。对于 dropWhile，迭代器会在找到第一个不符合条件的元素后停下。唯一能改变这种情况的标准方法是使用 duplicate:\nval it = Iterator(1, 2, 3, 4, 5) val t = it.duplicate t._1.next // 1 t._1.next // 2 t._1.next // 3 it.next // 4 t._2.next // 1 这样得到的两个迭代器之间是独立的，但原来的 it 则会跟随两个迭代器中更快的一个。显然这种行为很难控制，所以原则上我们可以认为原来的 it 不再可用。\n针对这样的特点，Scala 类库在 Traversable 和 Iterator 之上抽象出了 TraversableOnce，表示可以一次性地遍历访问，但访问后集合的状态不作保证。\n不过迭代器这样的特点也有一点不方便，例如我们实现一个类似 skipWhile 的函数：\ndef skipWhile[T](it: Iterator[T])(pred: T =\u0026gt; Boolean) = while (pred(it.next)) 这样的话，第一个不符合条件的元素被识别出来的时候，我们就已经失去它而指向下一个了。这时需要使用的是 BufferedIterator 这个 Trait 的实例，可以使用 head 方法来查看第一个元素而不会跳过它。\ndef skipWhile[T](ite: BufferedIterator[T])(pred: T =\u0026gt; Boolean) = while(pred(it.head)) it.next() skipWhile(it.buffer)(pred) 提取器 Extractor # 简单的提取器 # 目前为止，我们见到的模式匹配都和 case class 一起出现，但这并不是必须的。实际上，case 关键字的作用只是提供 unapply 方法而已。下面是一段解析 Email 地址的代码：\nobject Email { def unapply(str: String): Option[(String, String)] = { val parts = str split \u0026#34;@\u0026#34; if (parts.length == 2) Some(parts(0), parts(1)) else None } } List(\u0026#34;abc@efg\u0026#34;, \u0026#34;abcdefg\u0026#34;) flatMap { case Email(a, b) =\u0026gt; Some(a + \u0026#34; AT \u0026#34; + b); case _ =\u0026gt; None } 这样，我们避开了 case class，并没有创建 Email 这样一个类，而是仍然使用 String 来表示，但同样完成了需要的功能。对于通常的 case class，在提供提取器的同时，也会把类本身的结构暴露给调用方。而使用提取器时，我们能够将背后的实现隐藏起来。这样，同时也达到了类似于 getter 的效果，我们可以任意修改后面的实现，只要提供对应的新版提取器即可，不用担心下游代码的兼容性。\n当然，通常还会定义对应的 apply，并可以把对象直接声明为函数类型的子类：\nobject Email extends ((String, String) =\u0026gt; String) { def apply(user: String, domain: String) = user + \u0026#34;@\u0026#34; + domain def unapply(str: String): Option[(String, String)] = ... } 提取 0 个或 1 个元素的提取器 # 定义了 unapply 方法的对象就可以称作一个提取器 Extractor，无论有没有对应的 apply 方法。上面的例子返回多个变量，所以返回 Option[Tuple]。如果只返回一个，就无需使用元组。如果不提取任何值，就直接返回布尔值，不使用 Option 包装。这种情况通常很常见，尤其是当我们只是把提取器当做一个用来\u0026quot;检查\u0026quot;的工具。例如，一个检查全大写的提取器这样使用：\nobject UpperCase { def unapply(str: String): Boolean = str.toUpperCase == str } strList flatMap { case UpperCase() =\u0026gt; Some(\u0026#34;Y\u0026#34;); case _ =\u0026gt; None} 最后，我们要考虑使用 _* 匹配剩余的多个元素的情况。显然之前的 unapply 已经满足不了我们的需求，Scala 在这里使用 unapplySeq 方法：\nobject Domain { def unapplySeq(whole: String): Option[Seq[String]] = Some(whole.split(\u0026#34;\\\\.\u0026#34;).reverse) } \u0026#34;abc.def.com\u0026#34; match { case Domain(\u0026#34;com\u0026#34;, \u0026#34;example\u0026#34;, \u0026#34;www\u0026#34;) =\u0026gt; \u0026#34;example\u0026#34;; case Domain(\u0026#34;com\u0026#34;, _*) =\u0026gt; \u0026#34;com\u0026#34;; case Domain(x, _*) =\u0026gt; x } // com 标准库中的 List Array 等提取器就是这样通过在伴生对象上实现的。\n通常，使用 case class 能够带来略好的性能（因为 case class 的实现更加简单），并能够得到来自 sealed 关键字的编译错误的帮助，而提取器的灵活性更好。好在，由于调用方的代码并没有任何区别，如果仅仅用来进行模式匹配的话，二者可以无缝切换。\n正则表达式 # Scala 提供的正则表达式类位于 scala.util.matching.Regex，有多种方式来创建：\nimport scala.util.matching.Regex new Regex(\u0026#34;\\\\n[0-9]+\u0026#34;) // scala.util.matching.Regex = \\n[0-9]+ new Regex(\u0026#34;\u0026#34;\u0026#34;\\n[0-9]+\u0026#34;\u0026#34;\u0026#34;) // scala.util.matching.Regex = \\n[0-9]+ \u0026#34;[0-9]+\u0026#34;.r // scala.util.matching.Regex = [0-9]+ \u0026#34;\\\\d+\u0026#34; findAllIn \u0026#34;ab123c123d\u0026#34; // scala.util.matching.Regex.MatchIterator = \u0026lt;iterator\u0026gt; \u0026#34;\\\\d+\u0026#34;.r findFirstIn \u0026#34;ab123c123d\u0026#34; // Option[String] = Some(123) \u0026#34;[a-z]+\u0026#34;.r findPrefixOf \u0026#34;ab123c123d\u0026#34; // Option[String] = Some(ab) val Decimal = \u0026#34;\u0026#34;\u0026#34;(-)?(\\d+)(\\.\\d+)?\u0026#34;\u0026#34;\u0026#34;.r val Decimal(a, b, c) = \u0026#34;-1.23\u0026#34; // a: String = - // b: String = 1 // c: String = .23 可以看到，我们可以直接使用正则表达式对象进行模式匹配，并通过正则表达式的分组来提取字符串对象，同样是通过 unapplySeq 方法实现的。\n注解 # 注解实际上是调用了注解类的构造方法，所以能够支持构造方法所能支持的默认参数、带名参数、变长参数等，也能支持表达式，这点比 Java 要强。不过，如果要将一个注解传到其他地方，由于注解不是表达式，所以不能使用 @，而需要使用 new。常用的 Scala 提供的注解包括 @Serializable @transient SerialVersionUID scala.reflect.BeanProperty（生成getter setter）tailrec unchecked（关闭模式匹配检查）native（标记本地方法）volatile 等等。\nScala 不使用 Checked Exception（实际上这个检查也只在 javac 中进行，而不在 java 运行时中进行，所以并没有关系）。如果要让对接的 Java 代码看到 Checked Exception，也是用注解 @throws(classOf[IOException])。遗憾的是，Java 的注解必须在 Java 中编写，这考虑到了两种注解功能上的区别，也考虑到可能的 Scala 自己的反射的存在。\n编写相等性方法 # 常见的 equals 方法的错误 # 编写 equals 方法并不是一件十分简单的事。常见的 eqauls 的错误包括：\n错误的方法签名。Scala 中的 equal 应当是 def equals(other: Any): Boolean，使用 Any 之外的类型作为参数只会得到一个重载。而且，无论 Java 还是 Scala，使用哪一个重载都是由静态类型而非运行时类型决定的。就是说：\nobject A { def f(a: Any) = \u0026#34;Any\u0026#34;; def f(a: String) = \u0026#34;String\u0026#34; } val s: Any = \u0026#34;a\u0026#34; A.f(s) // Any 常见的错误是使用 def equals(other: this.type): Boolean。这种情况下，如果外部传来一个父类引用持有的对象，被选择的就会是我们还没有覆盖的 equals(other: Any)，从而带来错误。进一步，Scala 还把 Any 中的 == 定义为 final 的，这样就不会犯 def ==(other: Any) 的错误。\n另一个常见的错误是没有同时覆盖 hashCode，这会让对象在集合里的行为十分诡异。一个简单有效的办法是把所有有意义的值都放进一个 Tuple，然后使用 Scala 提供的方法，即 override def hashCode = (a, b).##。\n基于类似的原因，不能把可变的值用于相等性的判断，这同样会让集合的行为十分诡异。如果你在一个 HashSet 中保存一个可变的对象，那么 set contains elem 将会是 false，因为 HashCode 发生了变化。但 set.iterator contains elem 又会是 true，因为它的确在集合的数据结构里。总之，不要这样做。\n定义相等性 # 除了上面三种较为容易解决的问题之外，我们要来看一下相等关系的性质。相等关系应该满足这样的数学性质：\n自反，即 x equals x 为真。 对称，即 x equals y 与 y equals x 相等。 可传递，即当 x equals y，y equals z，那么 x eqauls z。 一致，即多次调用同一个表达式的值相等。 对空值 NULL，值为假。 当面向对象的继承关系和相等性出现在一起时，情况就变得十分复杂了。考虑一个简单的 Point 类及其子类：\nclass Point(val x: Int, val y: Int) { override def hashCode = (x, y).## override def equals(other: Any) = other match { case that: Point =\u0026gt; this.x == that.x \u0026amp;\u0026amp; this.y == that.y case _ =\u0026gt; false } } object Color extends Enumeration { val Red, Orange, Green, Blue = Value } class ColoredPoint(x: Int, y: Int, val color: Color.value) extends Point(x, y) { ... } 我们直觉上写出来的代码大致是这样的：\noverride def equals(other: Any) = other match { case that: ColoredPoint =\u0026gt; this.color == that.color \u0026amp;\u0026amp; super.equals(that) case _ =\u0026gt; false 注意我们这里并不需要重写 hashCode 方法（虽然推荐这样做）。原因是，实际上我们只需要保证两个相等的对象的 hashCode 相等，而不需要反过来保证。对于 HashMap 这样的类来说，其实现会首先使用 hashCode 进行查找，但最终仍然是通过 equals 方法来判断是否相等。毕竟避免哈希碰撞是大部分情况下是不可能的，而 HashMap 会使用拉链法来解决这个问题。\n不过，这样的实现是存在问题的：\nval p = new Point(1, 2) val cp = new ColoredPoint(1, 2, Color.Red) p == cp // true cp == p // false mutable.HashSet[Point](p) contains cp // true mutable.HashSet[Point](cp) contains p // false Point 类的 equals 方法只关心坐标，所以能够返回 true。这样，相等关系的对称性就被破坏了。\n这时我们想到的修补方法可能是，允许 cp equals p。也就是：\noverride def equals(other: Any) = other match { case that: ColoredPoint =\u0026gt; // same as before case that: Point =\u0026gt; super.equals(that) case _ =\u0026gt; false } 现在 cp == p 和 p == cp 都是真值了。但是，相等关系的可传递性又被破坏了，因为 redp == p，bluep == p，但 redp != bluep，除非我们完全放弃对颜色相等性的判断。\n看起来，放宽 equals 的条件是不可行的。更加可行的方式是，不允许 ColoredPoint 与 Point 相等。也就是说，在父类 Point 中：\noverride def equals(other: Any) = other match { case that: Point =\u0026gt; this.x == that.x \u0026amp;\u0026amp; this.y == that.y \u0026amp;\u0026amp; this.getClass == that.getClass case _ =\u0026gt; false } 不过，这样定义的结果是，基于 Point 产生的匿名子类的对象：new Point(1) { override val y = 2} 和 p 也不相等了。\n一种变通的方式是使用 canEqual 方法。\nclass Point(val x: Int, val y: Int) { override def hashCode = (x, y).## override def equals(other: Any) = other match { case that: Point =\u0026gt; (that canEqual this) \u0026amp;\u0026amp; (this.x == that.x) \u0026amp;\u0026amp; (this.y == that.y) case _ =\u0026gt; false } def canEqual(other: Any) = other.isInstanceOf[Point] } class ColoredPoint(x: Int, y: Int, val color: Color.Value) extends Point(x, y) { override def hashCode = (super.hashCode, color).## override def equals(other: Any) = other match { case that: ColoredPoint =\u0026gt; (that canEqual this) \u0026amp;\u0026amp; super.equals(that) \u0026amp;\u0026amp; this.color == that.color case _ =\u0026gt; false } override def canEqual(other: Any) = other.isInstanceOf[ColoredPoint] } 大致上可以认为，我们手动判断了相等关系的可交换性。\n泛型的相等性 # 考虑一个二叉树类的定义，先暂时忽略变型问题：\ntrait Tree[T] { def elem: T def left: Tree[T] def right: Tree[T] } object EmptyTree extends Tree[Nothing] { def elem = throw new NoSuchElementException(\u0026#34;EmptyTree.elem\u0026#34;) def left = throw new NoSuchElementException(\u0026#34;EmptyTree.left\u0026#34;) def right = throw new NoSuchElementException(\u0026#34;EmptyTree.right\u0026#34;) } class Branch[T]( val elem: T, val left: Tree[T], val right: Tree[T]) extends Tree[T] { override def equals(other: Any) = other match { case that: Branch[T] =\u0026gt; this.elem == that.elem \u0026amp;\u0026amp; this.left == that.left \u0026amp;\u0026amp; this.right == that.right case _ =\u0026gt; false } } 但是，编译时，Scala 编译器会提示我们，由于类型擦除，模式匹配中的类型参数无法进行比较。大多数情况下这还可以接受（不同类型引用的对象很难相等），但当遇到继承关系时，就会出现问题：\nval b1 = new Branch[List[String]](Nil, EmptyTree, EmptyTree) val b2 = new Branch[List[Int]](Nil, EmptyTree, EmptyTree) b1 == b2 // true 这里，由于实际的对象都是 Nil，两个对象被判断为相等。但实际上，二者的类型参数是不同的。至于这种情况应当判定为相等还是不相等，看法不一。不过，如果只是想消除编译器的警告的话，可以简单地标明类型参数的存在：\ncase that: Branch[_] =\u0026gt; ... 这样，至少我们向编译器表明，我们的确知道这里可以是任何类型，编译器也不会再抛出 unchecked 警告。下划线也可以使用一个小写字母来代替，总之它不是这个方法所在对象的类型参数 T。相应地，canEqual 方法也去匹配一个 Branch[_] 类型。\n"},{"id":19,"href":"/notes/core-java-impatient/6/","title":"6. Collections, Streams","section":"Core Java for Impatients","content":" 集合 # Collections.nCopies(n, o) 返回一个特殊的内部类 Coolections$CopiesList，能够作为多个拷贝的 List 来使用，但实际只存储一份。 Queue 是一个队列，Deque 是双向队列。 鼓励在代码中使用接口。如 List\u0026lt;T\u0026gt; l = new ArrayList\u0026lt;\u0026gt;()。 同样地，在编写有关集合的代码时，尽量使用接口作为参数，以扩大适用范围。 Collections 的一些静态方法 # 静态方法 功能 boolean disjoint(Collection\u0026lt;?\u0026gt; c1, Collection\u0026lt;?\u0026gt; c2) 判断是否有重复 void copy(List\u0026lt;? super T\u0026gt; dest, List\u0026lt;? extends T\u0026gt; src) 复制 boolean replaceAll(List\u0026lt;T\u0026gt; list, T oldVal, T newVal) 替换 void fill(List\u0026lt;? super T\u0026gt; list, T obj) 填充 int frequency(Collection\u0026lt;?\u0026gt; c, Object o) 数量 int indexOfSubList(List\u0026lt;?\u0026gt; source, List\u0026lt;?\u0026gt; target) 子列表，也有 last方法 迭代器 # 由 Iterable\u0026lt;T\u0026gt; 定义的方法：Iterator\u0026lt;T\u0026gt; iterator()，使用 hasNext() 和 next() 来访问所有元素。也可以直接使用 foreach 循环。 remove() 方法用来移除刚刚返回的元素，而不是现在指向的元素。这意味着，两次 next() 之间只能调用一次 remove()。 ListIterator\u0026lt;T\u0026gt; 作为子接口，加入了 set add 和 previous 方法。 许多非线程安全的集合类的迭代器是 fail-fast 的。这意味着如果其他线程改变了集合，将会抛出 ConcurrentModificationException。 常见的集合 # Set # SortedSet 接口提供顺序访问，NavigableSet 接口提供访问邻居元素的方法。TreeSet 实现了这两个接口。\nHashSet 的性能与元素的 hashCode() 相关。显然，碰撞越弱，性能越好。\nSet 的元素必须实现 Comparable\u0026lt;T\u0026gt;，或者在构造函数提供 Comparator\u0026lt;T\u0026gt;。\nSortedSet 提供的方法：first() last() headSet() subSet() tailSet。\nNavigableSet 提供的方法：higher() ceiling() floor() lower() pollFirst() pollLast() 等。\nMap # TreeSet 提供顺序访问，但性能更弱。\n当 Key 不存在时，会返回 null。但对于使用了装箱类的 Map，对 null 的拆箱操作就会引发异常。因此，最好使用 getOrDefault() 方法，提供缺省值。\nmerge 方法可以用来更新 Map 中的计数器：\ncounts.merge(word, 1, Integer::sum); 如果 word 键不存在，就会新建并设为 1。否则，就会加 1。\nHashTable 是线程安全的，不接受 null。相比之下 ConcurrentHashMap 更加实用。\n方法 功能 V putIfAbsent(K key, V value) 如果不存在，则插入。如果存在，不修改，返回 Map 内的值。 V compute(K key, V value, BiFunction\u0026lt;...\u0026gt; remappingFunction) （系列）如果存在 key 的对应 v，就对 v 和传入的 value 进行运算。如果运算结果是 null，就删除 Entry。 V remove(Object key) 删除并返回 value。replace() 类似。 boolean remove(Object key, Object value) 如果 Map 内的 key 和 value 都对应，删除。 Set\u0026lt;K\u0026gt; keySet() 类似的还有 values() entrySet() Properties - 一种 Map # Properties setings = new Properties(); settings.put(\u0026#34;width\u0026#34;, \u0026#34;200\u0026#34;); try (OutputStream out = Files.newOutPutStream(path)) { settings.store(out, \u0026#34;ProgramProperties\u0026#34;); } 得到：\n#ProgramProperties #{DateTime} width=200 Properties 文件是 ASCII 编码的。Unicode 字符将会以 escape 形式（\\uxxxx）存储。\ntry (InputStream in = Files.newInputStream(path)) { settings.laod(in); } String title = settings.getProperty(\u0026#34;title\u0026#34;, \u0026#34;defaultValue\u0026#34;); // don\u0026#39;t use get(), for it consumes (Object, Object) rather than String System.getProperties(); // Some system properties BitSet # 内部实现是一个 long[]，因此效率比 boolean[] 更高。（Java 规范并没有规定 boolean 的大小。）注意这个类并没有实现 Collection\u0026lt;Integer\u0026gt; 接口，是一个独立的类。\n提供的方法包括某个或某个范围内的 get set clear flip 等逻辑操作、previous next 等。\n枚举 Set 和 Map # EnumSet 包含静态工厂方法：\nSet\u0026lt;WeekDay\u0026gt; always = EnumSet.allOf(Weekday.class); Set\u0026lt;WeekDay\u0026gt; never = EnumSet.noneOf(Weekday.class); Set\u0026lt;WeekDay\u0026gt; workday = EnumSet.range(Weekday.MON, Weekday.FRI); Set\u0026lt;WeekDay\u0026gt; three = EnumSet.of(Weekday.MON, Weekday.TUE, Weekday.WED); EnumMap 是以枚举类型为 key，任意指定 value 的 Map。\nEnumMap\u0026lt;Weekday, String\u0026gt; map = new EnumMap\u0026lt;\u0026gt;(Weekday.class); map.put(WeekDay.MON, \u0026#34;abc\u0026#34;); 队列，栈，优先级队列 # Stack 类是历史遗留，不应该被使用。通常，使用 Queue 和 Deque 已经足够。如果不关心线程安全，可以直接使用 ArrayDeque。\n优先级队列以任意顺序插入，而只会弹出最小元素。可以用于作业调度，始终弹出优先级最高的任务。使用 add() 和 remove()。\nWeakHashMap # 弱哈希表解决这样一个问题：即使 key 已经不再被使用了，由于 Map 对其的引用，它并不会被 GC 掉。技术上说，WeakHashMap 使用 WeakReference。总之，当其唯一引用来自 Map 时，就自动删除。\n视图 # 集合视图（View）是一个轻量级的集合对象，它可以用来访问元素，但并不储存元素。与 SQL 中的视图非常类似。例如，keySet values Arrays.asList 方法都是这样，其中所有引用的元素都同时被原来的集合引用。这也意味着对这些元素的修改将会体现到原来的集合中。下面是一些常见的视图。\n// Ranges List\u0026lt;String\u0026gt; nextFive = sentence.subList(5, 10)； // in Navigable interface NavigableSet\u0026lt;E\u0026gt; headSet(E toElement, boolean inclusive); NavigableSet\u0026lt;E\u0026gt; subSet(E fromElement, E toElement, boolean inclusive); NavigableSet\u0026lt;E\u0026gt; tailSet(E fromElement, boolean inclusive); // other set and map, similar SortedSet\u0026lt;String\u0026gt; asOnly = words.subSet(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;); //subMap, headMap, tailMap... // empty views and singleton views Collections.emptyMap(); Collections.singletonMap(); // Immutable/readonly views public class Person { public List\u0026lt;Person\u0026gt; getFriends() { return Collections.unmodifiableList(friends); } } 这样，在视图上进行插入会抛出异常。另外，由于我们之前看到的泛型的不完全安全性，可以让视图来帮我们检查插入对象的类型：\nList\u0026lt;String\u0026gt; strings = Collections.checkedList(new ArrayList\u0026lt;\u0026gt;(), String.class); Collections 还提供了适合并发的视图，但更加推荐使用 concurrent 包中的数据结构，而非这些视图。\n流 # 创建流 # 流不直接存储数据。它按需生成元素，或者直接引用自集合。 流不改变源数据。例如，filter 不会删除集合中的元素。 流可以是延迟执行的，因此也可以是无限的。例如，我们只从流中取出 5 个元素，那么就只会对 5 个元素执行运算。 由于以上的原因，只要流还没有完全使用结束，就不能修改原集合。否则，行为是未定义的。 大多数集合可以使用 Collection 接口的 Stream()。parallelStream() 生成一个并行流。\n对于数组，使用静态方法 Stream.of()。这里实际上是一个可变参数，因此把元素一个个传入也是可行的。\nArrays.stream(arrya, from, to) 为数组的一部分生成流。Stream.empty() 返回一个空流。这里会自动进行泛型推断，也可以为方法指定泛型的类型变量 Stream.\u0026lt;String\u0026gt;empty()。\n要创建无限流，可以使用 generate 或 iterate 方法。\nStream\u0026lt;String\u0026gt; echos = Stream.generate(() -\u0026gt; \u0026#34;Echo\u0026#34;); // a Supplier\u0026lt;T\u0026gt; Stream\u0026lt;Double\u0026gt; ramdoms = Stream.generate(Math::random); Stream\u0026lt;BigInteger\u0026gt; integers = Stream.iterate( BigInteger.ZERO, n -\u0026gt; n.add(BigInteger.ONE)); // a seed and an UnaryOperator\u0026lt;T\u0026gt; to apply on it 除此之外还有很多其他地方的生成流的方法。例如，用于正则表达式的 Pattern 类有一个方法：splitToStream 用来使用正则表达式分割字符串成流。\n流转换 # 对于包含流的流，使用 flatMap 将其展开。limit(n) 用来返回一个包含一定数量元素的子流。skip(n) 与其相反，是跳过前 n 个元素。例如，生成 100 个随机数：\nStream\u0026lt;Double\u0026gt; randoms = Stream.generate(Math::random).limit(100); Stream.concat 将两个流连接起来。当然，这时第一个流不能是无限的。\nsorted 用来对流进行排序。可以使用 Comparator，也可以接收 Comparable 对象。\n最后，peek 方法是一个 \u0026ldquo;透明的\u0026rdquo; 流，不改变流的内容，适合用来打印调试信息甚至打断点：\nintStream.peek(System.out::println).limit(20).toArray(); 这样，将会打印 20 个随机数，并生成相应的数组。这里也可以看出之前说的延迟执行特性。\nOptional # 创建 Optional # Optional.empty(); Optional.of(obj); Optional.ofNullable(obj); // empty if null 使用 Optional # String s = opt.get(); // if empty, throw NoSuchElementException String s = opt.orElse(\u0026#34;\u0026#34;); // default value String s = opt.orElseGet(() -\u0026gt; System.getProperty(\u0026#34;user.dir\u0026#34;)); // calculate default String s = opt.orElseThrow(IllegalStateException::new); // throw an exception // use value in optional, or do nothing opt.ifPresent(results::add); // return void // or check the return value Optional\u0026lt;Boolean\u0026gt; added = opt.map(results::add); //now added may be true or false(return of add) or empty optional(if opt is empty) public static Optional\u0026lt;T\u0026gt; f() { ... } //in the T class Optional\u0026lt;U\u0026gt; g() { ... } Optional\u0026lt;U\u0026gt; result = s.f().flatMap(T::g); // s.f() returns a Optional\u0026lt;T\u0026gt;, flatMap to T , then T::g // overall ,if s.f() presents, call g. otherwise, return empty Optinal\u0026lt;U\u0026gt; 显然，这种方式可以链式地调用，任何一步返回 empty 都会终止。\n收集和处理流 # 收集到集合内 # stream.toArray(); stream.collect(Collectors.toList()); stream.collect(Collectors.toSet()); stream.collect(Collectors.toCollection(TreeSet::new)); // collect to string String result = stream.collect(Collectors.joining()); String result = stream.collect(Collectors.joining(\u0026#34;\\t\u0026#34;)); String result = stream.map(Object::toString).collect(Collectors.joining()); // same with long and double, sum and min and such IntSummaryStatics summary = stream.collect(Collectors.summarizingInt(String::length)); double averageLength = summary.getAverage(); double maxLength = summary.getMax(); // collect to Map Map\u0026lt;Integer, String\u0026gt; idToName = people.collect( Collectors.toMap(Person::getId, Person::getName)); // or Map\u0026lt;Integer, Person\u0026gt; idToPerson = people.collect( Collectors.toMap(Person::getId, Function.identity())); // thows IllegalStateException if has same ids, or Collectors.toMap(Person::getId, Function.identity, (exsitingVal, newVal) -\u0026gt; existingVal); //to keep the old one // if you want to specify the map, since it is the 4th param, must write the 3rd Collectors.toMap(Person::getId, Function.identity, (exsitingVal, newVal) -\u0026gt; existingVal, TreeMap::new); 另外还有相应的 toConcurrentMap 方法。\n归约操作 reduce # 之前见到的count 方法就是一个简单的归约操作。类似地，max min 返回流中的最大值和最小值，返回一个 Optional\u0026lt;T\u0026gt; 对象。当流为空的时候，就不会返回 null。除此之外，还有 findAny 匹配，以及经常和 filter 一起使用的 findFirst。\n如果需要知道流中是否含有匹配元素，使用 anyMatch 方法接受一个 predicate\u0026lt;T\u0026gt;，返回 boolean。类似的还有 noneMatch。\n除此之外，还常常使用 forEach，或者在并行流上可能需要 forEachOrdered。这时是为了使用代码的 \u0026ldquo;副作用\u0026rdquo;。\n此外，Java 提供了强大的 reduce 方法。简单地说，它接收一个二元操作，并对所有元素连续进行这个操作。操作必须满足结合律。这类操作包括最大值最小值、加法、拼接等。\n// sum of all elements, empty if stream is empty Optional\u0026lt;Integer\u0026gt; sum = values.stream().reduce((x, y) -\u0026gt; x + y); Optional\u0026lt;Integer\u0026gt; sum = values.stream().reduce(Integer::sum); // or offer an \u0026#34;start point\u0026#34;, 0 for add, 1 for multiply, etc. act as default value. int sum = values.stream().reduce(0, Integer::sum); 这里，0充当的是运算的起点，即单位元的作用。对于累加是 0，对于累乘就应当是 1。不过，reduce 只能接收一个 (T, T) -\u0026gt; T 类型的方法，即二元操作。如果要进行更复杂的操作：\nOptional\u0026lt;Integer\u0026gt; result = words.reduce((total, word) -\u0026gt; total + word.length(), (total1 ,total2) -\u0026gt; total1 + total2); 这是说，我们需要进行两类累加操作，并分别提供函数。当然，这种情况下将其 map 到一个 IntStream 再处理要简单得多。\nJava 中提供了三种基本类型流 IntStream LongStream DoubleStream，它们的 toArray 得到的是基本类型数组，具有 max sum average 等方法，并可能返回 OptinalInt 等类型。\n分组，分片，下游收集器 # Map\u0026lt;String, List\u0026lt;Locale\u0026gt;\u0026gt; contryToLocales = locales.collect( Collectors.groupingBy(Locale::getCountry)); // group by country // while group by boolean, faster method Map\u0026lt;Boolean, List\u0026lt;Locale\u0026gt;\u0026gt; englishAndOthers = locales.collect( Collectors.partitioningBy(l -\u0026gt; l.getLanguage().equals(\u0026#34;en\u0026#34;))); // concurrent map Collectors.groupingByConcurrent(Locale::getCountry); 上面都是默认使用了 List 作为 Map 的 value。如果要使用其他的，比如 Set，可以提供一个下游收集器（downstream collector）。\nMap\u0026lt;String, List\u0026lt;Locale\u0026gt;\u0026gt; contryToLocales = locales.collect( Collectors.groupingBy(Locale::getCountry, Collectors.toSet())); 类似这里 toSet 用法的还包括：counting summing maxBy(Comparator) minBy(Comparator)，以及比较复杂的 mapping。也有相应的 groupingByConcurrent 等。\nimport java.util.stream.Collectors.* Map\u0026lt;String, Set\u0026lt;String\u0026gt;\u0026gt; counttryToLangs = locales.collect( groupingBy(Locale::getCountry, mapping(Locale::getDisplayLanguage, toSet()))); 如果返回的值是 int long double，就可以用上面出现的 summarizingInt 系列方法来替代 toSet 进行统计。\n当然，下游收集器这种方法只适合在使用 groupingBy 和 partitioningBy 时使用，否则只需要直接对流使用归约 max count reduce 等即可。\n并行流 # 获得并行流：\nwords.parallelStream(); Stream.of(wordArr).parallel(); 显然，并行流中的操作不应该使用共享的内容。也就是说，所有操作都应当是无状态的，可以以任意顺序执行。例如，对字符串中的单词长度进行计数：\n// WRONG way int[] shortWords = new in[12]; words.parallelStream().foreach(s -\u0026gt; { if (s.length() \u0026lt; 12) { shortWords[s.length()]++; // competing condition } }); //Right Way Map\u0026lt;Integer, Long\u0026gt; shortWordCounts = words.parallelStream() .filter(s -\u0026gt; s.length \u0026lt; 12) .collect(Collectors.groupingBy(String::length, counting())); 默认，来自有序集合、range、生成器、迭代器和 sorted 得到的流都是有序的。有序不影响并行。但如果顺序不重要，可以使用 unordered 来提高性能。例如使用 limit 来取出几个元素但并不在乎是哪几个时。又比如，使用 distinct 来让流所有元素保持唯一时。\n又比如使用上面的 groupingBy 操作时，代价相当高。但如果使用 groupingByConcurrent，虽然使元素成为无序的，但可以进行并行操作，提高性能。如果你不是用与顺序有关的下游收集器，就无需考虑顺序问题。\n"},{"id":20,"href":"/notes/intro-algo/6/","title":"6. Dynamic Programming, Greedy, Amortize","section":"Introduction to Algorithms","content":" 动态规划 # 动态规划适合用来求解最优化问题。与分治法类似，通过递归地求解子问题来解决原问题。区别是，分治法可能会重复地多次解决同一个小问题，而动态规划选择将这些小问题的结果保存起来，避免重复多次地求解它们。\n钢条切割问题 # 不同长度的钢条有不同的价格，求最优解。使用递归思想的算法是，长度为 $n$ 的钢条的最优解是：\nCUT_ROD(prices, n) q = 0 for i = i to n q = max(q, prices[i] + CUT_ROD(prices, n - i)) return q 将函数的每次调用作为钢条的一次切割，则最优解为所有可能的切割方法中的最大值。显然，这种方法会大量地重复计算短钢条的最优价格。每当 n 增加 1，程序所用时间差不多增加一倍。\n有两种方法实现动态规划。第一种是带备忘的自顶向下法（top-down with memoization）。额外维护一个数组，记录每个子问题的解即可。另一种是自底向上法。自顶向下通常可以避免一些不需要的子问题计算，但自底向上不需要递归开销，通常具有更小的系数。两种算法的复杂度均为 $O(n^2)$。自底向上的算法是这样的：\nlet r[0..n] be a new array r[0] = 0 for j = 1 to n q = -1 for i = 1 to j q = max(q, prices[i] + r[j - i]) r[k] = q return r[n] 除此之外，对于钢条最优解问题，还需要额外存储最优解的切割方案。\n矩阵链乘法 # 由于多个矩阵相乘满足结合律，且矩阵相乘的代价和矩阵的阶相关，改变加括号的方式可能大大改变矩阵乘法的复杂度。尽量降低每一次乘法的阶数可以大大改善性能。因此，我们将矩阵链乘法问题描述为：求完全括号化方案，使得计算所需的标量乘法次数最少。暴力搜索的复杂度为 $\\Omega(2^n)$。\n与钢条问题类似，把每一次括号化作为一次递归调用。设从 $i$ 到 $j$ 的矩阵链的最优解为 $m[i,j]$，则：\n$$ m[i,j] = \\begin{cases} 0 \u0026amp; \\text{if } i=j \\cr \\min_{i \\leqslant k \u0026lt; j} \\{m[i,k]+m[k+1,j]+p_{i-1}p_kp_j\\} \u0026amp; \\text{if } i \u0026lt; j \\end{cases} $$\n于是，对于 $m[i,j]$，我们需要所有长度小于这个序列的矩阵链的最优解。对于自底向上算法，可以使用三层嵌套循环，分别代表矩阵链长度、位置和递归式中对括号化方式的的遍历操作。可以证明，递归的运行时间是 $\\Theta(n^3)$，并需要 $\\Theta(n^2)$ 的额外空间，比暴力穷举高效得多。\n动态规划原理 # 一个问题的最优解包含其子问题的最优解。比如，一个长钢条的最佳切割方案是两段钢条的最优切割方案构成的。这种性质称为最优子结构。具有这种性质的问题通常也适用于贪心算法。\n求无权最短路径的问题是最优子结构的。这条路径必然是无环的。将路径分解成两半，那么两条路径一定分别是最优解。然而，最长简单路径问题并不符合最优子结构性质。这是因为，在最长简单路径问题中，由于简单路径不能存在环，这两个子问题实际上是相关的，而最短路径问题是无关的。实际上，最长简单路径的问题是 NP 完全的，不太可能在多项式时间内求解。\n除此之外，显然，适合动态规划解决的问题还应该满足重叠子问题的性质。这个性质的核心是，子问题的数量应该是有限的。只有在递归过程中重复地求解同一个问题，才能体现出动态规划的优势。\n另外，算法设计中还需要考虑重构最优解的问题。\n最长公共子序列问题 # 注意，这个问题与最长公共子串不同：在这里，子序列可以不是连续的。这个属性更多地刻画了两个序列的相似性。\n这个问题的最优子结构是这样的：设 $X_m = \\langle x_1, x_2, \\cdots, x_m \\rangle$ 和 $Y_n = \\langle y_1, y_2, \\cdots, y_n \\rangle$ 是两个序列，其最长公共子序列为 $Z_k = \\langle z_1, z_2, \\cdots, z_k \\rangle$\n若 $x_m=y_n$，则 $z_k~x_m~y_n$，$Z_k-1$ 是 $X_{m-1}$ 和 $Y_{n-1}$ 的最长公共子序列。 若 $x_m\\neq y_n$，$z_k\\neq x_m$ 意味着 $Z$ 是 $X_{m-1}$ 和 $Y$ 的最长公共子序列。 若 $x_m\\neq y_n$，$z_k\\neq y_n$ 意味着 $Z$ 是 $X$ 和 $Y_{n-1}$ 的最长公共子序列。 于是，我们定义 $c[i,j]$ 为 $X_i$ 和 $Y_j$ 的最大公共子序列的长度，得到递归式：\n$$ c[i,j] = \\begin{cases} 0 \u0026amp; \\text{if $i=0$ or $j=0$} \\cr c[i-1,j-1] + 1 \u0026amp; \\text{if $i,j\u0026gt;0$ and $x_i=y_i$} \\cr \\max(c[i,j-1], c[i-1,j]) \u0026amp; \\text{if $i,j\u0026gt;0$ and $x_i\\neq y_j$} \\end{cases} $$\n如果递归求解这个问题，复杂度同样为指数。\n使用动态规划，除了代价矩阵 $c$ 之外，还可以另外维护一个最优解矩阵 $b$。$b$ 中的元素指向算法在这一步选择的最优解。\n也可以不使用 $b$。由于我们已经有了各个子问题的代价值和递归式，也可以根据这个位置的代价值反推选择最优解的路径。甚至，由于这个过程并不需要 $c$ 中所有的代价值，可以进一步缩减这个操作的复杂度。\n最优二叉树 # 对于已知概率的结点的查找，构建一颗二叉搜索树，使得总的访问结点树最小。这实际上就是 Huffman 树解决的问题。Huffman 编码过程属于贪心算法，如上所述，贪心算法和动态规划可以使用的范畴基本重合，动态规划也可以用于这个问题。具体实现省略。\n贪心算法 # 贪心算法的基础是，不断选择当前看起来最优的解，并希望所有这些局部最优解综合起来成为全局最优解。这个性质并不一定总能保证。后面我们将讨论贪心算法的适用问题。\n活动选择问题 # 有一批活动需要使用同一个资源，比如一个阶梯教室。每个活动有一定的时间区间。问题的目的是找到一个活动子集，使得所有活动兼容且集合最大。\n首先从递归动态规划方向来考虑问题。有 $n$ 个活动的集合 $\\{a_1, a_2, \\cdots, aa_n\\}$，并已经按照结束时间排序好。设 $c[i,j]$ 是最优解的大小，$S_{ij}$ 是所有在 $a_i$ 活动结束后开始，在 $a_j$ 活动开始前结束的活动的集合，即所有这两个活动之间的活动的集合，则：\n$$ c[i,j] = \\begin{cases} 0 \u0026amp; \\text{if } S_{ij} = \\emptyset \\cr \\max_{a_k\\in S_{ij}} \\{c[i,k]+c[k,j]+1\\} \u0026amp; \\text{if } S_{ij} \\neq \\emptyset \\end{cases} $$\n这时我们就可以设计动态规划算法。不过，在选择最优解的过程中，直觉告诉我们，选择最先结束的活动，即 $a_1$ 可能就是最好的选择。这样，就只剩下一个子问题需要解决，而不需要遍历各种子问题。这个选择可以被证明是最优的。于是，我们可以使用递归算法来解决这个问题，也很容易将其转化为迭代形式，变成不断加入最早结束的兼容活动的算法。\n背包问题 # 0-1 背包问题这样定义：每种商品有不同的重量和价格，希望得到在限定重量下商品的最大价值。另一种变体是分数背包问题。在这里，物品不再是整数，而是可以任意分割携带。两个问题都具有最优子结构性质，但分数背包问题可以用贪心策略来完成，0-1 背包问题只能用普通的动态规划来完成。\n分数背包问题的答案比较明显，根据贪心策略，只需要尽量往背包里装满平均价值最高的商品就可以了。但在 0-1 背包问题中，由于背包可能无法装满，相当于影响了商品的单位价值。因此，这个问题不适用于贪心策略。\n摊还分析 # 在摊还分析中，我们求一个操作序列中所有操作的平均时间，来评价操作的代价。摊还分析并不涉及概率。我们将介绍三种摊还分析的方法：\n聚合分析，确定一个 $n$ 个操作的序列的总代价的上界 $T(n)$，则每个操作的摊还代价为 $T(n)/n$。 核算法，用来分别分析每个操作各自的摊还代价。核算法将序列中较早的操作的 “余额” 与数据结构中的特定对象相关联，在序列中随后的部分，用来为那些缴费少于书记代价的操作支付差额。 势能法，同样用于分析各个操作的摊还代价。与核算法的区别是，将 “势能” 作为一个整体储存起来，而不与单个对象关联分开储存。 聚合分析 # 以一个栈为例。我们为栈新定义一种操作 MUILTIPOP，弹出指定数量的元素。如果超过栈内存储的元素的量，就停止弹出而不报错。于是，对于一个长度为 $n$ 的操作序列，原来的入栈和出栈操作的代价均为 $O(1)$，而新的多重出栈的最坏情况代价为 $O(n)$。对于整个序列，最坏情况看起来应当为 $O(n^2)$，即所有操作都是参数为 $n$ 的多重出栈操作，然而，这并不是一个确界。\n下面我们利用聚合分析。虽然多重出栈的代价可能很高，但是对于一个初始为空的栈，可以执行的出栈次数最多与入栈次数相同，无论是不是多重出栈。因此，整个操作序列的最坏运行时间事实上是 $O(n)$，所以三种栈操作的摊还代价都是 $O(1)$。\n类似地，考虑一个二进制计数器。每次计数器加一时需要反转的位数是不同的。01 变成 10 需要改变 2 位，011 变成 100 需要反转 3 位。以自增操作的数量为 $n$，需要反转次数似乎正在随着计数器内值的大小而增长。然而实际上需要反转大量的位的情况是少见的，而且越是高位，需要反转的情况就越少见。对一个从 0 开始的计数器，需要进行的反转次数的总数为：\n$$ \\sum_{i=0}^{k-1} \\lfloor \\frac n {2^i} \\rfloor \u0026lt; n \\sum_{i=0}^\\infty \\frac 1 {2^i} = 2n $$\n因此，最坏情况时间为 $O(n)$，摊还代价为 $O(1)$。\n如果加入多重入栈操作，就可以压入任意多的数字。或者对计数器加入自减操作，使计数器内的值有可能在需要大量改变位数的位置颠簸，如 7-8 之间，显然，复杂度又会升高到原来的水平。\n核算法 # 对不同操作赋予不同的费用，称为他们的摊还代价。当摊还代价超过实际代价时，将差额存入数据结构中的特定对象，称为信用。对于后续摊还代价小于实际代价的情况，可以用信用来支付差额。\n回顾前面的栈问题。可以认为，PUSH 操作的实际代价为 1，POP 的代价为 1，MULTIPOP 的代价是 $\\min(k,s)$，$k$ 是参数，$s$ 是栈内元素的数量。然后，我们为这些操作分别赋予摊还代价，PUSH 为 2，POP 和 MULTIPOP 为 0。显然，PUSH 为后来的两种 POP 操作预存了代价费用。于是，长度为 $n$ 的操作序列的总代价为 $O(n)$。\n势能法 # 令 $c_i$ 为第 $i$ 个操作的实际代价，$D_i$ 为执行第 $i$ 个操作后得到的结果数据结构，则第 $i$ 个操作的摊还代价 $\\hat{c_i}$ 用势函数 $\\Phi$ 定义为：\n$$ \\hat{c_i} = c_i + \\Phi(D_i) - \\Phi(D_{i-1}) $$\n例如，对于之前的栈操作，以栈内元素的数量为势函数。于是，PUSH 操作的摊还代价为 2，另外两种为 0，结论与上一种方法相同。\n又如，对于之前的二进制计数器，将计数器内 1 的个数作为势能。同样可以得出，$n$ 个操作的最坏时间情况为 $O(n)$。\n表的扩张和收缩 # 摊还分析适合于这一类操作：一个长度可以动态变化的线性表，如 Java 中的 ArrayList。当数组空间不足以存储所有的元素时，需要重新分配一个数组，并将目前所有的元素移动到新的数组里。虽然每一次操作的代价是不同的，将势函数定义为：\n$$ \\Phi(T) = 2T.num-T.size $$\n这样定义的目的是，让每一次扩张之后，数据结构的势能为 0。如果每个元素插入、删除和移动的代价都是 1，那么插入操作的摊还代价为 3。也就是说，整个长度为 $n$ 的操作序列的代价为 $O(n)$。\n涉及到收缩的情况比较复杂。如果我们让表的容量达到 1/2 时立刻收缩，那么在特定值附近的颠簸将会使表不停地扩张和收缩，带来大量的复制和内存分配，将每个操作的代价拉回到 $O(n)$。所以，我们规定容量达到 1/4 时才发生收缩，使表的装载因子变回 1/2。这时，就需要这样定义势能：\n$$ \\Phi(T) = \\begin{cases} 2T.num-T.size \u0026amp; \\text{if } \\alpha \\geqslant \\frac 1 2 \\cr T.size/2 - T.num \u0026amp; \\text{if } \\alpha \u0026lt; \\frac 1 2 \\end{cases} $$\n这样，就相当于分别定义了收缩情况的势能和扩张情况的势能。摊还代价收缩回 3，整个操作序列的代价回到 $O(n)$。\n"},{"id":21,"href":"/notes/intro-algo/7/","title":"7. B-Tree, Fibonacci Heap, vEB Tree","section":"Introduction to Algorithms","content":" B 树 # B 树的基本思想是这样的：设想一个树，每个节点都被储存在磁盘里，我们每次取出一个结点，这是很自然的。在进入下一层时，需要访问一个新的结点，这常常意味着新的磁盘访问。相应地，在节点内部进行的比较操作，也就是寻找前进方向的操作，只需要在主存里进行。\n显然，主存中的操作的时间代价要低得多。所以，我们让每个节点持有众多的关键字和众多的结点，从而尽量降低树的高度。这意味着，相比于二叉树，使用 B 树将会带来更多的比较操作和更少的磁盘访问。因此，这种数据结构适合大块的数据访问。典型的例子如 BtrFS。\n和其他各种树一样，B 树上操作的时间复杂度同样为 $O(\\log_tn)$，只是这里的 $t$ 比二叉树中的 2 通常要大得多。\nB 树的操作 # B 树的基本操作在之前的 2-3 树已经介绍过。在插入关键字时，如果结点的大小超过了限制，就 “挤出” 一个最中间的关键字给父节点，并分裂当前结点。由于被挤出的是中间关键字，分类得到的两个新节点的长度应该相近。随后我们需要递归地对父节点进行判断，直到根节点分裂，这是 B 树高度增长的唯一方式。\n在回顾了分裂结点的操作后，我们就可以理解 B 树定义中 $t$ 的意义：每个节点内关键字的数量在 $t$ 和 $2t$ 之间，这分别是刚刚分裂过的结点和即将要分裂的结点。\n这里有一个不错的 B 树操作的可视化演示。\nB+ 树 # B+ 树是 B 树的一个变种，其所有的关键字都储存在叶子节点当中。由于 B 树中所有的结点高度相同，其查找的时间复杂度非常稳定。B+ 树可以接受重复关键字，将实际数据存储在叶子节点中，内部结点只用来指示存储位置即可。\n斐波那契堆 # 可合并堆 # 一个可合并堆应当支持以下的操作：\n创建空堆 插入关键字 取得最小值 删除最小值 合并两个堆 除此之外，斐波那契堆还可以支持减小已有元素的关键字和删除关键字的操作。\n之前出现过的二叉堆在前四个操作上效果都不错，时间代价为 $O(\\log_2n)$。然而，在合并堆的操作上，二叉堆的速度非常慢。可以这样实现：将两个堆直接合并起来，再执行建堆操作。这样，复杂度会达到 $\\Theta(n)$。斐波那契堆通常相比二叉堆具有更好的摊还代价。\n堆 新建 插入 取堆顶 删除堆顶 合并堆 减小 删除 二叉 $\\Theta(1)$ $\\Theta(\\log_2n)$ $\\Theta(1)$ $\\Theta(\\log_2n)$ $\\Theta(n)$ $\\Theta(\\log_2n)$ $\\Theta(\\log_2n)$ 斐波那契 $\\Theta(1)$ $\\Theta(1)$ $\\Theta(1)$ $\\Theta(n)$ $\\Theta(1)$ $\\Theta(1)$ $\\Theta(n)$ 不过，实际情况下，斐波那契堆的常数代价比较高，编程实现也比较困难。因此，并不十分常用。\n斐波那契堆 # 一个斐波那契堆是一个森林，其中的每一棵树都符合最小堆性质，即，任何子节点的 key 大于其父节点。所有的根，即每一个结点的所有子节点，都被链接成一个环形双向链表。这样做的目的是，可以非常快速地向链表中插入一个新的节点，或者合并两个链表（在堆的合并操作中）。同一个链表中所有元素的顺序是任意的。\n每个结点有两个额外的属性：子节点的数目 degree，和标记其自改变了父节点以来，是否失去过孩子的布尔值 mark。也就是说，新插入的结点和改变了父节点的结点的 mark 都是 False。堆的入口是一个指向 key 最小的根节点的指针 H.min。除此之外，堆还有一个属性 H.n 用来指示整个堆中的结点数。以 $t(H)$ 为树的数目，$m(H)$ 为已标记的结点数目，那么势函数：\n$$ \\Phi(H) = t(H) + 2m(H) $$\n另外，斐波那契堆中结点的度数，即子节点数将会得到限制。当支持堆合并操作时，最大度数 $D(n) \\leqslant \\lfloor \\log_2n \\rfloor$，当支持后两个操作时，会使得 $D(n) = O(\\log_2n)$。\n斐波那契堆的操作 # 斐波那契堆的核心思想是尽量推后操作。所有的数据结构整理操作都被放在取出最小值的过程中。\n合并两个堆时，直接根链表连接起来中，并依需更新 H.min 和 H.n 即可。\n插入一个新的元素时，实际上和把当前堆与一个大小为 1 的堆合并是一样的。\n取得最小节点时，直接取 H.min 即可。\n抽取最小节点时，首先将 H.min 指向的结点的所有子节点修改到根链表中。判断堆是否变成了空堆，随后，遍历所有根结点，把相同度数的根节点合并起来。操作结束后的根链表中所有结点的度都不相同。这个过程可以使用一个数组，数组的下标代表结点的度数。当出现冲突时，说明需要进行合并操作。合并时，直接把根节点较大的那个树变成另一棵树的子树即可。\n斐波那契堆的其他操作 # 在降低一个节点的 key 值时，如果其父节点的 mark 为 False，可以直接将其切断变成一个根节点。这样，所有的性质都不被破坏。此时，依据上面所说，将其父节点的 mark 标记为 True。\n当下一次要再次降低这个父节点的另一个儿子时，其父节点的 mark 就是 True，需要进行一次级联切断，将其父节点也切断变成根节点，并一直向上递归判断 mark 的值。\n在删除一个结点时，相当于把这个节点的 key 降为最小，进行降低 key 值操作后，再执行取出最小值的操作。\nvEB 树 # 叠加二叉树结构 # 如图，使用一个二叉树作为索引，指示下面数组的对应位置是否存在元素。\n类似地，可以将其扩展，增加结点的大小：\nvEB 树 # vEB 树可以认为是从带树索引的数组演化而来。其空间占用与 key 的取值范围 $u$ 有关。使用多层的树，树的每个节点都是另外一个 vEB 树，其取值范围是父节点所在树的二次方。自顶向下直到取值范围达到 2，成为叶子节点。\n对于每一个结点，其数据结构包括其取值范围 $u$，极值 min 和 max，一个指向 $vEB(\\sqrt u)$ 的指针 summary，以及一个长为 $\\sqrt u$ 的数组，其中每个位置是一个指向 $vEB(\\sqrt u)$ 的指针，即下一层 vEB 树。\nsummary 的作用是上一节中每一层的 0-1 数组的作用。因为其大小同样为 $\\sqrt u$，所以同样使用 vEB 树来储存。在访问时，需要先在 summary 中寻找对应的值，再去 cluster 中寻找数据。\n实际情况中，$\\sqrt u$ 不一定是整数。因此，使用 $\\sqrt[\\uparrow]u \u0026gt; \\sqrt u \u0026gt; \\sqrt[\\downarrow]u$，使得$\\sqrt[\\uparrow]u \\cdot \\sqrt[\\downarrow]u = u$。那么，summary 指向一棵 $vEB(\\sqrt[\\uparrow]u)$，cluster 数组中的每一个指针指向 $vEB(\\sqrt[\\downarrow]u)$。\nvEB 树的查找 # vEB_FIND(V, x) if x == min or x == max return TRUE else if u == 2 return FALSE else return vEB_FIND(V.cluster[high(x)], x) 查找最大值和最小值相当于把上面的流程简化。\n用于处理不相交集合的数据结构 # 不相交集合的操作 # MAKE_SET：新建一个仅包含参数的集合 UNION：取两个集合的并集 FIND_SET：寻找包含参数的集合 不相交集合的一个应用是处理无向图的连通分量。在寻找连通分量的过程中将会需要大量的不相交集合操作。\n不相交集合的链表表示 # 使用一个链表来表示一个不相交集合，集合对象保留 head 和 tail 属性，每个元素对象保留指向集合的指针。这样，在寻找元素所在的集合时，就可以直接用指针返回集合。合并两个集合的链表非常容易，但合并时还需要修改其中一个集合的所有指向集合的指针。于是，最终操作的摊还代价达到了 $\\Theta(n)$。\n不相交集合森林 # 使用一棵树来表示一个集合。这样，FIND_SET 操作就变成了一直追随父节点直到树的根。UNION 操作只需要把一棵树变成另一棵的子树，即将其根节点指向另一棵树的根节点即可。\n显然，无论链表还是森林，使较小或较矮的集合合并到较大的集合里都是提高效率的方法。此外，对于森林表示，还可以使用路径压缩的方式，这种方式的 FIND_SET 这样实现：\nFIND_SET(x) if x != x.p x.p = FIND_SET(x.p) return x.p 这样，整个函数经过的路径上所有的结点都变成了直接指向根节点的结点。于是在未来的查询中，效率就能得到提升。\n"},{"id":22,"href":"/notes/core-java-impatient/7/","title":"7. IO, Regexp, Serialization","section":"Core Java for Impatients","content":" IO 流 # 创建和使用字节流 # // Create Stream Path path = new Path(\u0026#34;...\u0026#34;) InputStream in = Files.newInputStream(path); OutputStream in = Files.newOutputStream(path); byte[] bytes = ...; InputStream in = new ByteArrayInputStream(bytes); ByteArrayOutputStream out = new ByteArrayOutputStream(); byte[] bytes = out.toByteArray(); // read bytes int b = in.read(); //single byte, 0~255, or -1 for EOF. but byte is -128~127. byte[] bytes = ...; actualBytesRead = in.read(bytes); //read till EOF or byte[] full, return count actualBytesRead = in.read(bytes, start, length); //to read all bytes from a file byte[] bytes = Files.readAllBytes(path); //to read all byte from a stream public static byte[] readAllBytes(InputStream in) throws IOException { ByteArrayOutputStream out = new ByteArrayOutputStream(); // will show below copy(in, out); out.close(); return out.toByteArray(); } //write bytes out.write(int); out.write(bytes); out.write(bytes, start, length); // must close to flush. better: try (OutputStream out = ...) { out.write(bytes); } // save an input stream to file Files.copy(in, path, StandardCopyOption.REPLACE_EXISTING); // the copy method used above public static void copy(InputStream in, OutputStream out) throws IOException { final int BLOCKSIZE = 1024; byte[] bytes = new byte[BLOCKSIZE]; int len; while ((len = in.read(bytes)) != -1) { out.write(bytes, 0, len); } } 处理字符流 # 编码，大小端序和 BOM # 对于 16 进制数 0x00FF ：\n地址 0x00 0x01 大端序 0x00 0xFF 小端序 0xFF 0x00 这就是 \u0026ldquo;大端序高地址存放低位，小端序高地址存放高位\u0026rdquo;。也就是说，小端序是逆习惯的。但这对 CPU 设计有利。对于文本编码，在文件头使用一个值为 0xFEFF 的值来标记。如果文件头是 0xFEFF，说明是大端序。如果是 0xFFFE，说明为小端序。这就是 BOM（byte order mark）。\n显然，对于 UTF-8，不存在大小端序问题。对于更多字节编码的文本，实际上，Unicode 推荐即使是 UTF-8 也使用 BOM，Windows 记事本就是这样做的。但很多其他编辑器对其支持不好。Java 选择忽略了这个问题，所以如果要手动处理文本文件，需要手动进行判断。\nCharset shiftJIS = Charset.forName(\u0026#34;Shift-JIS\u0026#34;); String str = new String(bytes, shiftJIS); String str = new String(bytes, StandarCharsets.UTF_8); 对于常见的字符集，建议使用 StandardCharset 枚举变量。这样可以避免拼写错误，例如 Charset.forName(UTF 8) 会抛出异常。\nReader # Reader in = new InputStreamReader(inStream, charset); int ch = in.read(); // UTF-16 Code Unit String content = new String(Files.readAllBytes(path), charset); //throws UncheckedIOException List\u0026lt;String\u0026gt; lines = Files.readAllLines(path, charset); try (Stream\u0026lt;String\u0026gt; lines = Files.lines(path, charset)) { ... } Scanner in = new Scanner(path, \u0026#34;UTF-8\u0026#34;); // a charset string in.useDelimiter(\u0026#34;\\\\PL+\u0026#34;); // regex // read with buffer (by block) try (BufferedReader reader = new BufferedReader( new InpitStreamReader(url.openStream()))) { Stream\u0026lt;String\u0026gt; lines = reader.lines(); } // from file Files.newBufferedReader(path, charset); Writer # Writer writer = new OutputStreamWriter(outStream, charset); writer.write(str); // to file Writer writer = Files.newBufferedWriter(path, charset); // PrintWriter with println() and such PrintWriter out = new printWriter(writer); PrintWriter out = new PrintWriter(outStream, \u0026#34;UTF-8\u0026#34;); // a charset string System.out 实际上是一个 PrintStream 对象，它和 PrintWriter 类具有类似的方法。\nString content = ...; Files.write(path, content.getBytes(charset)); // here, lines is an Iterable\u0026lt;? entends CharSequence\u0026gt; Files.write(path, lines, charset); // similarly, append to a file: Files.write(path, content.getBytes(charset), StandardOpenOption.APPEND); // for unsupported character in the charset, will be transfered as ? or \\ufffd StringWriter writer = new Strignwriter(); throwable.printStackTrace(new PrinWriter(writer)); Strin stackTrace = writer.toString(); 读写二进制数据 # DataInput 接口提供了一系列读取基本类型的方法，全部都是大端序的。DataOutput 情况类似。 readUTF 和 writeUTF 是 JVM 的修改过的格式，不能直接用来输出 UTF-8。 DataInput in = new DataInputStream(Files.newInputStream(path)); DataOutput out = new DataOutputStream(Files.newOutputStream(path)); RandomAccessFile 类支持对一个文件进行随机读写。第二个参数代表读写模式。\nRandomAccessFile file = new RandomAccessFile(path.toString(), \u0026#34;rw\u0026#34;); // \u0026#34;r\u0026#34; for read only int value = file.readInt(); file.seek(file.getFilePointer() - 4); // move the ponter file.seek(file.length()); file.writeInt(value + 1); 内存映射文件 # 内存映射方式适合用来随机存取大文件，只将文件的一部分取到内存中。\nFileChannel channel = FileChannel.open(path, StandardOpenOption.READ, StandardOpenOption.WRITE); ByteBuffer buffer = channel.map(FileChannel.MapMode.READ_WRITE, 0, channel.size()); int offset = ...; int value = buffer.getInt(offset); buffer.put(offset, value + 1); buffer.order(ByteOrder.LITTLE_ENDIAN); 通道关闭时，更改会被写回文件。\n文件锁 # FileChannel channel = FileChannel.open(path); FileLock lock = channel.lock(); // 阻塞，直到被放开，然后被当前线程锁住 FileLock lock = channel.tryLock(); // 如果被锁，返回 null try (FileLock lock = channel.lock()) { ... } 在锁或通道关闭后，其他线程就可以访问同一文件。\n路径和文件 # 使用 Path 类 # Path homeFolder = Paths.get(\u0026#34;/home/xxx\u0026#34;); Path picFolder = homefolder.resolve(\u0026#34;pic\u0026#34;); // relative path Path picFolder = homefolder.resolve(Path.get(\u0026#34;pic\u0026#34;)); // relative path Path musicFolder = picFolder.resolveSibing(\u0026#34;music\u0026#34;); // to /home/xxx/music Paths.get(\u0026#34;/home/cay\u0026#34;).relativize(Paths.get(\u0026#34;/home/fred/app\u0026#34;)); // get ../fred/app path.normalize(); path.toAbsolutePath(); // with application working folder path.getParent(); path.getFileName(); path.getRoot(); path.getName(0); path.subpath(1, p.getNameCount()); // @param: (start, length). here get all files but the first for (Path p : path) { // Iterable\u0026lt;Path\u0026gt; ... } 文件和目录 # Files.createDirectory(path); Files.createDirectories(path); Files.createFile(path); // throws exception if exist Files.exists(path); // create an check exist are atom oparation in java Files.isDirectory(path); Files.isRegularFile(path); // rather than symlink and such // system temp Files.createTempFile(dir, prefix, suffix); Files.createTempFile(prefix, suffix); Files.createTempDirectory(dir, prefix); Files.createTempDirectory(prefix); //for example, createTempFile(null, \u0026#34;.txt\u0026#34;) may get /tmp/1231.txt Files.copy(fromPath, toPath); Files.move(fromPath, toPath); //if want to replace Files.copy(fromPath, toPath, StandardCopyOption.REPLACE_EXISTING, StandardCopyOption.COPY_ATTRIBUTES); Files.mvoe(fromPath, toPath, StandardCopyOption.ATOMIC_MOVE); Files.delete(path); // throws Exception if not exist boolean deleted = Files.deleteIfExists(path); try (Stream\u0026lt;Path\u0026gt; entries = Files.list(path)) { // deal with subs ... } try (Stream\u0026lt;Path\u0026gt; entries = Files.walk(path)) { // recurrsively, DFS ... } Files.walk(path, depth, FileVisitOption.FOLLOW_LINKS); // follow symlinks Files.walk(path, depth, (path, basicFileAttribute) -\u0026gt; ...); FileSystem zipfs = FileSystems.newFileSystem(Paths.get(zipname), null); zipfs.get(\u0026#34;path_in_zip\u0026#34;); URI uri = new URI(\u0026#34;jar\u0026#34;, zipPath.toUri().toString(), null); // file://myfile.zip try (FileSystem zipfs = FileSystems.newFileSystem( uri, Collections.singletonMap(\u0026#34;create\u0026#34;. \u0026#34;true\u0026#34;))) { Files.copy(sourcePath, zipfs.getPath(\u0026#34;/\u0026#34;).resolve(targetPath)) } // save to an zip URL 连接 # 如果我们有一个资源的 URL，可以直接从 URL 打开：\nInputStream in = new URL(\u0026#34;http://xxx\u0026#34;).openStream(); URLConnection conn = url.openConnnection(); conn.setRequestProperty(\u0026#34;Accept-Charset\u0026#34;. \u0026#34;UTF-8, GBK\u0026#34;); conn.setDoOutput(true); try (OutputStream out = conn.getOutputStream()) { // write to out } conn.connect(); Map\u0026lt;String List\u0026lt;String\u0026gt;\u0026gt; headers = conn.getHeaderFields(); try (InputStream in = conn.getInputStream()) { // read from in } 正则表达式 # 匹配和提取 # // match one String regex = \u0026#34;\u0026#34;; CharSequence input = ...; if (Pattern.matches(regex, input)) { ... } // precompile and match much Pattern pattern = Pattern.compile(regex); Matcher matcher = pattern.matcher(input); if (matcher.matches()) { ... } strs.filter(pattern.asPredicate()); // all that matches // find all match patterns in a string, one at once while (matcher.find()) { String match = matcher.group(); } 也可以按照下标提取匹配子串。例如，对于字符串 :\nBlackwell Toaster USD29.95\n使用正则表达式，其中 Alnum 表示 字母和数字：\n(\\p{Alnum}+(\\s+\\p{Alnum}+)*)\\s+([A-Z]{3})([0-9.]*) 于是可以使用：\nif (matcher.matches()) { item = matcher.group(1); currency = matcher.group(3); price = matcher.group(4); } 这里的下标从 1 开始。0 下标表示整个字符串。group 按照左小括号分组。因此，四个 group 分别为：\nplain text 0: Blackwell Toaster USD29.95 1: Blackwell Toaster 2: Toaster 3: USD 4: 29.95\n在这里我们不需要第二个匹配。因此，可以把这个匹配忽略：\n(\\p{Alnum}+(?:\\s+\\p{Alnum}+)*)\\s+([A-Z]{3})([0-9.]*) 或者，为每个匹配命名：\n(?\u0026lt;item\u0026gt;\\p{Alnum}+(?:\\s+\\p{Alnum}+)*)\\s+(?\u0026lt;currency\u0026gt;[A-Z]{3})(?\u0026lt;price\u0026gt;[0-9.]*) 然后，在 Java 代码中：\nmatcher.matches(); item = matcher.group(\u0026#34;item\u0026#34;); 处理 # Pattern commas = Pattern.compile(\u0026#34;\\\\s*,\\\\s*\u0026#34;); String input = \u0026#34;1, 2, 3\u0026#34;; // split String[] tokens = commas.split(input); // get [\u0026#34;1\u0026#34;, \u0026#34;2\u0026#34;, \u0026#34;3\u0026#34;] Stream\u0026lt;String\u0026gt; tokens = commas.splitAsStream(input); // stream String[] tokens = input.split(\u0026#34;\\\\s*,\\\\s*\u0026#34;); // no compile // replace matcher.replaceAll(\u0026#34;,\u0026#34;); input.replaceAll(\u0026#34;\\\\s*,\\\\s*\u0026#34;, \u0026#34;,\u0026#34;); \u0026#34;3:45\u0026#34;.replaceAll(\u0026#34;(\\\\d{1,2}):(?\u0026lt;minutes\u0026gt;)\\\\d{2}\u0026#34;, \u0026#34;$1 hours and ${minutes} minutes\u0026#34;); // $num or ${match name} 正则表达式标记 # Pattern pattern = Pattern.compile(regex, Pattern.CASE_INSENSITIVE | Pattern.UNICODE_CHARACTER_CLASS); // integers, bit operation // or, in regex string: String regex = \u0026#34;(?iU:expression)\u0026#34;; 序列化 # Serializable # Serializable 是一个标记接口，没有方法。如果对象中所有的成员都是基本类型、枚举或其他 Serializable 对象，那么对这个对象进行序列化就是安全的。对于数组和集合，如果他们存储的对象是可序列化的，那么它们就也是可序列化的。\nObjectOutputStream out = new ObjectOutputStream(Files.newOutputStream(path)); out.writeObject(obj); ObjectInputStream in = new ObjectInputStream(Files.newInputStream(path)); Employee e = (Employee) in.readObject(); 在序列化的过程中，对于基本类型，直接以二进制方式写入。对于对象，则会递归地使用 writeObject。Java 会为每一个对象保持一个序列号。这样，如果两个对象引用同一个对象，那么重复的对象就可以只存储一次。\n打上 transient（瞬态）标记的变量不会被序列化。通常是缓存等类型的数据。\n反序列化得到的对象并不会被构造，而是直接从对象流读取进来。\n继承与序列化 # 考虑这样一个情况：父类没实现 Serializable 接口，而子类实现了。那么，父类的实例变量就不会被序列化。\n于是，当我们反序列化一个子类对象时，Java 选择的办法只能是使用默认的无参构造函数构造一个父类对象（因为子类的构造函数也没有被调用）。如果父类没有无参构造方法，就会抛出异常。对于这种情况，我们还有一些其他的解决办法。\n重写 writeObject 和 readObject # 这两个方法非常特殊：它们并不是存在于 Serializable 接口中。甚至，它们是 private 的。事实上，Java 通过反射方式来寻找这两个方法。\n例如，JavaFX 中的 Point2D 类是不可序列化的。假设我们实现了一个 LabeledPoint 类，其中包含一个 String 和一个 Point2D，并要将其序列化：\nclass LabeledPoint { private String label; private transient Point2D point; // transient, to prevent NotSerializableException private void writeObject(ObjectOutputStream out) throws IOException { out.defaultWriteObject(); // normally serialize this obj and label out.writeDouble(point.getX()); out.writeDouble(point.getY()); } private void readObject(ObjectInputStream in) throws IOException, ClassNotFoundException { in.defaultReadObject(); double x = in.readDouble(); double y = in.readDouble(); point = new Point2D(x, y); } } 这里的两个方法只应该访问自己的变量，而不能访问父类的变量。显然，那些应该被放在父类的方法里。\n另外，还有 Externalizable 接口提供两个方法可以用来自定义序列化格式。和 Serializable 不同，这个接口提供的流程是先使用无参构造函数构造，再调用接口方法来读取变量数据。\nreadResolve 和 writeReplace # 前面说到，反序列化的对象没有被构造。这会带来一些问题。例如，传统的单例模式，只允许构造方法调用一次。以前用来模拟枚举类型的方法以及一些数据库连接池也类似。不过现在，枚举类型的序列化已经自动化，你也可以使用枚举来构建单例模式：\npublic enum PersonDB { INSTANCE; public Person findByID(int id) { ... } } 虽然已经不常见，我们仍然需要处理这一类的问题。\npublic class Person implements Serializable { private int id; priavte Object writeReplace() { return new PersonProxy(id); } } public class PersonProxy implements Serializable { private int id; public PersonProxy(int id) { this.id = id; } public Object readResolve() { return PersonDB.INSTANCE.findById(id); } } 这样，当 Person 被序列化时，实际上被序列化的将会是 writeReplace 的返回值，即一个 PersonProxy 对象。然后，当我们从对象流中读取一个 PersonProxy 对象时，其 readResolve 方法将会被调用，最终恢复出来的将是其返回值，即一个 Person 对象。这样做的最终结果是，我们只序列化了 id，在读取时，通过 id 从数据库中将其他属性读取出来，成为我们最终需要的 Person 对象。\n版本化 # 在对象中定义一个常量：\nprivate static final long serialVersionUID = 1L; 每当类实现的新版本产生了不兼容时，就改变这个值。这样，在试图将旧版的序列转换为新版的对象时，就会抛出 InvalidClassException 异常。\n如果 UID 没有改变，却增加了新的实例变量，那么对象引用会被声明为 null，基本类型声明为 0。\n如果没有定义 UID 常量，系统会在序列化时根据实例变量、方法和父类的信息自动生成一个 hash 值作为版本 UID。可以用命令行的 serialver 工具来查看这个值。显然，如果之前忘记定义这个值，同时希望新版本兼容旧版序列化，那么直接把 serialver com.test.Clazz 得到的结果写入 serialVersionUID 常量即可。\n"},{"id":23,"href":"/notes/intro-algo/8/","title":"8. Graphs","section":"Introduction to Algorithms","content":" 图的表示与基本算法 # $G(V,E)$ 其中 $G$ 为 Graph 图，$V$ 为 Verticle 结点，$E$ 为 Edge 边。 邻接链表 邻接矩阵 BFS 广度优先遍历（队列） # 在 BFS 中，把未发现的结点和边标记为白色，发现但未完成的为灰色，已完成的为黑色。在 BFS 中，不需要对灰色和黑色进行区分。这个颜色用法也适用于 DFS 的结点。边只分为黑白两种。\nBFS 可以发现两个结点间的最短路径。\nDFS 深度优先遍历（栈） # 让 DFS 中一个节点的发现时间和完成时间分别为 $u.d$ 和 $u.f$。那么，对于任意两个节点，它们的 $[d,f]$ 时间区间要么完全分离，要么互相包含。被包含的在 DFS 树中是另一个节点的后代。\n结点 $v$ 是 $u$ 的后代，当且仅当发现 $u$ 时，存在一条白色（完全未发现的）路径连接二者。\n在 DFS 中将边分类。DFS 树中的称为树边，从一个节点连接到其祖先的为后向边，连接到其子孙的为前向边，其余的为横向边。于是，当且仅当图中无后向边，有向图无环。\n当第一次查看边 $(u,v)$ 时，若 $v$ 为白色，说明这是一条树边。若为灰色，说明是一条后向边（栈中的都是当前结点的祖先）。若为黑色，是前向或横向边（结点已经被访问过）\n拓扑排序 # 拓扑排序可以看做把图排列成一条直线，让所有的边都指向同一个方向。显然，有环图不能被线性排序。拓扑排序可以这样被简单实现：在 DFS 中，每完成一个结点，就将其排列到直线的最前面第一个位置。\n强连通分量 # 强连通分量是指任何两个节点都能从正逆两个方向连通的一个分量。可以由 DFS 获得。在下面的算法中，我们以这样一个图为例：\nKosaraju 算法 # 首先得到 DFS 树，然后将所有边的方向反转，在考虑完成时间 $u.f$ 顺序的前提下，再进行一次 DFS。最后完成的结点将被作为 DFS 的最先结点。这一次 DFS 得到的每一个树，就是一个强连通分量。其复杂度为 $O(V+E)$。不过，由于其需要两次 DFS，常数项相对下面的方法较大。\n以上面的图为例。在从 0 开始的 DFS 中，得到的完成时间序列从晚到早将会是：0,1,2,3,4,5。在如下的反向图中，以同样的顺序进行 DFS，将会首先得到 0,1,2 这棵树。然后，开启第二次 DFS，从 序列中下一个结点：3 开始，得到另一个强连通分量。\nTarjan 算法 # 这个算法只需要一次 DFS。对于每个节点，我们需要记录其发现时间 $u.d​$ 和从其出发能够到达所有结点中 $d​$ 的最小值 $lowlink​$，这个值将会在结点完成时确定下来。显然，$lowlink \\leqslant d​$。当从结点出发无法到达其他的结点时，二者相等，这个结点在这个算法中称为强连通分量的根。\n每当一个节点完成时，检查其是否是一个强连通分量的根。如果是，那么在其之前完成的尚未被划分入一个强连通分量的结点构成一个强连通分量。\n在上图的例子中，第一轮从 0 开始的 DFS 中 $u.d$ 的顺序将会是：0,1,2,3,4,5。而相应的 $lowlink$ 的序列将会是：0,0,0,3,3,3。由此得到两个根 0 和 3，两个强连通分量。\nGabow 算法 # Gabow 算法与 Tarjan 算法的思想是相同的。区别是，Gabow 使用一个栈代替了 $lowlink$ 属性。\n最小生成树算法 # 最小生成树是指，对于一个无向有权连通图，寻找一棵生成树，使得所有边的权重相加最小。注意，图的最小生成树的边数是一定的，为 $V-1$。下面的两种常用算法都使用了贪心策略。\nKruskal 算法 # Kruskal 算法的核心是，不断寻找权最小的边，加入到生成树中。把每一个节点看成一棵树，并对所有的边进行排序。从权最小的边开始寻找，如果一条边能够将两棵树连接起来，就将其加入到树中。最终，所有的树都将连接起来变成一棵。可以看到，这里实际上利用了之前提到的不相交集合相关的算法。如果使用之前的路径压缩的森林实现，算法的复杂度为 $O(E\\log_2V)$。\nPrim 算法 # Prim 算法同样采取贪心策略，可以认为这个算法是在不断寻找权值最小的结点。以一个节点为起点构建一棵树。在其所有经过一条边能够抵达的结点中，选取需要的权最小的一个节点加入到树中。可以看到，这里适合使用之前的最小优先队列算法。事实上，算法的代价取决于最小优先队列。如果使用二叉最小优先队列，渐进代价将被控制在 $O(E\\log_2V)$。\n单源最短路径算法 # BFS 就是一个可以寻找最短路径的算法，但它不能被应用于有权图。在这里，单源最短路径问题，指从一个节点出发求取其到所有节点距离的算法。其他类型的最短路径可以认为是这个算法的变体。下面的 Dijikstra 算法将是一个 贪心算法，而 Floyd 算法将是一个动态规划算法。\n定义一个边的松弛（Relax）操作。也就是，判断这条边是否能够对最短路径带来一定优化。在以下的算法中，我们常常为每一个节点维护一个值 $d$ 及其前驱 $\\pi$，代表从这个节点到源的目前已知的最短路径的值，初始化为正无穷。那么，对于一条边，将一个节点的 $d$ 与权相加，如果小于另一个节点的值，就进行替换。\n对于任何一个结点，其最短路径的前驱子图上所有的结点都取到了最短路径。\nBellman-Ford 算法 # Bellman-Ford 算法可以接受负权重的图。不过，如果图中存在一个从源节点可以抵达的负权重的环，最短路径就无法被定义。显然，再次经过这个环就能得到一个权重更小的路径。因此，Bellman-Ford 算法在遇到这种情况时将会返回 False。算法相当简单：两层嵌套的循环去松弛每一条边。\nfor i=1 to V-1 for each egde relax(edge) for each edge if can relax return FALSE return TRUE 有向无环图中的单源最短路径 # 在有向无环图中，由于不存在环路，无论权重是正或负都可以找到最短路径。由于有向无环图可以进行拓扑排序，得到的图中所有的边都沿同一方向，于是直接按照这个方向去松弛每一条边即可。\nDijikstra 算法 # Dijikstra 算法要求所有边权重非负。算法在运行过程中维持一个集合 $S$，这个集合内所有的结点都已经找到了最短路径。算法重复地从集合 $V-S$ 中选择最短路径估计最小的结点合并到 $S$ 内，并松弛所有从这个节点发出的边。\n算法的总运行时间依赖于最小优先队列的实现。如果使用二叉堆来构建，则可以控制在 $O((V+E)\\log_2V)$。如果使用线性数组来构建优先队列，则为 $O(VE)$。\n差分约束和最短路径 # 线性规划问题是，在满足一组线性不等式的条件下优化一个线性函数。这里我们先讨论一个特例，这个特例可以被规约到单源最短路径问题，以期用 Bellman-Ford 算法来解决。\n线性规划问题 # 在通用的线性规划问题是：给定一个 $m\\times n$ 的矩阵 $A$，一个 $m$ 维向量 $b$，一个 $n$ 维向量 $c$。那么，$Ax\\leqslant b$ 就是一组不等式。我们的目的是，在满足这一组不等式，即约束条件的前提下，优化目标函数 $\\sum_{i=1}^nc_ix_i$ 的值，使其取得最大。很多时候，我们只是希望寻找是否存在这样一个 $x$ 成为可行解。\n差分约束系统 # 差分约束问题是线性规划的一个特例。矩阵 $A$ 的内容只包括 0、1 和 -1，于是问题变成了一组形如 $x_j-x_i\\leqslant b_k$ 的不等式。例如，寻找以下的向量 $x=(x_i)$：\n$$ \\begin{bmatrix} 1 \u0026amp; -1 \u0026amp; 0 \\cr 1 \u0026amp; 0 \u0026amp; -1 \\end{bmatrix} \\begin{bmatrix} x_1 \\cr x_2 \\cr x_3 \\end{bmatrix} \\leqslant \\begin{bmatrix} 0 \\cr -1 \\end{bmatrix} $$\n相当于寻找三个变量，使得：\n$$ x_1 - x_2 \\leqslant 0, x_1 - x_3 \\leqslant - 1 $$\n约束图 # 可以从图论的观点来理解差分约束系统。将矩阵 $A^\\mathrm{T}$ 看做一个由 $n$ 个结点和 $m$ 条边组成的图的邻接矩阵。那么，图中的每一个节点对应目标向量 $x$ 中的一个维度 $x_i$。每条有向边对应一个不等式。这时，寻找一个最短路径就是寻找一个解。目标向量 $x$ 的每一个维度就是从起点到每一个结点的最短路径长度。即：\n$$ x=(\\delta(v_0, v_1), \\delta(v_0, v_2), \\delta(v_0, v_3), \\cdots, \\delta(v_0, v_n)) $$\n显然。作为一个允许存在负权重的图，如果存在负权重环路，则问题无解。使用 Bellman-Ford 算法可以把代价控制在 $O(n^2+nm)$。如果进一步优化算法，则可以达到 $O(nm)$。\n扩展 # Karp 最小平均权重路径算法\n所有节点间的最短路径问题 # 显然，可以通过 $|V|$ 次单源最短路径算法来解决这个问题。如果使用 Dijkstra 算法，在线性数组作为优先队列的情况下，复杂度为 $O(V^3)$。使用二叉堆时，将控制在 $O(VE\\log_2V)$。当图中有负权时，使用 Bellman-Ford 算法将会使复杂度来到 $O(V^2E)$。在图非常稠密的情况下，这个值将会逼近 $O(V^4)$。\n在接下来的算法中，我们将会更多地使用邻接矩阵，这种形式更适合这类问题的解决。算法的输出将同样是一个 $n$ 阶矩阵，内容为两个结点之间的最短路径长度。为了得到最短路径本身，还需要一个同样为 $n$ 阶的前驱结点矩阵。这样，通过不断地查询前驱结点矩阵，就可以得到任何两个节点之间的完整路径。（因为最短路径去掉最后一个节点，即前驱结点后，仍然是一个最短路径。）\n基于矩阵乘法动态规划的算法 # 递归解决最短路径问题 # 我们知道，一条最短路径的一部分必然也是一条最短路径。\n规定 $l^{(m)}_{ij}$ 为从 $i$ 到 $j$ 所有包含边数不大于 $m$ 的路径中最小的权重。\n显然，当两个节点在 $m$ 条边内不可达时，$l^{(m)}_{ij}$ 为 $\\infty$。当两个节点重合时，该值为 0。设 $k$ 是 $j$ 可能的前驱结点。那么：\n$$ {l^{(m)}_{ij}} = \\min_{1 \\leqslant k \\leqslant n} \\{l^{(m-1)}_{ik} + w_{kj}\\} $$\n其中 $w_{kj}$ 是结点 $k$ 和 $j$ 之间边的权重。又因为，简单路径的长度不可能大于结点数，所以实际的最短路径权重 $\\delta(i,j)=l^{(n-1)}_{ij}$。\n自底向上计算最短路径权重 # 现在我们可以将所有的 $l^{(m)}_{ij}$ 组成一个矩阵 $L^{(m)}$。那么，我们就可以从 $L^{(1)}$ 开始，利用上面的公式，求取下一个矩阵，直到 $L^{(n-1)}$，就是我们需要的最短路径权重矩阵。\n这个算法和矩阵乘法的形式十分类似，只是把求和工作变成求最小值工作。这样我们就可以定义这样一种新的 “矩阵乘法”，那么 $L^{(m)}=L^{(m-1)}\\cdot W=W^m$，其中 $W$ 为邻接矩阵。显然，每次 “矩阵乘法” 的复杂度为 $\\Theta(n^3)$，总的算法复杂度为 $\\Theta(n^4)$。\n使用重复平方技术改进算法 # 然而，我们并不关心其他的矩阵，只需要 $L^{(n-1)}$。因此，我们可以将上面逐个求取 $L^{(m)}$ 的过程进行修改。首先，我们知道，对于任意 $m\\geqslant n-1$，$L^{(m)}=L^{(n-1)}$。然后，我们使用 $L^{(2m)}=(L^{(m)})^2$ 来求取更靠后的矩阵。每次矩阵乘法的复杂度仍然为 $\\Theta(n^3)$，总的算法复杂度被控制在了 $\\Theta(n^3\\log_2n)$。\nFloyd-Warshall 算法 # 递归求解的方法 # 也称 Floyd 算法。算法的核心是，维护一个结点的集合，及其所有节点之间，仅在集合内部的最短路径。不断将新的结点加入到这个集合里，并检查这个边是否对节点之间的最短路径发生了改变。令 $d^{(k)}_{ij}$ 为所有位于节点集合 ${1,2,\\cdots,k}$ 内部的从 $i$ 到 $j$ 的最短路径的权重。那么就可以使用递归式来求解其值：\n$$ d^{(k)}_{ij} = \\begin{cases} w_{ij} \u0026amp; \\text{if } k=0 \\cr \\min(d^{(k-1)}_{ij}, d^{(k-1)}_{ik} + d^{(k-1)}_{kj}) \u0026amp; \\text{if } k \\geqslant 1 \\end{cases} $$\n于是，矩阵 $D^{(n)}=(d^{(n)}_{ij})$ 给出的就是我们的最后答案。\n自底向上实现算法 # 当 $k=0$ 时，最短路径权重矩阵 $D^{(0)}$ 既是所有直接相连结点之间的关系。然后，逐渐加入结点到集合内，即增大 $k$ 的值。每加入一个节点，就调用 $d^{(k)}_{ij} = \\min(d^{(k-1)}_{ij}, d^{(k-1)}_{ik} + d^{(k-1)}_{kj})$，更新所有节点之间的路径关系。即三层 for 循环的嵌套。因此，算法的复杂度为 $\\Theta(n^3)$。\n有向图的传递闭包 # 给定一个有向图 $G=(V,E)$，我们希望知道任意两个结点是否可达。定义图 $G$ 的传递闭包为 $G^*=(V,E^*)$，其中若结点 $i$、$j$ 可达，则 $(i,j)\\in E^*$，即所有可达关系的集合。\n一种有效的方法是为所有边赋权重 1，再运行 Floyd 算法，以得到所有可达的路径。另一种效率更高的方法是，以逻辑或操作和与操作来代替 Floyd 算法中的 min 和 +。这样，可以将整数型的存储空间减少到布尔型的，同时也能够带来更快的速度。\nJohnson 算法 # Johnson 算法适用于在稀疏图中寻找最短路径，复杂度为 $O(V^2\\log_2V+VE)$，或者通常情况使用二叉堆时的 $O(VE\\log_2V))$。当 $E$ 较小时，其时间代价低于上面两种算法。Johnson 算法的基本思想是重新赋予权重。\n首先，对于非负权重的图，我们可以对每个节点使用 Dijkstra 算法来寻找单源最短路径，如果使用斐波那契堆，其复杂度为 $O(V^2\\log_2V+VE)$。如果图中包含负权重但没有负权重的环，则只要计算出一组新的非负权重进行替换，再使用 Dijkstra 算法即可。我们将在这个过程耗费 $O(VE)$ 的时间。\n重新赋予权重的方法 # 显然，赋予权重不能改变最短路径。权重函数如下定义：\n$$ \\hat w(p) = w(p) + h(v_0) - h(v_k) $$\n可以做如下的证明：\n$$ \\begin{aligned} \\hat w(p) \u0026amp; = \\sum^k_{i=1} \\hat w(v_{i-1},v_i) \\cr \u0026amp; = \\sum^k_{i=1} \\left[w(v_{i-1}, v_i) + h(v_{i-1}) - h(v_i)\\right] \\cr \u0026amp; = \\sum^k_{i-1} w(v_{i-1}, v_i) + h(v_0) - h(v_k) \\cr \u0026amp; = w(p) + h(v_0) - h(v_k) \\end{aligned} $$\n又因为 $h(v)$ 不依赖于当前的具体路径，对于更大的 $w(v_1,v_2)$，必然有更大的 $\\hat w(v_1,v_2)$。而且，如果原图中有一个权重为负的环路，在新图中同样会是一个负权重的环路。\n接下来，我们需要让 $\\hat w$ 取得非负值。首先，我们为一个新图 $G\u0026rsquo;=(V\u0026rsquo;,E\u0026rsquo;)$ 增加一个结点，并让其具备指向每一个结点的边，权重为 0。\n然后，我们定义 $h(v)=\\delta(s,v)$。这样，对于所有与 $s$ 间以负权重相连的结点，其 $h$ 都将是负值，而以正权重路径相连的结点的 $h$ 将会是 0。这样，我们就得到了新的权重函数。这个过程可以使用 Bellman-Ford 算法来实现，既能够发现负权重环又能够求得 $h(v)$。\n最大流算法 # 流网络 # 流网络的形式 # 流网络可以被概括成这样一个有向图：图中包括一个源点和一个汇点，二者的速率相等。每条边都有一个固定的最大流量。物料在结点上不能储存，即每个节点的入量与出量应当相等。这条流量守恒性质与基尔霍夫电流定律等价。图中不能存在两个节点之间往返的两条边（反平行边）。\n形式上，我们使用一个有向图 $G=(V,E)$，每条边有一个非负的最大流量值 $c(u,v)$，及其实际流量 $f(u,v)$。为方便起见，把所有不存在的边的流量设置为 0，并假设所有节点都在源点 $s$ 与汇点 $t$ 的一条路径上。这样，流网络一定是连通的，并且除了 $s$ 之外，每个节点都有入边。因此，$|E|\\geqslant|V|-1$。我们将其特性总结为两个显而易见的性质：$0\\leqslant f(u,v)\\leqslant c(u,v)$，以及任何一个节点的出流量和入流量相等。\n使用反平行边来模拟问题 # 我们的算法中通常不允许反平行边的出现。当遇到这种问题时，可以通过加入一个节点来规避这种情况。如，对于 $u\\to v\\to u$，可以转化为 $u\\to e\\to v\\to u$。\n具有多个源点和汇点的问题 # 解决这个问题的方法实际相当简单。加入新的超级源点和超级汇点，并将所有的源点和汇点变成与它们直接连接的结点即可。显然，这些新加入的边的容量应当为 $\\infty$。这样才能使得新的图与旧的图性质相同。\nFord-Fulkerson 方法 # 残存网络 # 已知一个流网络 $G(V,E)$ 及其中的一个流 $f$，定义残存网络的容量 $c_f(u,v)$，则：\n$$ c_f(u,v) = \\begin{cases} c(u,v) - f(u,v) \u0026amp; \\text{if } (u,v) \\in E \\cr f(v,u) \u0026amp; \\text{if } (v,u) \\in E \\cr 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n残存网络的容量相当于这个流在这个流网络中还能变动的空间，其形式与流网络类似，只是允许反平行边的存在。如果 $f\u0026rsquo;$ 是残存网络中的一个流，那么定义流 $f\u0026rsquo;$ 对流 $f$ 的递增（augmentation）：\n$$ f \\uparrow f\u0026rsquo; = \\begin{cases} f(u,v) + f\u0026rsquo;(u,v) - f\u0026rsquo;(v,u) \u0026amp; \\text{if } (u,v) \\in E \\cr 0 \u0026amp; \\text{otherwise} \\end{cases} $$\n相当于在叠加了这两个流之后得到的新流。\n增广路径 # 增广路径是残存网络中的一条从 $s$ 到 $t$ 的简单路径。定义这条路径 $p$ 的残存容量 $c_f(p) = \\min\\{c_f(u,v) | (u,v)\\text{belongs to path }p\\}$。\n那么，我们就可以在原来的流基础上，将这条增广路径上的所有边的流量均增加 $c_f(p)$，而不会破坏流网络的性质。此外，由于增广路径有可能经过与原网络中的边相反的边，这时原图中的流的某些边的流量就会减小。\n可以证明，新的到的流比原来的流更加接近最大流。而且，当且仅当残存网络中不再存在增广路径时，得到的流是最大流。这就是 Ford-Fulkerson 方法的核心思想。\nFord-Fulkerson 算法的实现 # 在算法的每次迭代中，寻找一条增广路径，并将其应用到流上，以 $f\\uparrow f_p$ 来代替 $f$，直到不存在增广路径为止。\n算法的时间代价取决于寻找增广路径的方法。不过，当边的容量为无理数时，寻找增广路径的过程有可能无法终止。实际情况中我们处理的大多是整数容量，如果遇到有理数容量，则可以将其乘一个系数。\n我们以整数容量的情况为例。如果使用 DFS 或 BFS 来寻找路径，那么找到一条路径的时间为 $O(V+E\u0026rsquo;)=O(E)$。每一次使用增广路径都对流的容量带来一次增加，所以整个算法的运行时间为 $O(E|f^*|)$，其中 $|f^*|$ 为最大流的容量。\n使用 BFS 作为路径算法的 Ford-Fulkerson 方法称为 Edmonds-Karp 算法。由于使用 BFS，每次找到的增广路径都必定是一个最短路径。整个算法的复杂度为 $O(VE^2)$。\n最大二分分配 # 最大二分分配问题是指，给定一个二分图（由两组结点组成的图，边只存在两种结点之间，而不存在于同种结点之间），求一组边，实现最大匹配。即，一个所有边组成的集合的子集，这个子集的元素数量最多。\n这个问题相当于一个多源点多汇点的最大流问题，每条边的容量都被设定为 3。随后，应用 Ford-Fulkerson 方法即可。\n"},{"id":24,"href":"/notes/core-java-impatient/8/","title":"8. Threading","section":"Core Java for Impatients","content":" 线程和进程 # 线程 # 如果要等待另一个线程完成，使用 join 方法。可以为这个方法加入超时时间。\nthread.join(millis); 当 run 方法返回或者抛出异常时，线程结束。每个线程都有自己的异常处理器，默认继承自线程组，通常就是我们熟悉的全局错误处理器。（err 流）可以使用 setUncaughtExceptionHandler 来改变这个处理器。\n有时，我们让几个线程执行类似的任务，而最终只需要一个结果，其他的线程都可以被取消。可以在 Runnable 中检查是否中断：\nRunnable task = () -\u0026gt; { while (check == true) { if (Thread.currentThread().isInterrupted()) return; ...; } } 实际上，中断（Interrupt）并没有一个非常准确的定义，更多的是程序员自己用来处理一些问题。\n如果线程在 wait 等待状态或者 sleep 休眠状态被中断，就会直接抛出 InterruptedException。对于这种情况，可以直接在 Runnable 中使用 try-catch 块进行处理。\nRunnable task = () -\u0026gt; { try { ...; } catch (InterruptedException ex) { Thread.currentThread().interrupt(); } } 如果线程在运行中被中断，那么一旦 sleep 被调用，就会立刻抛出异常。很多情况下我们并没有什么可做的，可以考虑直接把线程设置一下中断状态，或者继续将其抛出给真正能够处理的位置。\n线程变量和其他属性 # 很多时候我们并不真的需要在线程之间共享变量。Java 提供了 ThreadLocal 类来解决这个问题。这样，每个线程都将得到独立的变量。\npublic static final NumberFormat format = NumberFormat.getCurrencyInstance(); String amount = format.format(total); // may cause race condition, but no need of synchronize public static final ThreadLocal\u0026lt;NumberFormat\u0026gt; format = ThreadLocal.withInicial(() -\u0026gt; NumberFormat.getCurrencyInstance()); String amount = format.get().format(total); Thread 类还有一些不常用的其他属性。线程可以分组，但在有了线程池之后，这种方法就不再常用。线程的优先级和虚拟机的实现有关，因此意义不大。线程有状态，但在 Java 中很少用到，对系统开发可能更有用。对于未捕获的异常，可以为每个线程设置不同的异常处理器。\n此外还有 thread.setDaemon(true) 方法。当虚拟机中只剩下 daemon 线程时，虚拟机会退出。因此我们就不再需要手动处理这个问题。\n进程 # ProcessBuilder builder = new ProcessBuilder(\u0026#34;gcc\u0026#34;, \u0026#34;myapp.c\u0026#34;); builder = builder.directory(path.toFile()); // returns itself, so: Process p = new ProcessBuilder(\u0026#34;gcc\u0026#34;, \u0026#34;myapp.c\u0026#34;).redirectInout(inputFile) .directory(path.toFile()).start(); OutputStream processIn = p.getOutputStream(); // input of process InputStream processOut = p.getInputStream(); InputStream processErr = p.getErrorStream(); builder.redirectErrotStream(true); // combine err and out stream Map\u0026lt;String, String\u0026gt; env = builder.environment(); env.put(\u0026#34;LANG\u0026#34;, \u0026#34;zh_CN\u0026#34;); env.remove(\u0026#34;JAVA_HOME\u0026#34;); // wait for process to end int result = p.waitfor(); // or with timeout if (p.waitfor(delay, TimeUnit.SECONDS)) { int result = p.exitValue() } else { p.destroy(); // SIGTERN // or p.destroyForcibly(); // SIGKILL } 异步 # 这种问题最常见于 GUI 程序：UI 线程不应该被任务阻塞，因此需要把任务方法其他的线程里。而且，UI 框架（例如 JavaFX 和 Android）通常不是线程安全的，因此只有 UI 线程本身应当修改屏幕上的组件。\nCompletableFuture\u0026lt;String\u0026gt; contents = readPage(url); CompletableFuture\u0026lt;List\u0026lt;URL\u0026gt;\u0026gt; links = contens.thenApply(Parser::getLinks); 这样，代码的每一句都不会被阻塞，所有数据都被包装在 Future 中。只有上一步结束之后，下一步才会被执行。\n类似 thenApply 这样的方法通常都有两个变种，其中带有 Async 的会在另一个线程中执行。\nCompletableFuture\u0026lt;U\u0026gt; future.thenApply(Function\u0026lt;? super T, U\u0026gt;); CompletableFuture\u0026lt;U\u0026gt; future.thenApplyAsync(Function\u0026lt;? super T, U\u0026gt;); void future.thenAccept(Consumer\u0026lt;T\u0026gt;); void thenRun(Runnable); CompletableFuture\u0026lt;U\u0026gt; future.thenCompose(Function\u0026lt;? super T, CompletableFuture\u0026lt;U\u0026gt;); 然后我们要考虑抛出异常的情况。当 CompletableFuture 里抛出异常时，异常会被包装在一个 Unchecked 的 ExecutionException 里，在 get 方法被调用时抛出。很多时候我们并不一定调用 get，这时使用 handle 方法，这些方法同样也有 Async 变种：\n// handle exception, returns null if no exception future.handle(BiFunction\u0026lt;T, Throwable, U\u0026gt;); future.whenComplete(BiConsumer\u0026lt;T, Throwable\u0026gt;); 此外，还有一些用来对多个 CompletableFuture 的结果进行处理的方法。技术上说，它们的参数实际上是 CompletionStage 接口类型，但 Java 类库中只有 CompletableFuture 实现了这个接口。第三方框架可能会有其他实现在这里也省去了必须的 super 和 extends。\nthenCombine(CompletableFuture\u0026lt;U\u0026gt;, BiFunction\u0026lt;T, U, V\u0026gt;); thenAcceptBoth(CompletableFuture\u0026lt;U\u0026gt;, BiConsumer\u0026lt;T, U\u0026gt;); runAfterBoth(CompletableFuture\u0026lt;?\u0026gt;, Runnable); // apply when one of them complete applyToEither(CompletableFuture\u0026lt;T\u0026gt;, Function\u0026lt;T, V\u0026gt;); AcceptEither(CompletableFuture\u0026lt;T\u0026gt;, Function\u0026lt;T, V\u0026gt;); runAfterEither(CompletableFuture\u0026lt;?\u0026gt;, Runnable); // returns void after condition static allOf(CompletableFuture\u0026lt;?\u0026gt;...); static anyOf(CompletableFuture\u0026lt;?\u0026gt;...); 线程池 # Executor # Runnable task = () -\u0026gt; {}; Executor exec = ...; exec.execute(task); // get executors from factory Executors.newCachedThreadPool(); Executors.newFixedThreadPool(nthreads); // get CPU core for fixed pool int processors = Runtime.getRuntime().availableProcessors(); 这里，CachedThreadPool 适合大量小任务，或者等待（阻塞）时间较长的任务，如网络相关的。如果加入新任务时有空闲的线程，就会直接利用空闲线程。如果线程空闲超过一分钟，就会被移除。\nFixedThreadPool 硬性限制了线程的最大数量，适合计算密集型的任务，有利于将一个任务长期绑定在一个 CPU 核心上，避免上下文切换开销。\nExecutorService # 对于可能抛出异常和需要返回值的任务，需要使用 Callable 和 ExecutorService 实例。ExecutorService 是 Executor 的子接口。同样使用两种 ThreadPool，调用的方式有所不同：\nCallable\u0026lt;V\u0026gt; task = ...; Future\u0026lt;V\u0026gt; result = exec.submit(task); Future\u0026lt;V\u0026gt; 接口表示未来可能用到的的结果，包含这些方法：\nV get() throws InterruptedException, ExecutionException; V get(long timeout, TimeUnit unit) throws InterruptedException, ExecutionException, TimeoutException; boolean cancel(boolean mayInterruptIfRunning); boolean isCanceled(); boolean isDone(); get 方法可以阻塞，或者使用超时时间，然后返回计算值。如果抛出了异常，这个异常将被包装，并作为 ExecutorException 被抛出。\n如果在任务尚未开始时取消任务，那么任务会直接被删除。否则，只有在 MayInterruptIfRunning 为真时，才能中断线程，这是为了安全考虑。\n如果要让任务可中断，除了标记之外，还要在任务内部循环检查是否被中断，以便正确处理。\nCallable\u0026lt;V\u0026gt; task = () -\u0026gt; { while (...) { if (Thread.currentThread().isInterrupted()) return null; // actrual work } return result; } 多个任务的情形 # 如果有一批任务需要分别处理，可以把 Callable 集合传进 exec：\nList\u0026lt;Callable\u0026lt;Long\u0026gt;\u0026gt; tasks = ...; List\u0026lt;Future\u0026lt;Long\u0026gt;\u0026gt; results = executor.invokeAll(tasks); invokeAll 还有一个支持超时的变种。不过，这种方式下，直到所有任务都完成，主调任务都会阻塞。\n一种解决方法是使用 ExecutorCompletionService：\nExecutorCompletionService service = new ExecutorCompletionService(executor); for (Callable\u0026lt;V\u0026gt; c : tasks) service.submit(task); for (int i = 0; i \u0026lt; tasks.size(); i++) { service.take().get(); ... } 任务返回的 Future 会以结束的顺序来排列，也就是说，程序将在 get() 处阻塞。\n另外一种方式是使用 invokeAny 方法，这种方式适用于搜索之类的任务。只要有任何一个任务完成，就直接返回结果，而其他尚未完成的任务则被取消。因此这个方法只有一个返回值，而不是一个 List。\nV found = executor.invokeAny(tasks); 线程安全 # 非原子操作 缓存 指令重排 保证更新可见的几种情况 # 这里讨论的是由于线程各自有独立的栈带来的问题：线程 A 对变量 a 的更新，可能在一定时间之后才会显示在线程 B 的栈上。下面这些情形下，一个线程进行的改变会立刻体现到所有的线程里：\nfinal 变量在初始化后的值对所有线程可见 静态变量在静态初始化结束后的值对所有线程可见 对 volatile 变量的更新对所有线程可见 发生在锁释放前的改变，对任何试图获取同一个锁的线程是可见的 这些情况适合处理只有一个线程对变量的值进行修改的情况。\n竞争条件 # 竞争条件出现在两个线程同时修改一个变量的情形。由于对变量的更新不是原子的，变量修改可能无法正常体现。这种情况下，volatile 关键字就不足以解决问题。常用的方法有三种：\n不使用多线程共享变量。 使用 Immutable 变量（实际上也是避免了多线程的共享） 使用锁（降低性能）。 使用分区锁，这是锁的一种特殊情况。ConcurrentHashMap 和流都使用了这种方法。 在实现 Immutable 类的过程中，要保证所有的实例变量都是 final 的，所有的修改方法都返回新的对象，而且所有的方法也都是 final 的，以免被继承覆盖。除此之外，还要保证构造方法里没有可能访问的情形。例如，如果在构造方法里运行一个新的线程，那么这时使用的 Runnable 对象就持有当前对象的 this 引用。如果 Runnable 运行时构造还没有结束，就有可能改变我们创建的对象。\n使用并行算法 # Stream API 中的 parallelStream 可以生成并行的类，这里就使用了分段锁。因此，在这个流上进行的操作需要避免共享变量。类似地，Arrays 类中也有一些并行方法：\nArrays.parallelSetAll(values, i -\u0026gt; i % 10); Arrays.parallelSort(words, Comparator.comparing(String::length)); Arrays.parallelSort(words, words.length / 2, words.length); // range to be sorted Arrays.parallelPrefix(arr, (left, right) -\u0026gt; ...); // for [1, 3, 5, 7], (left, right) -\u0026gt; left * right : // get [1, 3, 15, 105] which is [1*1(default), 1*3, 1*3*5, 1*3*5*7] 使用线程安全的数据结构 # 多个线程同时处理一个数据结构，如队列或者 HashMap，可能会造成数据结构的损坏，甚至线程死锁。\njava.util.concurrent 包中的集合都是线程安全的，相应地，它们产生的迭代器都是 弱一致的。这意味着，相比java.util 包中的集合，即使迭代器构造之后修改集合，也不会抛出 ConcurrentModificationException 异常。不过相应地，迭代器有可能不能准确地反映出元素的修改情况。\nConcurrentHashMap # 考虑一个计数器的例子：\nConcurrentHashMap\u0026lt;String, Long\u0026gt; map = new ConcurrentHashMap\u0026lt;\u0026gt;(); ... Long oldVal = map.get(word); map.put(word, (oldVal == null) ? 1 : oldVal + 1); ConcurrentHashMap 中的锁没有办法控制 map.put 的原子性。我们可能先取出值，这个值被另一个线程修改，再放回这个值，这时就出现了数据的错误。因此，需要使用 map 中的其他方法：\nmap.compute(word, (k, v) -\u0026gt; (v == null) ? 1 : v + 1); 类似的，还有 computeIfAbsent computeIfPresent putIfAbsent 方法。\n或者，对于计数器这种操作，还可以用更加简单的 merge 方法：\nmap.merge(word, 1L, (oldVal, newVal) -\u0026gt; oldVal + newVal); mep.merge(word, 1L, Long::sum); 这样，如果 map 中不存在对应的值，就会使用默认值 1L。\n当然，这里传入的 lambda 表达式应当是短小快速的，并且不应该视图修改 map 的映射本身。这样，map 才能很好地保证其原子性。在 compute 方法出现之前，人们使用一种反复取值以保证修改成功的模式：\ndo { oldVal = map.get(word); newVal = oldVal + 1; } while (!map.replace(word, oldVal, newVal)); 如果 map 中的值已经不再是我们取出来的值，就需要从头进行取值和计算工作。\n阻塞队列 Blocking Queues # 阻塞队列是一个常用的解决生产者-消费者问题的数据结构。生产者从一端压入，消费者从另一端取出。如果像一个满队列压入或对一个空队列取出，操作就会阻塞。\n对于 BlockingQueue 来说，put 方法和 take 方法是阻塞的，而 add 方法和 remove 方法会抛出异常。这样就使其和一般的 Queue 接口兼容。offer 方法添加一个元素并返回真，如果队列满则返回假。poll 和 peek 则是在队列为空时返回 null。此外，还有使用超时方式的变种：\nboolean success = q.offer(x, 100, TimeUnit.MILLISECONDS); Object head = q.poll(100, TimeUnit.MILLISECONDS); 与普通的集合类似，也有 BlockingDeque 提供双向队列，LinkedBlockingQueue 提供链表，ArrayBlockingQueue 使用数组，LinkedBlockingDeque 等实现。\n其他数据结构 # Java 的 concurrent 包提供了另外一些数据结构：ConcurrentSkipListMap ConcurrentSkipListSet CopyOnWriteArrayList CopyOnWriteArraySet。\nSkipListMap 中的数据是有序的。在有序集合中，显然 TreeMap 效率最高，其次是使用 Collections.synchronizedSortedMap() 来包装 TreeMap，当并发量很高时，跳跃链表的优势才显现出来。\n暂时还没有并发的Set 数据结构。ConcurrentHashMap 的静态方法 newKeySet 产生一个 Set\u0026lt;K\u0026gt; 对象，实际上是一个 ConcurrentHashMap 的包装，一个内部类 KeySetView，其所有的 value 都是 Boolean.TRUE，可以直接当做一个 Set 来使用。在这个 Set 中删除元素会影响 Map 本身，但向其中插入是没有意义的，因为没有相应的 value。相比之下，如果向 Map.keySet(V default) 得到的 Set 中插入值，则会使用默认值作为 value。\n原子值 # 原子值放在 java.util.concurrent.atomic，包括 AtomicXxx AtomicXxxArray AtomicXxxFieldUpdater 等，其中 Xxx 可以为 Integer Long Reference。\npublic static AtomicLong num = new AtomicLong(); long id = num.incrementAndGet(); num.updateAndGet(x -\u0026gt; Math.max(x, someOtherNum)); // DO NOT use num.set() num.accumulateAndGet(someOtherNum, Math::max); // combine the value and param 这一类更新操作是乐观的（在 JDK 8 中，其实现就是上面的 do-while 方式）。这意味着当并发量很高时，竞争将会十分激烈。这时，可以使用 LongAdder 或 LongAccumulator。\nfinal LongAdder count = new LongAdder(); count.increment(); count.add(1L); long total = count.sum(); LongAccumulator 的用法我们之前也见过类似的例子，常用于处理累加类的问题。在构造时提供操作和中立元素，即离散数学中的单位元，对于累加是 0，对于累乘是 1。然后使用 accumulate 方法传入值，使用 get 方法获取值。\nLongAccumulator acc = new LongAccumulator(Long::sum, 0); acc.accumulate(val); long sum = acc.get(); 这两个原子值类的思想是类似的：将运算操作推后，集中起来进行。因为大多数情况下，我们并不需要在运算的每一步立刻得到结果，而只是需要一个最终的累加值。显然，这样的操作应该遵守交换律和结合律。Java 提供了类似的 DoubleAdder 和 DoubleAccumulator。\n实现上，LongAccumulator 内部实际上有一个 Long 类型的集合，每个元素都被初始化为单位元，在这里是 0。每当 accumulate(param) 的时候，就将新加入的值与其中一个值进行运算，例如在这里是用 0 + param 来替换原来的 0。最后，到了调用 get 时，所有的值才会进行累加操作。\n之前我们已经看到 compute(K, Function\u0026lt;? super K, ? extends V\u0026gt;)。如果把 LongAdder 作为 map 的 value，那么就可以使用：counts.computeIfAbsent(key, k -\u0026gt; new LongAdder()).increment()，对映射中的原子值进行操作。\n锁 # 可重入锁 ReentrantLock # Lock countLock = new ReentrantLock(); int count; // variable to be shared countLock.lock(); try { count++; // critical area } finally { countLock.unlock(); // release the lock，no matter throw exception or not } 这里，我们显式地使用锁来处理多线程冲突问题。可以看到，这个过程和操作系统提供的锁非常类似。不过，大多数情况下我们使用 synchronized 关键字来隐式地锁住对象。不过，处理锁的问题相当复杂，有可能会造成糟糕的死锁。所以如果能使用线程安全的数据结构，在线程之间传递数据，应该尽量避免使用锁。\n\u0026ldquo;可重入\u0026quot;的典型情况体现在递归方法上。假设我们在一个线程上递归访问同一个同步变量，那么这个线程的栈中的每一次方法调用之间并不会发生冲突，而是被计数。直到计数为 0 的时候，说明所有的方法调用都结束，放开对象的锁，这时整个锁也被放开。\nsynchronized 关键字 # 上面的代码相当于：\nsynchronized (count) { count++; } 使用 synchronized 关键字也能够保证可见性。例如：\npublic class Flag { private boolean done; public synchronized void set() { done = true; } public synchronized boolean get() { return done; } } 这样，在 synchronized 方法内对变量进行修改，在所有的线程里都能观察到改变。\n由于对象的锁是公有的，所有位置都可以对一个对象上锁。对实例方法使用 synchronized 关键字相当于锁住整个对象，对静态方法使用相当于锁住整个类。对变量上锁则只是锁住对象。\n在标准库中，HashTable 的几乎所有方法都是 synchronized 的。因此它是线程安全的，但性能较差。更多关于锁的细节可以看这里。\n条件等待 # 现在我们尝试编写一个阻塞的多线程的队列。为了方便起见，我们使用链表来实现这个队列。这样，插入过程永远不会阻塞，删除过程在队列为空时阻塞。对于一个非阻塞的实现，方法应该这样实现：\npublic class Queue { private Node head; private Node tail; ...; public synchronized void add(Object val) { // push at the tail } public synchronized Object remove() { if (head == null) return null; // remove and return value } } 如果要实现一个阻塞形式的队列，我们就需要在判断 head == null 时让线程等待。不能在这里直接自旋，因为当前线程持有队列对象的锁，其他线程无法进来插入新的元素。\npublic synchronized Objetc take() throws InterruptedException { while (head == null) wait(); ...; } public synchronized void add(Object val) { ...; notifyAll(); } 两个方法持有的都是所在对象的锁。在 notifyAll() 之后，所有正在 wait() 的线程都会被唤醒。因此，需要使用循环再次检查，最终只有一个线程能够进行操作，其他所有的都会再次进入 wait（因为是 synchronized 的）。如果只想唤醒一个线程，则可以使用 notify()，但如果被唤醒的线程再次等待，而没有其他的 notify，就可能发生死锁。\n这种方法可以用来构建阻塞的数据结构，但如果只是想让线程等待某个条件，最好直接使用 Java 提供的其他同步类库，例如 CountDownLatch 和 CyclicBarrier。\n"},{"id":25,"href":"/notes/core-java-impatient/9/","title":"9. Notations","section":"Core Java for Impatients","content":" 注解 # 注解元素 # @Test(timeout=10000) @BugReport(showStopper=true, assignedTo=\u0026#34;Harry\u0026#34;, testCase=CacheTest.class, status=BugReport.Status.COMFIRMED ) @SuppressWarnings(\u0026#34;unchecked\u0026#34;) // only one parameter, equals @SuppressWarnings(value=\u0026#34;unchecked\u0026#34;) @BugReport(reportedBy={\u0026#34;harry\u0026#34;, \u0026#34;fred\u0026#34;}) // array of above @BugReport(ref=@Reference(id=1123)) // another annotation 注解元素可以是：\n基本类型 String class 对象 enum 实例 注解 这些元素的一维数组 元素可以有默认值。例如 JUnit 的 @Test 注解的 timeout 元素默认为 0。\n使用注解 # 一个位置可以有多个注解。如果注解被声明为可重复的，甚至可以重复同一个注解多次。\n泛型类的类型参数也可以使用注解。在包的 package-info.java 中包含了带有注解的包语句，是包的 Javadoc。\npublic class Cache\u0026lt;@Immutable V\u0026gt; {} @GPL(version=\u0026#34;3\u0026#34;) package com.test; import org.gnu.GPL; 注解可以用来标记一些信息，这些信息通常会在编译时进行检查。典型的例子是 @NotNull。它们出现的位置包括：\nList\u0026lt;@NotNull String\u0026gt;; // anywhere generic parameter is Comparator.\u0026lt;@NotNull String\u0026gt;reverseOrder; class Warning extends @Localized Message; // superclass, implemented interface new @Localized String(); // constructor Map.@Localized Entry; // nested types(Map.Entry) (@Localized String) text; // casts and instanceof (text instanceof @Localized String); // don\u0026#39;t influence result public String read() throws @Localized IOException; // exception specifications List\u0026lt;@Localized ? extends Message\u0026gt;; // wildcards and type bounds List\u0026lt;? extends @Localized Message\u0026gt;; @Localized Message::getText; // method reference 相应的，在类名处和 import 语句处不能使用注解。例如：\n@NotNull String.class; // WRONG import java.lang.@NotNull String; // WRONG 在上面的使用中，我们都只能为参数添加注解，而不能为被调用者添加注解。如果要这样做的话，需要一个比较特殊的语法：\npublic class Point { public boolean equals(@ReadOnly Point this, @ReadOnly Object other) {} } 我们新增了第一个参数，并命名为 this。通过这种方式，为方法所在的对象添加了注解。不过，构造方法不能使用这种方式。严格来说，构造方法的运行过程中 this 引用还没有指向某个对象。\n在内部类的构造方法中，虽然对象本身的 this 还没有完成，但其外部类已经构造完成。因此，可以对外部类使用注解：\nstatic class Sequence { class Iterator implements java.util.Iterator\u0026lt;Integer\u0026gt; { public Iterator(@ReadOnly Sequence Sequence.this) { this.current = Sequence.this.from; ...; } } } 定义注解 # 每个注解是由一个注解接口 @interface 声明的，接口的方法对应注解元素，Java 将实际编译一个接口。接口的方法不能有参数，不能声明抛出异常，也不能是泛型的。以 JUnit 中的 @Test 为例：\n@Target(ElementType.METHOD) @Retention(RetentionPolicy.RUNTIME) public @interface Test { long timeout() default 0L; ... } 这里的 @Target 和 @Retension 是元注解（meta-annotation）。\ndefault 标记了注解元素的默认值，对于数组，使用 {} 提供一个空数组的默认值。需要注意的是，默认值是在虚拟机启动之后动态计算得到的。例如，在默认值为 1 的版本下编译的类，如果在加载了默认值为 2 的版本的注解的环境下运行，其默认值就会是 2。\n@Target # @Target 指示注解可以出现的位置，如果不使用 @Target，那么注解可以出现在除类型参数和类型用途之外的任何地方。\nANNOTATION_TYPE ：注解类型声明 PACKAGE TYPE ：包括 enum 和接口，接口也包括注解 METHOD CONSTRUCTOR FIELD ：包括 enum 常量 PARAMETER LOCAL_VARIABLE TYPE_PARAMETER TYPE_USE 类型的用途 @Retention # @Retention 指示访问注解的方式，RetentionPolicy 枚举有三种选择。\nSOURCE ：对源代码处理器可见，不包含在 class 文件里。 CLASS ：注解包含在类文件里，但虚拟机 Class Loader 不会加载。这是默认行为。 RUNTIME ：注解在运行时可见，并可以通过反射 API 访问。 标准注解 # 注解接口 应用于 目的 @Override 方法 检查重载 @Deprecated 所有声明 标记弃用 @SuppressWarnings 除包之外 抑制警告 @SafeVarargs 方法和构造函数 断言不定长参数安全 @FunctionalInterface 接口 函数式接口 @PostConstruct 方法 在构造后立即调用 @PreDestroy 方法 在删除被注入对象前调用 @Resource 类，接口，方法，域 资源或依赖注入 @Resources 类，接口 资源数组 @Generated 所有声明 工具产生的源代码 @Target 注解 使用注解的位置 @Retention 注解 指定使用注解的位置 @Documented 注解 指定包含在被注解的文档中 @Inherited 注解 指定被子类继承 @Repeateble 注解 指定可以应用多次 编译相关的注解 # 打上 @Deprecated 的方法在编译时会发出警告。 @SafeVarargs(\u0026quot;unchecked\u0026quot;) 断言一个方法不损坏其可变参数。 @Generated(value=\u0026quot;com.test\u0026quot;, date=\u0026quot;...\u0026quot;) 需要唯一标识符和可选的日期与注释，标记代码由代码生成工具自动生成。 @SafeVarargs 需要使用的一个例子：\npublic static \u0026lt;T\u0026gt; fun(T... args) { ...; } 这时编译器会报警告，表示这里有可能出现类型安全问题。如果方法是 final 或 static 的，且程序员能够保证这里不会出现安全问题，则可以使用这个注解去除警告，而不必使用 @SuppressWarnings。\n资源管理相关的注解 # @PostConstruct 和 @PreDestroy 用于需要控制对象生命周期的情形。\n@Resource 用于资源注入。例如：\n@Resource(name=\u0026#34;jdbc/employeedb\u0026#34;) private DataSource source; 然后，容器会将相应的数据库来源注入到 source 变量。\n元注解 # @Documented 标记的注解会出现在被修饰者的 Javadoc 中。功能性的注解如 @FunctionalInterface 需要用到，而 @SuppressWarnings 这样的实现相关的注解就不适合使用。\n@Inherited 用于打在类上的注解。这样的注解会被子类自动继承。这样的注解的用途可以参照 Serializable 这样的标记接口。\n@Inherited @interface Persistent {} @Persistent class Employee {} class Manager extends Employee {} // is also @Peristent @Repeatable 标记一个注解可以使用多次。对于这一类注解，需要为它再提供一个容器注解，这是由实现决定的。这些重复的注解会被装进容器注解的元素中。当然，这也会让这类注解的处理变得复杂。\n@Repeatable(TestCases.class) @interface TestCase { String params(); String expected(); } @interface TestCases { TestCase[] value(); } 运行时注解处理 # 定义运行时注解 # 运行时注解（@Retention(RetentionPolicy.RUNTIME)）最常见的用法是使用反射检查这一类对象。我们以一个 @ToString 注解为例，我们希望实现的是修改 toString 方法。\n@Target({ElementType.FIELD, ElementType.TYPE}) @Retention(RetentionPolicy.RUNTIME) public @interface ToString { boolean includeName() default true; } @ToString(includeName=false) public class Point { @ToString(includeName=false) private int x; @ToString(includeName=false) private int y; } @ToString public class Rectangle { @ToString(includeName=false) private Point topLeft; @ToString private int width; @ToString private int height; } 这样编写，我们希望一个矩形的 toString 得到 Rectangle[[5,10],width=20,height=30]。然后我们来处理 toString 方法。\n使用反射检查对象 # 反射类 Class Field Parameter Method Constructor Package 都实现了 AnnotatedElement 接口。从这个接口可以得到关于注解的信息。接口的内容包括：\nT getAnnotation(Class\u0026lt;T\u0026gt;) T[] getAnnotationByType(Class\u0026lt;T\u0026gt;) Annotaion[] getAnnotations() 这些方法的 getDeclared 变种。 然后，我们通过反射来查找注解。从这里我们可以看到将注解声明为接口，并把元素声明为方法的原因。\nToString ts = obj.getClass.getAnnotation(ToString.class); // annotation is an interface if (ts != null \u0026amp;\u0026amp; ts.includeName) ...; // element is a method 特别的是，我们之前看到，可重复的注解被包装进了另一个注解。因此，对于这一类就要先寻找容器注解，然后从其 value 的返回值里寻找其他注解。更好的方法是使用 getAnnotaionByType，这个方法会进入到容器注解的内部，并返回一个数组。\n编写方法 # 然后，我们在 ToStrings 类中实现一个 toString 方法。这个方法接收一个任意的 Object 对象，并使用反射来遍历其所有的域。因此我们需要这样编写这个类：\npublic class ToStrings { public static String toString(Object obj) { if (obj == null) return \u0026#34;null\u0026#34;; ToString ts = obj.getClass.getAnnotation(ToString.class); if (ts == null) return obj.toString(); // no annotation StringBuilder result = new StringBuilder(); if (ts.includeName()) { ...; } for (Field f : c1.getDeclaredFields) { ts = f.getAnnotation(ToString.class); ...; // check and recursive } ...; return result.toString(); } } 源码级注解处理 # 注解的另一种用法是自动处理源文件，以产生源码、配置文件、脚本等。我们依然修改 toString 方法，但不再使用反射，而是在源码上解决这个问题。最终的效果是，我们将不再使用反射来遍历域，而是为每一个打上了 @ToString 注解的类生成一个特定的 toString 方法，在这个方法里遍历对象的域。为了避免使用反射，我们将所有需要访问的域用一个 getter 包装起来。最终我们将得到与上面的运行时方式类似的一个 ToStrings 类，可以看做 @ToString 的伴随类：\npublic class ToStrings { public static String toString(Point obj) {...} public static String toString(Rectangle obj) {...} public static String toString(Object obj) { return Objects.toString(obj); } } 注解处理器 # javac -processor ProcessorClassName,Processor sourceFiles 编译器会定位源文件中的注解，并交给处理器来处理，产生新的源文件，再交给下一个处理器。最终，会产生一个新的源文件，而原来的不会被改动。最后，javac 进行编译。\n@SupportedAnnotationTypes(\u0026#34;com.test.annotations.ToString\u0026#34;) @SupportedSourceVersion(SourceVersion.RELEASE_8) public class ToStringAnnotationProcessor extends AbstractProcessor { @Override public boolean process(Set\u0026lt;? extends TypeElement\u0026gt; annotations, RoundEnvirionment currentRound) { ...; } } 在指明处理的注解类型时，可以使用通配符 com.test.annotations.*，甚至直接使用 * 处理所有注解。在每一轮中，都会调用 process 方法来处理本轮发现的所有文件的注解集合和相应的 RoundEnvironment 引用。\n语言模型 API # 接下来我们要处理源码。编译器会产生一个树节点是类实例的树，这些类实现了 javax.lang.model.element.Element 接口和它的各种子接口：TypeElement VariableElement 和 ExecutableElement 等。这里我们只进行简要的介绍。\nSet\u0026lt;? extends Element\u0026gt; getElementsAnnotatedWith(Class\u0026lt;? extends Annoattion\u0026gt; a); A getAnnotation(Class\u0026lt;A\u0026gt; annotationType); A[] getAnnotationsByType(Class\u0026lt;A\u0026gt; annotationType); typeElement.getEnclosedElements(); // return fields and methods element.getSimpleName().toString(); typeElement.getQualifiedName.toString(); 生成源码 # 注解处理器只能产生新类，而不能修改已有的类。因此，我们最终将把生成的代码放在 ToStrings 类中。对于类似的目标：\n@ToString public class Rectangle { @ToString(includeName=false) public Point getTopLeft() { return topLeft; } @ToString public static getWidth() { return width; } @ToString public static getWidth() { return width; } } 我们先编写一个产生 toString 方法的工具，这个工具接收一个类作为参数，如这里的 Point 和 Rectangle：\nprivate void writeToStringMethod(PrintWriter out, TypeElement te) { String className = te.getQualifiedName().toString; out.printf(\u0026#34;public static String toString(%s obj) {\\n\u0026#34;, className); out.printf(\u0026#34;StringBuilder result = new Stringbuilder();\\n\u0026#34;); /* prints method head code: * public static String toString(Rectangle obj) { * StringBuilder result = new Stringbuilder(); */ ToString ann = te.getAnnotation(ToString.class); if (ann.includeName()) { out.printf(\u0026#34;result.append(\\\u0026#34;%s[\\\u0026#34;);\\n\u0026#34;, className); } /* prints field class name code: * result.append(\u0026#34;Rectangle[\u0026#34;); */ for (Element c : te.getEnclosedElements) { // traverse all fields and methods ann = c.getAnotation(ToString.class); if (an != null) { if (an.includeName()) { // check if print field name out.printf(\u0026#34;result.append(\\\u0026#34;%s=\\\u0026#34;);\\n\u0026#34;, c.getName().toString().subString(3).toLowerCase()); } /* prints field name code: * result.append(\u0026#34;width=\u0026#34;); */ out.printf(\u0026#34;result.append(toString(obj.%s()));\\n\u0026#34;, c.getName().toString()); out.print(\u0026#34;result.append(\\\u0026#34;,\\\u0026#34;);\\n\u0026#34;); /* prints field value code: * result.append(toString(obj.getWidth())); * result.append(\\\u0026#34;,\\\u0026#34;); */ } } out.println(\u0026#34;result.append(\\\u0026#34;]\\\u0026#34;);\u0026#34;); out.println(\u0026#34;return result.toString();\u0026#34;) } 在这个方法中，我们接收一个 PrintWriter 和一个类，并遍历其所有的域，寻找我们之前编写的打上了注解的方法，为这些方法输出对应的代码语句。这些语句将是最终对源代码修改的一部分。\n最后，我们来编写处理器的 process 方法。\npublic boolean process(Set\u0026lt;? extends TypeElement\u0026gt; annotations, RoundEnvironment currentRound) { if (annotations.size() == 0) return true; try { JavaFileObject sourceFile = processingEnv.getFiler() .createSourceFile(\u0026#34;com.test.ToStrings\u0026#34;); try (PrintWriter out = new PrintWriter(sourceFile.openWriter())) { out.println(\u0026#34;package com.test.xxx;\u0026#34;) out.println(\u0026#34;public class ToStrings {\u0026#34;) out.print for (Element e : currentRound.getElementsAnnotatedWith(ToString.class)) { // traverse all annotated classes writeToStringMethod(out, (TypeElement) e); // call method above to process every class } out.print(\u0026#34;public static String toString(Object obj) {\\n\u0026#34;); out.print(\u0026#34;return Objects.toString(obj);\\n}\\n}\u0026#34;) } catch (IOException ex) { processingEnv.getMessager().printMessage( Kind.ERROR, ex.getMessage()); } } return true; } 这是一项非常需要细心的工作。无论如何，我们现在成功生成了 ToStrings 整个类的代码。当然，生成的其他文件不仅仅可以是 Java代码，也可以是脚本或 XML 配置等等。如果想要查看每一轮处理的详细情况，可以在编译时加上 -XprintRounds 参数，将会打印出每一轮处理的输入，处理的注解等情况。\n"},{"id":26,"href":"/cs/","title":"Computer Science","section":"czdm75 Blog","content":" Computer Science # Linux IO Multiplexing "},{"id":27,"href":"/notes/core-java-impatient/","title":"Core Java for Impatients","section":"Notes on Books","content":" Core Java For Impatients # 1. Basic OOP 2. Interface, Lambda 3. Inheritance, Reflection 4. Exception, Logging 5. Generics 6. Collections, Streams 7. IO, Regexp, Serialization 8. Threading 9. Notations "},{"id":28,"href":"/notes/ddia/","title":"Designing Data-Intensive Applications","section":"Notes on Books","content":" Designing Data-Intensive Applications # 1. Data System and Data Model 2. Storage, Query, Encoding 3. Replication and Partition 4. Transaction and Consistency "},{"id":29,"href":"/distributed/","title":"Distributed Systems","section":"czdm75 Blog","content":" Distributed Systems # Hadoop Basic Concepts Spark RDD Programming Spark SQL Programming "},{"id":30,"href":"/notes/in-depth-jvm/gc/","title":"Garbage Collection","section":"In-depth Understanding JVM","content":" 垃圾回收 # GC 算法 # 判断对象是否存活 # 判断对象存活的算法主要有两个：引用计数算法与可达性算法。其中，引用计数的实现十分简单，但这个算法对于循环引用不能很好地解决。因此，包括 C#、JAVA、Lisp 在内的主流实现使用的都是可达性分析算法。\n可达性分析算法 # 可达性分析算法的首要问题是 GC Roots 的选择。GC Root 可以理解为我们\u0026quot;希望存活\u0026quot;的引用。JVM 中可以作为 GC Root 的包括:\nVM 栈帧中的引用\n方法区中的静态属性\n方法区中的常量属性\n本地栈中的本地方法中的引用\n此外，在进行部分 GC 时，还可能把不收集部分指向收集部分的引用包括在内。例如，对新生代的 GC，老年代的对象中的引用也应该成为 GC Roots。此外，弱引用在新生代 GC 不被回收，因此属于 GC Root。到了 Full GC，弱引用需要被回收，就不属于 Root。当然，这些优化同时也会带来一定的代价，不同的 GC 器可能有不同的实现和权衡。无论如何，上面列出的四种一定会作为 GC Root 出现。\nJDK 1.2 之后，为了进行更加细致的 GC 处理，Java 引入了四种不同强度的引用。\nFinalize() 方法 # finalize() 方法的规范比较奇怪，这和 Java 的历史沿革有一定的关系。这个方法不应该在正常的开发中被使用。\n在 GC 过程中，对象要首先被标记，然后再进行真正的 GC。在标记之后，JVM 会对对象进行判断。如果：\n对象所在的类覆盖了 finalize() 方法 对象的 finalize() 方法没有被调用过 那么这个对象将会被放进队列，并执行 finalize() 方法。如果在 finalize() 方法中，对象重新建立了自己与存活对象的引用关系，例如将自己放进了一个集合中，那么这个对象就不会被回收。不过，finalize() 方法只能被调用一次。在下一次回收中，如果这个对象再次被标记，就没有机会再次\u0026quot;拯救自己\u0026quot;。而且，这个方法的优先级相当低。\npublic class FinalizeEscapeExample { public static Object obj = null; @Override protected void finalize() throws Throwable { super.finalize(); Systemout.println(\u0026#34;Finalize() Called\u0026#34;); obj = this; // escape from gc } public static void printStatus() { System.out.println(obj == null ? \u0026#34;dead\u0026#34; : \u0026#34;alive\u0026#34;); } public static void main(String[] args) { obj = new FinalizeEscapeExample(); obj = null; System.gc(); // not promised a gc, so use VM arg Thread.Sleep(500); printStatus(); obj = null; System.gc(); Thread.sleep(500); printStatus(); } } 得到的结果是：\nFinalize() Called alive dead 这说明，在第一次被回收时，对象成功从 GC 中逃脱。但 finalize() 只能被调用一次，因此第二次 GC 中这个对象被回收了。\n方法区的回收策略 # JVM 并未要求虚拟机对方法区进行垃圾回收。实际应用中这个区域也很少变得非常大，但在现代框架大量的动态生成的状况下，进行垃圾回收仍有必要。现在，HotSpot 默认为 Metaspace 不规定最大大小，但仍然在达到一定大小后进行 Full GC。\n方法区中的常量池的 GC 相对比较简单，只要这个常量没有被引用，就可以认为是无用的而进行 GC。而类型信息的 GC 必须满足：\n不存在任何该类的实例对象 加载类的 ClassLoader 已经被回收 类对应的 java.lang.Class 对象没有在任何地方被引用。这意味着无法通过反射访问该类的方法。 类至少要满足这样的条件才能被回收。相关的 JVM 参数包括：-Xnoclassgc -verbose:class -XX:+TraceClassLoading 等。\nGC 算法 # 标记 - 清除算法 Mark-Sweep # 标记清除算法的实现十分显而易见。这个算法的缺点主要有两个：首先，标记和清除两个过程的效率都不算高。其次，简单的标记清除会带来大量的内存碎片。不仅为内存分配管理带来困难，也可能在内存仍有大量剩余时就出现无法分配对象的情况（在 HotSpot 这类对象需要连续空间的 JVM 上）。\n复制算法 # 复制算法是当前绝大部分现代 JVM 用来处理新生代对象的算法。朴素的复制算法将内存分成两半，每次只使用一块。进行 GC 时，将所有存活的对象复制到另一块，这样得到的新的内存空间就是连续完整的。当然，这样做的代价未免太大。\n现代 JVM 使用的复制算法将新生代内存分成三部分：一块 Eden 和两块 Survivor 部分。JVM 在同一时间使用 Eden 空间和其中一块 Survivor 空间。\n在分配新对象时，所有的新生对象都被分配在 Eden 空间中。进行 GC 时，将 Eden 和 From Survivor 中所有存活的对象全部复制到 To Survivor 区域中。这样，我们就得到了新的 Survivor 和空白的 Eden 空间。\n这种设计基于 90 % 以上的对象都是 \u0026ldquo;朝生夕死\u0026rdquo; 这个实际情况。我们认为，一块 Survivor 足以装下上次 GC 到现在的所有仍然存活的对象。Eden 和 Survivor 的比例默认是 8，也就是 8:1:1 的分配比例，同时也意味着会有 10% 的内存被空闲。当 Survivor 空间不足时，我们使用老年代内存来进行分配担保（Handle Promotion）。\n标记 - 整理算法 Mark-Compact # 复制算法在对象存活率较高时需要大量的复制，并且如果不适用分配担保，就需要两倍的内存空间。因此，这种算法不适用于老年代的收集。因此，在这里通常使用标记 - 整理算法。其基本流程和标记清除算法一样，只是会将对象进行移动，\u0026ldquo;紧凑\u0026rdquo; 到内存区域的头部。\nHotSpot 中的算法实现：GC 停顿 # 枚举 GC Root # 枚举 GC Root 过程的主要问题是 Stop the World。显然，如果引用关系发生变化，就不可能进行 GC。考虑到现代很多应用仅方法区就能达到数百 MB，排查引用关系的时间可能是无法接受的。即使是几乎不会停顿的 CMS 收集器，枚举 GC Root 也要进行停顿。\n早期 Solaris 平台上出现的 Exact VM 提出了准确式 GC 的概念。这类 GC 的特点是，收集器明确地知道内存中每一块数据的实际类型，例如是整数还是引用地址。这样的结果是在进行对象的一次移动之后，收集器可以十分迅速的将所有指向该对象的引用进行修改。通过这种方式，Exact VM 抛弃了基于句柄的对象查找方式，减少了寻址的开销。\n在 HotSpot 中，使用一组称为 OopMap 的数据结构来完成这件事。在类加载过程和 JIT 编译过程中，虚拟机都会记录下对象内、寄存器内和栈中引用的位置。\n安全点 SafePoint # 但是，将所有引用的变化都记录到 OopMap 中是不现实的。因此，HotSpot 规定了一定的 \u0026ldquo;安全点\u0026rdquo;，只有在安全点才会发生 OopMap 的修改，也才能进行 GC。显然，SafePoint 太少会造成 GC 的等待，太多则会降低性能。为此，HotSpot 以 \u0026ldquo;长时间执行\u0026rdquo; 或 \u0026ldquo;指令序列复用\u0026rdquo; 的原则来规定 SafePoint，例如方法调用、循环跳转、异常跳转等。\n另一个需要考虑的问题是，GC 时需要让所有的线程都停下来，这就带来了线程之间的互相等待。这个过程主要有两种想法：\n抢先式中断，即首先中断所有的线程，再将未达到 SafePoint 的线程重新唤醒。显然，这种方式下线程的实现较简单，而收集器实现较复杂，而且会带来性能浪费。目前几乎没有 JVM 使用这种方式。 主动式中断，即在要进行 GC 时，将一个全局的中断标记设置好，由线程去轮询这个标记。轮询的位置和安全点相同。 安全区域 # 实际情况中，线程不一定始终都是运行的。如果线程被阻塞，就不会到达安全点，更不会去轮询标记。不过，这种情况下的线程显然对 GC 没有不良影响。因此，处于休眠或阻塞状态的线程被认为进入了安全区域（Safe Region）。GC 可以正常在此时发生。当线程被重新唤醒时，首先要检查GC Root 枚举或者 GC 是否已经完成。如果没有，就要等待完成才能离开安全区域，继续任务。\n垃圾收集器 # OpenJDK / Oracle JDK 7u14 之后，G1 收集器正式脱离了试验阶段。至此，HotSpot 中一共有七个可用的收集器。在下面的图示中，有连线的收集器可以协同使用。\nSerial / Serial Old / ParNew # Serial 是最早的收集器，使用单线程的方式进行 GC。进行 GC 时，JVM 在 SafePoint 停止所有的线程，并由 GC 线程完成的 GC 工作。二者的区别是使用场景，Serial Old 使用标记 - 整理算法，适合老年代。Serial 使用复制算法，适合新生代。在某些环境下，尤其是桌面客户端环境下，Serial 仍然是默认的收集器，因为其单线程模式对 CPU 的负担较小，且对于几百兆的堆，几十毫秒的暂停完全可以接受。\n同时，Serial Old 还担负着作为 CMS 收集器 和 G1 收集器的后备预案的作用，在新生代或老年代的 Minor GC 无法正常完成时，进行 Full GC，使用这个收集器。同时，Parallel Scavenge 收集器的老年代部分与 Serial Old 也十分相似。\nParNew 收集器是 Serial 的多线程版本，适用于新生代。其控制参数、收集算法、暂停情况、回收策略等几乎都与 Serial 一样。这个收集器被广泛用于 Server 的一个原因是，Parallel Scavenge 无法和 CMS 收集器配合工作，ParNew 就成为了新生代收集最合适的选择。当然，简单的多线程在单核心上带来的效果可能是负优化的，因此客户端上 Serial 仍然是一个比较好的选择。\nParallel Scavenge / Parallel Old # Parallel Scavenge 是一个新生代收集器。与追求最短的停顿时间的 CMS 不同，Parallel Scavenge 追求一个可控制的吞吐量（Throughput）。这个概念是指，希望更多的 CPU 时间被用来运行用户代码而非垃圾收集。这意味着，追求短暂停顿的 CMS 适合需要与用户交互的程序，能够带来更短的响应时间。而 Parallel Scavenge 追求更大的用户代码占比，也就是希望更快地完成一个后台批处理式的任务。这个收集器也被称为 \u0026ldquo;吞吐量优先\u0026rdquo; 收集器。\n相关的参数主要是 -XX:GCTimeRatio，控制 GC 时间在总时间中的占比，即吞吐量。若为默认值 99，即允许 1% 的 GC 时间。如果设置为 19，则允许 5% 的 GC 时间。-XX:MaxGCPauseMillis 指定允许的最长 GC 停顿。当然，停顿越短，GC 就会发生得越多，吞吐量也会下降。此外还有 -XX:+UseAdaptiveSizePolicy，如果开启，就不需要手动指定新生代大小、Eden 区域比例等参数。\n这时我们会发现，如果使用 Parallel Scavenge 作为新生代收集器，那么老年代就只能使用老旧的 Serial Old。因此，JDK 1.6 之后引入了 Parallel Old 收集器，这是 Parallel Scavenge 的老年代版本。总的来看，这两个收集器的合作效果相对于 Serial + Serial Old 的主要区别，是在暂停所有线程开始 GC 时，能够充分利用多个线程的优势，更快地完成 GC。\nCMS # CMS 的流程 # CMS（Concurrent Mark Sweep）的设计目标是争取最短的停顿时间，以适应交互式应用的需求，尤其适合 Web 场景。为了尽量减少 Stop The World，CMS 分成了四个步骤：\n初始标记 Initial Mark 并发标记 Concurrent Mark 重新标记 Remark 并发清除 Concurrent Sweep 其中初始标记和重新标记阶段需要 Stop The World，另外两个则可以在线程继续运行时进行。\n初始标记阶段只标记 GC Root 直接关联的对象，这个过程通常很快。然后，线程可以重启，CMS 进行并发标记，即 GC Root Tracing 过程，判断对象的存亡。接下来，再次停下线程，进行重新标记，目标是处理那些在并发标记阶段发生了引用关系变化的对象。这个过程虽然比初始标记略长，但比并发标记短得多。最后的清除过程显然可以在程序运行时进行，不受干扰。\n于是，一次 CMS 的 GC 需要停顿两次，但耗时最长的部分都能够并发地进行而不需要中断线程。因此一些文档也将其称为 Concurrent Low Pause Collector。\nCMS 的缺点 # 首先，并发的 CMS 收集器必然会消耗相当多的 CPU 资源，降低总的吞吐量。并发数越高，消耗的 CPU 核心必然越大。这也是为什么尽管它能够带来高响应，却不能用于客户端应用。\n其次，CMS 会面临浮动垃圾问题，也就是在并发标记阶段用户程序新产生的垃圾。由于这时还没有进行清除，且本次 GC 不会清除它们，如果用户产生的浮动垃圾很多，就会引发内存不足，需要一次 Full GC 才能解决。这个错误被报告为 Concurrent Mode Failure。因此，上面我们提到，在这时使用 Serial Old 作为后备预案进行老年代 GC，这个时间在服务器的大堆上就变得相当长了。为此，CMS 并不像其他收集器一样等到老年代几乎满了再收集，而是使用 -XX:CMSInitiatingOccupancyFraction 定义的比例来判断。如果这个比例太高，就会产生大量糟糕的 Concurrent Mode Failure。\n最后，CMS是一款标记 - 清除类型的收集器，这意味着它可能产生内存碎片。当大对象找不到位置来分配时，就会引发 Full GC。JVM 的设计者在这里提供了两个参数，-XX:+UseCMSCompactAtFullCollection 默认开启，指定在 CMS 导致的内存碎片带来 Full GC 时使用标记 - 整理算法，不过整理过程时无法并发的，性能较差。或者使用 -XX:CMSFullGCsBeforeCompaction 来指定每经过多少次 Full GC 就进行一次整理，默认为 0，即每次都整理。\nG1 # G1 的特点与核心思想 # G1（Garbage First）收集器在 JDK 7u4 才从实验状态移出，投入实用，这是一种面向服务端的收集器，同时包括新生代和老年代的管理能力。其主要特点包括：\n并行与并发 分代收集 空间整理：G1 整体来看基于标记 - 整理算法，局部来看则是复制算法。无论哪个，都不会造成内存碎片。 可预测停顿：除了减少停顿之外，G1 还追求可预测的停顿时间。允许指定一定时间内用于 GC 的最多时间。 首先，G1 将内存分为多个 Region。新生代和老年代不再是直接区分，而是分别由一系列 Region 组成。G1 跟踪各个 Region 垃圾堆积的价值大小，即回收后能够释放的空间大小与回收所需时间的评判。每次 GC 时，G1 根据所允许的 GC 停顿时间，选择回收价值最大的 Region。这样，就能够保证尽量高的收集效率和可控的收集时间。\n可以看到，G1 时采用了 \u0026ldquo;化整为零\u0026rdquo; 的方式进行 GC。不过，每个 Region 都不是独立的，在进行引用关系分析时必然要对整个堆进行扫描。为了解决这个问题，虚拟机为每个 Region 维护了一个 Remembered Set。每当引用关系发生变化时，虚拟机检查引用的对象是否位于不同的 Region，并记录到 Remembered Set 中去。对 Region 进行回收时，只要将 Remembered Set 加入到 GC Root 中即可。回顾上面说到的 GC Root 的概念，也就是说，被其他 Region 引用的对象不会被回收。在这种部分收集中，不收集部分也可以作为 GC Root，以避免全堆扫描。这里的 Remembered Set 起到的就是这个作用。也就是说，最终的结果来看，Remembered Set 中存储的是从其他 Region 指向这个 Region 的引用的信息。在进行可达性分析时，只需要从这里出发即可。\nG1 的运行 # 排除更新 Remembered Set 的过程，G1 GC 基本上可以分为四部分：\n初始标记 Initial Marking 并发标记 Concurrent Marking 最终标记 Final Marking 筛选回收 Live Data Counting \u0026amp; Evacuation 可以看到，这个流程与 CMS 有一定的相似性。首先，单线程地进行快速的初始标记，记录下与 GC Root 直接相连的对象，并修改 TAMS（Next Top at Mark Start）的值，以改变下一阶段并发进行时，用户程序分配新对象的 Region。然后，并发标记阶段回复程序运行，并进行可达性分析，找出存活的对象。\n与 CMS 类似，最终标记阶段也是用来处理并发标记过程中标记发生改变的部分对象。虚拟机将这段时间的变化记录在 Remembered Set Logs 中，最终标记阶段会检查这部分内容并将其合并到 Remembered Set 中去。这个阶段需要停顿线程，但可以并行执行。最后，筛选回收阶段根据优先级和用户定义的停顿时间制定回收计划。在目前的实现中，这个阶段也要停顿线程，以追求更快的回收。在筛选回收的阶段，G1 将 Region 中所有存活的对象拷贝到其他 Region，并将这个清空。这样，就能得到整理内存的效果。\nG1 同时提供了新生代和老年代的 GC。它不再严格区分新生代和老年代，而是以 Region 为单位管理内存。当部分收集也出现了内存问题时，就只能使用 Serial Old 的 Full GC。\n在下面的图示中，浅色箭头表示 CPU 正在执行用户线程，深色表示 JVM 的收集器线程。\n内存分配策略 # 下面介绍的内容基于 Serial / Serial Old 组合，目的是介绍最基本的一些内存分配原则。不同的收集器下，分配的规则会发生变化。在这个搭配下，Minor GC 仅针对于新生代，Major GC / Full GC 指老年代回收，通常在其之前伴随一次 Minor GC，但并不绝对。\n优先分配于 Eden\n大对象直接进入老年代（避免复制）\n长期存活的对象进入老年代：每经过一次 Minor GC，年龄加一。\n动态对象年龄判定：若 Survivor 中相同年龄对象超过了总 Survivor 空间的一半，这个年龄及更老的直接进入老年代。\n空间分配担保：若老年代的剩余空间大于新生代所有对象的总和，则可以确保安全。否则检查 HandlePromotionFailue。如果允许，则继续检查老年代最大的连续可用空间是否大于历次晋升数量的平均大小。如果是，则尝试一次有风险的 Minor GC。剩余的情况，都会改成一次 Full GC。\n这个做法略微复杂。总的来看，就是需要保证老年代能够承接 Survivor 装不下的对象。如果第一个条件不满足，我们就只能取一个平均值作为 Survivor 中存活的对象量的一个预测。因此这时的 Minor GC 是有风险的。极端情况就是，所有的新生代对象都存活。担保失败或选择不予担保的情况，就只能通过 Full GC 来解决。\n"},{"id":31,"href":"/distributed/hadoop-basic/","title":"Hadoop Basic Concepts","section":"Distributed Systems","content":" Hadoop 基本概念 # HDFS # 功能特性 # 向各种计算框架开放文件读写端口，适合大文件、一次写入多次读取的存储。\n仅支持单线程 append 写，不支持随机多线程访问。\n结构 # HDFS Client # 与 NameNode 交互，获取文件 Block 所在结点等信息\n切分文件为 Block\n与 DataNode 交互，实际传输文件\n其他管理 HDFS 的工作\nNameNode # 作为集群的 Master，管理名称空间\n管理 Block 的映射信息\n配置副本策略\n处理 Client 的读写请求\nNameNode 是整个 HDFS 最重要的部分，在 Hadoop 2.0 之后和 YARN 一起引入了 HA 架构，使得 Hadoop 可以用于在线应用。\nDataNode # 执行 NameNode 下达的操作\n存储数据\n执行读写\nSecondaryNameNode # 定期将 edits 和 fsimage 合并\n起到辅助恢复的作用，但并不是热备\nSecondaryNameNode 的作用 # SecondaryNameNode 能够保存 HDFS 的变化信息，在集群故障时辅助进行恢复工作。同时，也为 NameNode 分担了一小部分工作。其工作流程是基于变化的：\nNameNode 维护两部分信息：edits 和 fsimage。一切新的修改都执行在 edits 上。因此，访问文件也是优先访问 edits 部分。\n当到达一定时间或 edits 达到一定大小（默认为 3600s 和 64MB）时，将二者共同传送给 SecondaryNameNode。然后，建立一个新的 edits.new，存储接下来文件系统的变化。\nSecondaryNameNode 接收到 edits 和 fsimage 后，将二者合并，成为一个新的文件系统快照。这个快照在集群故障时就可作为恢复依据。将合并后的新的文件快照 fsimage.ckpt 发回给 NameNode。\nNameNode 收到合并后的 fsimage.ckpt 后，将其作为新的 fsimage 使用，而在这段时间内存储文件系统变化的 edits.new 既可作为新的 edits 使用。\n通过这样的流程，SecondaryNameNode 将合并文件系统变化的工作接了过来，NameNode 只需要负责及时响应客户端的变化即可。\nHDFS 的读写流程 # 在读取文件时，客户端获取到 DFS 实例后，DFS 实例从 NameNode 通过 RPC 获取到一批 block 的位置，但并不一定是全部 block。这些 block 按照其所在的 DataNode 距离客户端从近到远的顺序排序。Client 依次串行地读取每一个 block，直到全部读完，再从 NameNode 获取下一批 block 的位置。这样，避免了文件较大时读取队列过长的情况。\n在写入文件时，调用 DFS 实例的 create 方法，要求 NameNode 创建一个大小为 0 的文件。也就是说，HDFS 写入新文件的方式是先 touch 再 append。在创建时，NameNode 会检查是否有同名文件、目录权限等问题。\n随后，Client 的 DFSOutputStream 将数据切分成 block 进入 data queue 队列，并向 NameNode 请求 3 个最合适的 DataNode（取决于副本策略）。此外，DFSOutputStream 还包含一个 ack queue 队列，用于等待 DataNode 的写入完成回复。只有在收到了来自 DataNode 的确认信息之后才会将 block 从 data queue 中移除。写入全部完成后，关闭流，通知元数据节点。\nHDFS 的副本策略 # 决定副本策略的权衡因素包括集群的可靠性以及读写带宽。默认副本因子为 3，要求同机架（rack）两份，其他机架一份。在 Client 需要读取文件时，选择距离最近的结点进行读取。\n在 Hadoop 的安全模式下，会对所有的文件检查副本数量。\nMapReduce # 适用于离线处理的，高容错（在一个节点上失败将会移到其他节点）的分布式计算框架\n代码示例 # public class WordCount { public static class TokenizerMapper extends Mapper\u0026lt;Object, Text, Text, IntWritable\u0026gt; { private final static IntWritable one = new IntWritable(1); private Text word = new Text(); public void map(Object key, Text value, Context context ) throws IOException, InterruptedException { StringTokenizer itr = new StringTokenizer(value.toString()); while (itr.hasMoreTokens()) { word.set(itr.nextToken()); context.write(word, one); } } } public static class IntSumReducer extends Reducer\u0026lt;Text,IntWritable,Text,IntWritable\u0026gt; { private IntWritable result = new IntWritable(); public void reduce(Text key, Iterable\u0026lt;IntWritable\u0026gt; values, Context context ) throws IOException, InterruptedException { int sum = 0; for (IntWritable val : values) { sum += val.get(); } result.set(sum); context.write(key, result); } } public static void main(String[] args) throws Exception { Configuration conf = new Configuration(); Job job = Job.getInstance(conf, \u0026#34;word count\u0026#34;); job.setJarByClass(WordCount.class); job.setMapperClass(TokenizerMapper.class); job.setCombinerClass(IntSumReducer.class); job.setReducerClass(IntSumReducer.class); job.setOutputKeyClass(Text.class); job.setOutputValueClass(IntWritable.class); FileInputFormat.addInputPath(job, new Path(args[0])); FileOutputFormat.setOutputPath(job, new Path(args[1])); System.exit(job.waitForCompletion(true) ? 0 : 1); } } MR 内部逻辑 # Map 阶段 # Map 阶段由 InputFormat、Mapper 和 Partitioner 三部分构成。\n输入的每个 Split 交给一个 Mapper 进行处理。Split 与 HDFS 中 Block 的对应关系取决于 InputFormat 和压缩格式，通常为一对一。默认使用的是 TextInputFormat，其传递给 Mapper 的格式中，Key 为行的偏移量，Value 为行内容。\n经过用户编写的 Map 逻辑后，数据根据 Key 由 Partitioner 分组选择不同的 Reduce 节点。默认方法是进行 Hash 取模，模数即为 Reduce 节点的数量。因此，相同的 Key 将会被分配到同一个 Reduce 节点。\n在 Partitioner 之前，可以使用 Combiner 将数据进行合并。例如，在 wordcount 程序中，可以先使用 Combiner 将相同 Key 的数据计算出来。那么，Partitioner 输出的数据就将不再形如 \u0026lt;k1,1\u0026gt; \u0026lt;k1,1\u0026gt; \u0026lt;k2,1\u0026gt;，而是经过 Combiner 计算变成 \u0026lt;k1,2\u0026gt; \u0026lt;k2,1\u0026gt;。相当于进行了局部的 Reduce 操作。因此，这个参数在很多时候（比如上面的例子）可以直接使用 Reducer，即使不传入也不会对结果有影响。\nReduce 阶段 # Reduce 阶段由 Shuffle、Sort、Reducer、OutputFormat 四个部分构成。\n在 Shuffle 部分，数据从 Mapper 拷贝到 Reduce 节点，具体拷贝哪一个由前面的 Partitioner 决定。拷贝之后，在 Reduce 节点本地进行 Sort，将相同的 Key 排列到一起以供 Reduce 使用。\nReducer 将数据处理完成后，根据 OutputFormat 输出到磁盘。默认为 TextOutputFormat。\nShuffle 阶段的具体细节 # 在 Mapper 处理完成后，数据并不是直接进入磁盘，而是先储存在内存中的 buffer 中。buffer 是一个环形缓冲区，其大小由 io.sort.mb 属性控制，阈值由 io.sort.spill.percent 控制，默认为 100M 和 80%。缓冲区达到阈值后，将缓冲区内的内容进行 Partition，合并写入磁盘。\n这样，一次缓冲区满的 80 MB 数据将被按 key 排列写入，每一片最终进入一个 Reducer。\nHadoop 序列化 # Java 的序列化比较复杂，而我们在 Hadoop 中需要的序列化只要把必须的数据依次序列化即可，可以更加紧凑。在集群间进行通讯和 RPC 时，需要进行大量的序列化和反序列化操作。\nHadoop 为 Java 的八种基本类型准备了对应的 Writable 类。其中，Boolean 的实现使用了一个字节来存储。另一种常用的序列化类型是 Text。默认使用 UTF-8 编码。\nText text = new Text(\u0026#34;xxx\u0026#34;); text.set(\u0026#34;xxx\u0026#34;); text.set(\u0026#34;\u0026#34;.toByteArray()); 所有的序列化类型都要实现 WritableComparable 接口。\npublic interface Writable { void write(DataOutput out) throws IOException; void readFields(DataInput in) throws IOException; } public class IntWritable implements WritableComparable\u0026lt;IntWritable\u0026gt; { private int value; @Override public void readFields(DataInput in) throws IOException { value = in.readInt(); } @Override public void write(DataOutput out) throws IOException { out.writeInt(value); } @Override public int compareTo(IntWritable o) { int thisValue = this.value; int thatValue = o.value; return (thisValue\u0026lt;thatValue ? -1 : (thisValue==thatValue ? 0 : 1)); } } MR 的可插拔（pluggable）组件 # IO Format # IO Format Key Value 备注 TextInputFormat 字节偏移量 行内容 KeyvalueInputFormat 行前半 行后半 默认以制表符分割，可自定义 NLineInputFormat 字节偏移量 行内容 每个Mapper 固定收到 N 行 SequenceInputFormat 自定义 自定义 读取 Hadoop Sequence 文件 TextOutputFormat key value 输出结果类似 KeyValueInputFormat 输入格式 SequenceFileOutputFormat 多输入 / 输出 # MultipleInputs.addInputPath( job, aInputPath, TextInputFormat.class, ADataMapper.class ); MultipleInputs.addInputPath( job, bInputPath, TextInputFormat.class, BDataMapper.class ); MultipleOutputs.addNamedOutput( job, \u0026#34;text\u0026#34;, TextOutputFormat.class, LongWritable.class, Text.class ); Combiner # Combiner 可以减少 Map Task 的磁盘 IO 与 Shuffle 的网络 IO。通常可以和 Reducer 使用同一个类。\njob.setCombinerClass(IntSumReducer.class); Partitioner # Partitioner 的作用是为相同的 key 分配相同的 Reducer。默认的实现是 hash(key) mod R，R 为 Reducer 的数量。自定义 Partitioner，需要继承 Partitioner 类并重写 getPartition 方法：\nclass MyPartitioner\u0026lt;K, V\u0026gt; extends Partitioner\u0026lt;K, V\u0026gt; { @Override public int getPartition(K key, V value, int numPartitions) { return key.hashCode() % numPartitions; } } 需要注意的是，如果只有一个 Reduce Task，MR 会直接进行分配，不会执行我们自定义的 Partitioner。可以使用 job.setNumReduceTasks(number); 设置。\n考虑一个全局排序的场景。默认的 HashPartitioner 不能保证 Reducer 之间的有序性，因此我们需要自定义 Partitioner。Hadoop 库中的 TotalOrderPartitioner 使用了前缀树来处理这个问题。\nYARN # YARN 将原本 JobTracker 的任务划分为 ResourceManager 和 ApplicationManager 两部分，再统一到 YARN 中，将资源分配和任务调配统一起来。MapReduce 任务将直接部署到 YARN 上，资源的表示方法也从 slot 变成了直接的 RAM 和 CPU。\nYARN 的组成 # ResourceManager 处理请求，启动 AM，监控 NM 与 AM，进行资源调度。\nNodeManager 管理单个结点的资源，处理 RM 和 AM 发来的命令。\nApplicationMaster 负责单个任务的监控、调度以及向 RM 申请资源。\nContainer 封装运算资源\n总的来看，用户将任务提交给 ResourceManager，RM 为每个具体的应用程序在某个结点上启动一个 ApplicationMaster。这时 RM 只分配了 AM 所需的资源。AM 启动后，再根据任务的情况向 RM 请求任务执行所需的资源，并与 NodeManager 通信进行计算。计算完成后，AM 向 RM 注销，表示完成任务。\nContainer 的作用出现在 AM 向 RM 需求分配资源时。AM 向 RM 发送的数据包括优先级、需要的 RAM、CPU 以及是否接受跨机架等信息。当然，YARN 默认会尽量在同一个机架上完成任务。然后，RM 向 AM 返回一个 Container指示了分配到的资源所在位置等信息。也就是说，一个 Container 封装了一个节点上一定量计算资源的信息。然后，AM 将计算任务与 Container 的信息一同发送给 NodeManager 进行计算。\nYARN 的容错 # 在整个任务过程中，AM 和 NM 都要不停地发送心跳信息。如果 AM 出现问题，RM 可以直接启动另一个 AM。如果 NM 出现问题或者任务失败频率较高，则AM 可以将任务转移到其他结点。\n"},{"id":32,"href":"/notes/in-depth-jvm/","title":"In-depth Understanding JVM","section":"Notes on Books","content":" 深入理解 Java 虚拟机 # Garbage Collection Java Synchronization JVM Memory Model JVM Memory Regions ThreadLocal and Reference "},{"id":33,"href":"/notes/intro-algo/","title":"Introduction to Algorithms","section":"Notes on Books","content":" Introduction to Algorithms # 1. Compexity, Divide 2. Sorting, Order Statistic 3. LinkedList, HashTable 4. BST, Balanced BSTs 5. Trie-Tree, Extending Data Structures 6. Dynamic Programming, Greedy, Amortize 7. B-Tree, Fibonacci Heap, vEB Tree 8. Graphs "},{"id":34,"href":"/pl/java-nio-2/","title":"Java NIO Internal","section":"Programming Languages","content":" Java NIO: 原理 # Buffer 内部实现与包装 # 更多形式的 get 与 put # 也存在一些其他形式的 get() 和 put() 方法。\nbyte get(); ByteBuffer get(byte dst[]); ByteBuffer get(byte dst[], int offset, int length); byte get(int index); ByteBuffer put(byte b ); ByteBuffer put(byte src[]); ByteBuffer put(byte src[], int offset, int length); ByteBuffer put(ByteBuffer src); ByteBuffer put(int index, byte b); 可以认为，这里接收 index 参数的方法是\u0026quot;绝对的\u0026quot;，它们直接对 buffer 中某个位置进行操作，而不受 buffer 目前状态的影响，绕过了下面我们将提到的 buffer 的统计方法。\nBuffer 的统计方式 # Buffer 的行为相当于一个简单的数组，它有三个重要的属性：\ncapacity 数组的大小。\nlimit 在写入时，就是 capacity。在读取时，就是最多能够读取的字节数。\nposition 当前的指针位置。\n当 position 到达 limit 时，继续 get 会抛出异常。这时，可以使用 rewind() 将 position 重新指回开头，使用 clear() 将 limit 和 position 一起重置，或者使用 compact() 保留未读的数据。\nbuf // HeapByteBuffer[pos=4 lim=4 cap=8] buf.rewind() // HeapByteBuffer[pos=0 lim=4 cap=8] buf.clear() // HeapByteBuffer[pos=0 lim=8 cap=8] // some operation buf // HeapByteBuffer[pos=1 lim=2 cap=8] buf.compact() // HeapByteBuffer[pos=1 lim=8 cap=8] 可以看到，clear() 同时也将 buffer 设置回了写入模式。\n如果我们并不想直接回到 buffer 的最开始位置，也可以通过 mark() 和 reset() 来手动标记。\nbuf.mark() // HeapByteBuffer[pos=1 lim=4 cap=8] buf.get() buf // HeapByteBuffer[pos=2 lim=4 cap=8] buf.reset() // HeapByteBuffer[pos=1 lim=4 cap=8] 需要注意的是，buffer 的读取模式和写入模式并没有严格的区分，而是通过 position 与 limit 的相对位置来控制。例如，在读取模式下调用 get() 将会同样返回当前位置的值（这个值是上一次读写遗留下来的），并将 position 加一。应当避免这类情况。\n总的来看，写入模式下，limit 与 capacity 相等，pos 指向要写入的位置，pos 以下的是\u0026quot;有效数据\u0026quot;。读取模式下，limit 为有效数据的范围，pos 为当前读取的位置。limit 以下的是\u0026quot;有效数据\u0026quot;。在 buffer 的 equals() 方法中，只比较有效数据。当然，如果有效数据的长度不相等，equals() 一定为假。\n类型化、包装 # byte[] array = new byte[8] // { 0, 0, 0, 0, 0, 0, 0, 0 } ByteBuffer buffer = ByteBuffer.wrap(array) // HeapByteBuffer[pos=0 lim=8 cap=8] buffer.putInt(2147004567) // HeapByteBuffer[pos=4 lim=8 cap=8] array // { 127, -8, -80, -105, 0, 0, 0, 0 } buffer.flip() // HeapByteBuffer[pos=0 lim=4 cap=8] buffer.getInt() // 2147004567 在这里我们做了两件事。首先，我们将一个数组包装成了 buffer。并且可以看到，对 buffer 的操作可以立刻体现在数组里。其次，我们使用了 ByteBuffer 的类型化 getter 和 setter。这类方法可以将 long、double 等类型的变量存入 ByteBuffer 并原样读取出来，对复杂数据的操作，例如 Unicode 文本和图片都有利。\n除了对数组进行包装，也可以将一个 buffer 的一部分包装成另一个 buffer。与上面的情况类似，这两个 buffer 共享同一个底层数组。通过这种方法，可以更好地分离功能，让我们处理数据的代码的耦合度得到降低。\nbuffer.putLong(465132465415L) // HeapByteBuffer[pos=8 lim=8 cap=8] buffer.position(3) // HeapByteBuffer[pos=3 lim=8 cap=8] buffer.limit(7) // HeapByteBuffer[pos=3 lim=7 cap=8] ByteBuffer another = buffer.slice() // HeapByteBuffer[pos=0 lim=4 cap=4] 只读、直接缓冲区、内存映射 # Java NIO 还提供了只读缓冲区包装和速度更快的直接缓冲区。对于直接缓冲区，JVM 会尽量从系统 IO 调用中直接读取或写入系统调用，避免中间更多的缓冲区开销。\nbuffer.asReadOnlyBuffer() // HeapByteBufferR[pos=3 lim=7 cap=8] ByteBuffer.allocateDirect(8) // DirectByteBuffer[pos=0 lim=8 cap=8] 最后，Java 还提供了将文件直接映射到内存里的方式。\nMappedByteBuffer mbb = fc.map(FileChannel.MapMode.READ_WRITE, start, size); mbb.put(0, (byte)97); mbb.put(1023, (byte)122); NIO 中的各种 Channel # FileChannel # 前面我们已经见到 FileChannel 的使用，它是一种只有阻塞模式的 Channel。FileChannel 操作的是随机读写的文件。因此，还有一些相应的属性和方法。truncate 代表从当前位置截取一定长度的文件，force 代表强制将系统缓存写入到磁盘。接收的参数代表是否写入元数据。\nlong pos = channel.position(); channel.position(pos + 123); long fileSize = channel.size(); channel.truncate(1024); channel.force(true); 出于网络数据与本地文件之间的转换非常常见，NIO 提供了一些便利的转换方法：\nRandomAccessFile toFile = new RandomAccessFile(\u0026#34;toFile.txt\u0026#34;, \u0026#34;rw\u0026#34;); FileChannel toChannel = toFile.getChannel(); long position = 0; long count = fromChannel.size(); toChannel.transferFrom(fromChannel, position, count); count 只能代表传输量的最大值，如果通道的数据量小于 count，就只传递通道内实际有的数据。类似地，也可以使用 FileChannel.transferTo(position, count, channel) 来将文件内容传输到其他位置。\nAsynchronousFileChannel 异步文件通道 # 异步文件通道的读写返回的是一系列的 Future 包装的对象，并且不会阻塞，可以立即返回。其创建和其他的通道类似：\nAsynchromousFileChannel fileChannel = AsynchronousFileChannel.open( Paths.get(\u0026#34;data/test.xml\u0026#34;), StandardOpenOption.READ); Future\u0026lt;Integer\u0026gt; operation = fileChannel.read(buffer, 0); while(!operation.isDone()); 这里 read() 的第二个参数是读取文件的起始位置。然后，我们可以通过 isDone() 来判断读取的完成。\nfileChannel.read(buffer, position, attachment, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer result, Attatchment attachment) { ...; } @Override public void failed(Throwable exc, Attachment attachment) { ...; } }); 这里的第三个参数是 IO 操作的 Attachment，和前面 SelectionKey 的 Attachment 类似，可以为 null。然后，我们继承一个 CompletionHandler , 其第二个泛型类型变量是 Attachment 的类型。如果 read 成功，随即就会调用 completed 方法。通常，对于一般的网络应用，我们可以把同一个 buffer 作为 Attachment 传入，并在 completed 方法中使用 buffer 回应。相应地，如果读取失败，会将对应的 Throwable 传到 failed 方法。\n写数据的方法和读取相似：\nAsynchromousFileChannel fileChannel = AsynchronousFileChannel.open( Paths.get(\u0026#34;data/test.xml\u0026#34;), StandardOpenOption.WRITE); // possible throws java.nio.file.NoSuchFileException Future\u0026lt;Integer\u0026gt; operation = fileChannel.write(buffer, position); while(!operation.isDone()); fileChannel.write(buffer, position, buffer, new CompletionHandler\u0026lt;Integer, ByteBuffer\u0026gt;() { @Override public void completed(Integer result, ByteBuffer attachment) { ...; } @Override public void failed(Throwable exc, ByteBuffer attachment) { System.out.println(\u0026#34;Write failed\u0026#34;); exc.printStackTrace(); } }); SocketChannel # 前面见到了服务端开启 SocketChannel 的方法，并将其和传统方式比较了一下。客户端的情况还要更简单一些：\nSocketChannel socketChannel = SocketChannel.open(); socketChannel.connect(new InetSocketAddress(\u0026#34;http://jenkov.com\u0026#34;, 80)); 阻塞模式下的操作我们已经比较熟悉，和传统 IO 基本相同。在非阻塞模式下，就要使用更多的方法来检查 Channel 的情况：\nsocketChannel.configureBlocking(false); socketChannel.connect(new InetSocketAddress(\u0026#34;http://jenkov.com\u0026#34;, 80)); while(!socketChannel.finishConnect()){ // wait } 类似地，在非阻塞模式下的 read 方法总是能被调用，同时也意味着它有可能并不读出任何数据。因此，必须要检查方法的返回值，即读入的字节数。非阻塞模式和 Selector 的运行方式也更加吻合。\nServerSocketChannel # 阻塞模式的运行方式我们同样已经比较熟悉。在非阻塞模式下，调用 accept() 方法可能会返回 null，代表没有连接建立。\nDataGramChannel # UDP 本身就是无连接的，因此数据包通道的操作和传统 IO 没有太大的区别：\nDatagramChannel channel = DatagramChannel.open(); channel.socket().bind(new InetSocketAddress(9999)); channel.receive(buf); int byteSent = channel.send(buf, new InetSocketAddress(\u0026#34;example.com\u0026#34;, 80)); // connecting to a specific address channel.connect(new InetSocketAddress(\u0026#34;example.com\u0026#34;), 80)); 如果在 recieve 时实际数据量超过了 buffer 的大小，多出的数据会被直接抛弃。\n在调用了 connect 方法之后，就可以像 TCP 一样调用 read 和 write 方法。只不过，背后实际调用的仍然是 TCP 模式，数据的读写也无法得到保证。\nPipe 管道 # 管道能够提供将一个通道的数据发送到另一个的功能。也可以通过方法将数据取出来。这是一个抽象类，具体的实现需要继承编写。\nPipe pipe = Pipe.open(); Pipe.SinkChannel sinkChannel = pipe.sink(); Pipe.SourceChannel sourceChannel = pipe.source(); 提供其他相关功能的类 # java.nio.file.Path # Path 是在 Java 7 的 NIO 2 中引入的，其伴随类 Paths 同样位于 java.nio.file 包。这个类和原来的 java.io.File 类的功能类似，在 NIO 中，专用来处理路径。而关于具体文件的操作大多交给 Files，以 Path 作为其参数。\nPath path = Paths.get(\u0026#34;/home/user/a.txt\u0026#34;); // use base path and relativepath to create absolute path Path file = Paths.get(\u0026#34;/home/user\u0026#34;, \u0026#34;a.txt\u0026#34;); Paths.get(\u0026#34;/home/./../opt\u0026#34;).normalize(); // equals /opt 如果将 UNIX 绝对路径用在 Windows 上，得到的将会是相对于当前磁盘的路径。\njava.nio.file.Files # Files 用来和 Path 类共同操作文件，将其和传统的 java.io.File 类区分开。\n对文件的操作 # 在操作文件时，需要注意，并不支持递归创建。也就是说，在创建一个文件之前，必须首先创建其父文件夹。\n// check if exist if (!Files.exists(path)) { Files.createFile(path); } if (!Files.exist(path, new LinkOption[]{LinkOption.NOFOLLOW_LINKS})) { ...; } // create, copy, throws FileAlreadyExistsException, IOException Path newdir = Files.createDirectory(path); Files.copy(sourcePath, destinationPath); Files.copy(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); // move, delete, throws IOException Files.move(sourcePath, destinationPath, StandardCopyOption.REPLACE_EXISTING); Files.delete(path); FileVisitor 遍历目录结构 # walkFileTree() 方法可以查看目录结构。这个方法需要接受一个自己实现的 FileVisitor 对象。Java 只提供了一个全空实现的 SimpleFileVisitor。\nenum FileVisitResult { CONTINUE, TERMINATE, SKIP_SIBLINGS, // dont visit brother files; SKIP_SUBTREE; // dont visit subdir and subfile // only effective in preVisitDirectory, // or continue; } Files.walkFileTree(path, new FileVisitor\u0026lt;Path\u0026gt;() { @Override public FileVisitResult preVisitDirectory(Path dir, BasicFileAttributes attrs) throws IOException { //...; return FileVisitResult.CONTINUE; } @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { //...; return FileVisitResult.CONTINUE; } @Override public FileVisitResult visitFileFailed(Path file, IOException exc) throws IOException { //...; return FileVisitResult.CONTINUE; } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException { //...; return FileVisitResult.CONTINUE; } }); FileVisitor 的这种定义是为了解决递归问题。例如，我们要删除一个目录，必须先删除其所有的子目录和文件。我们不继承 FileVisitor，而是继承 SimpleFileVisitor，以继承类内的空方法。\nFiles.walkFileTree(rootPath, new SimpleFileVisitor\u0026lt;Path\u0026gt;() { @Override public FileVisitResult visitFile(Path file, BasicFileAttributes attrs) throws IOException { System.out.println(\u0026#34;delete file: \u0026#34; + file.toString()); Files.delete(file); return FileVisitResult.CONTINUE; } @Override public FileVisitResult postVisitDirectory(Path dir, IOException exc) throws IOException { Files.delete(dir); System.out.println(\u0026#34;delete dir: \u0026#34; + dir.toString()); return FileVisitResult.CONTINUE; } }); Files 与 Stream # 在 Java 8 中，Files 增加了新的便于逐行处理文本文件的方法。\npublic static Stream\u0026lt;String\u0026gt; lines(Path path,Charset cs) throws IOException {} public static Stream\u0026lt;Path\u0026gt; walk(Path start, int maxDepth, FileVisitOption... options) throws IOException {} public static Stream\u0026lt;Path\u0026gt; find(Path start, int maxDepth, BiPredicate\u0026lt;PathBasicFileAttributes\u0026gt; matcher, FileVisitOption... options) throws IOException {} 在 BufferedReader 中也有类似的方法。\n文件锁 # Linux 的文件锁可以从两个维度来区分：共享锁-排他锁，和劝告锁-强制锁。通常，读锁是共享的，写锁是排他的。Java 在这里使用的是劝告锁，这意味着内核并不真正禁止对文件的访问，而只是为应用程序提供一个已经上锁的状态指示。\n// open with write mode, gets exclusive lock RandomAccessFile raf = new RandomAccessFile(\u0026#34;usefilelocks.txt\u0026#34;, \u0026#34;rw\u0026#34;); FileChannel fc = raf.getChannel(); FileLock lock = fc.lock(start, end, false); lock.release(); 考虑到不同的 OS 可能提供不同种类的锁，为了保证安全，最好的办法是将所有锁都视为排他、劝告式的。\n"},{"id":35,"href":"/pl/java-nio-1/","title":"Java NIO Usage","section":"Programming Languages","content":" Java NIO: 应用 # 基本概念 # Java NIO 是一种非阻塞的、面向块而非字节的 IO 方式。虽然 Java 的传统 IO 也进行了一些基于 NIO 的改造，NIO 仍然能够带来许多优势。\n面向流的 IO 方便我们一个字节一个字节地处理数据，有利于实现过滤等功能，更加优雅和简单。相应地，其速度通常比较慢。\nJava NIO 的模型由三部分组成。\nChannel 通道，类似于传统 IO 中的流，用来实际传输数据。 Buffer 缓冲，我们用来读取和发送数据的位置。 Selector 选择器，可以在一个线程上绑定多个 Channel 和对应的 Buffer。 Channel # Channel 和流非常相似。区别是，通道支持异步读写，支持双向读写，而且基于缓冲区。相比之下，流通常是单向同步读写的。\n常用的 Channel 主要包括：\nFileChannel 文件 DatagramChannel UDP数据报 SocketChannel TCP 套接字 ServerSocketChannel TCP 服务端套接字 Buffer # Java 中的各种基本类型都有其对应的 Buffer，最常用的是 ByteBuffer。可以通过 Channel 或者手动写入数据。\nByteBuffer buffer = ByteBuffer.allocate(1024); // through a channel int bytesRead = inChannel.read(buf); // manually input buf.put((byte)127); 然后，要从 buffer 中读取数据，需要首先 flip() 它，变成读取模式。\nbuf.flip(); // write from buffer into channel. int bytesWritten = inChannel.write(buf); // manually get byte aByte = buf.get(); 需要注意的是，很多 Channel ,如 FileChannel 和非阻塞模式下的 SocketChannel 的 write 方法并不能保证将 buffer 全部写入文件。因此，要使用循环来处理：\nwhile(buf.hasRemaining()) { channel.write(buf); } 下面是一个简单的输出文件内容的示例：\nRandomAccessFile aFile = new RandomAccessFile(\u0026#34;data/nio-data.txt\u0026#34;, \u0026#34;rw\u0026#34;); FileChannel inChannel = aFile.getChannel(); ByteBuffer buf = ByteBuffer.allocate(48); int bytesRead = inChannel.read(buf); while (bytesRead != -1) { buf.flip(); while(buf.hasRemaining()){ System.out.print((char) buf.get()); } buf.clear(); bytesRead = inChannel.read(buf); } aFile.close(); InputStream 和 OutputStream 类也有类似的 getChannel 方法。当然，这样开启的通道只能是单向的。下面是一个将输入内容传输到输出的可复用的代码片段：\nwhile (true) { buffer.clear(); int r = fcin.read( buffer ); if (r==-1) { break; } buffer.flip(); fcout.write( buffer ); } Scatter / Gather # Scatter 和 Gather 可以译为分散和聚集，指的是向同一个通道写入和读出多个 Buffer 的过程。在处理复杂结构的数据，如 Header-Content 时，有利于代码整洁。Scatter Read 指从一个 Channel 读取到多个 Buffer，Gather Write 指从多个 Buffer 写入到一个 Channel。关于网络的内容还会在后面进一步解释。\nServerSocketChannel ssc = ServerSocketChannel.open(); ssc.socket().bind( address ); SocketChannel sc = ssc.accept(); int messageLength = firstHeaderLength + secondHeaderLength + bodyLength; ByteBuffer buffers[] = new ByteBuffer[3]; buffers[0] = ByteBuffer.allocate( firstHeaderLength ); buffers[1] = ByteBuffer.allocate( secondHeaderLength ); buffers[2] = ByteBuffer.allocate( bodyLength ); int bytesRead = 0; while (bytesRead \u0026lt; messageLength) { long r = sc.read( buffers ); bytesRead += r; } // flip all buffers long bytesWritten = 0; while (bytesWritten\u0026lt;messageLength) { long r = sc.write( buffers ); bytesWritten += r; } 网络和异步 IO # 异步 IO 的模式实际上来自于操作系统，如 Linux 的 IO 复用和 Windows 的 IOCP。因此，类似的编程模式很可能也适用于其他语言。\nTCP 异步 IO 的例子 # 异步 IO 不会阻塞，这也使得它可以处理许多的 IO 连接。在传统 IO 下，这通常需要通过轮询或多线程来实现。\n首先回顾一下普通的 IO 编程中处理 TCP 连接的方法：ServerSocket 类监听端口，客户端的 Socket 类构造时发出连接请求。这时，ServerSocket.accept() 从阻塞中脱离，返回服务端的 Socket 对象。\n然后，我们来看异步处理的方法：\nint[] ports = ...; Selector selector = Selector.open(); for (int i : ports) { // new channel ServerSockertChannel channel = ServerSocketChannel.open(); // config channel.configureBlocking(false); channel.socket().bind(new InetSocketAddress(i)); // register to selector SelectionKey key = channel.register(selector, SelectionKey.OP_ACCEPT); } 这里的 SelectionKey.OP_ACCEPT 是适用于 ServerSocketChannel 的唯一事件，即 TCP 连接建立的事件。\nint num = selector.select(); Set\u0026lt;SelectionKey\u0026gt; keys = selector.selectedKeys() select() 方法会阻塞直到有任何一个连接建立。selectedKeys 返回一个 Set\u0026lt;SelectionKey\u0026gt; 对象。在异步 IO 的类似处理过程中，由于我们已经通过 select 得到了这个连接信息，就不必再担心 accept 会阻塞：\nfor (SelectKey key : keys) { if ((key.readyOps() \u0026amp; SelectionKey.OP_ACCEPT) == SelectionKey.OP_ACCEPT){ // Accept the new connection // accept() the new socket and register to selector ServerSocketChannel channel = (ServerSocketChannel)key.channel(); SocketChannel sc = ssc.accept(); sc.configureBlocking( false ); SelectionKey newKey = sc.register(selector, SelectionKey.OP_READ); } else if ((key.readyOps() \u0026amp; SelectionKey.OP_READ) == SelectionKey.OP_READ) { // Read the data from socket SocketChannel sc = (SocketChannel)key.channel(); // use buffer to proccess } keys.remove(key); } 可以看到，我们将 accept 得到的结果重新放回了 selector 的监听列表，并且将监听事件修改为了 SelectionKey.OP_READ，即有数据到达的事件。这个过程和传统 IO 中从 ServerSocket.accept() 获得 Socket 的过程类似。\n然后，在 if 语句的另一个分支，我们来处理接收数据的过程。使用 channel() 方法得到双向读写的通道对象。随后，我们就可以使用之前熟悉的 buffer 来处理这个连接了。\n最后，我们需要把处理结束的连接从 keys 中删除，以免重复处理。在实际的应用场景中，我们还需要把关闭的连接从 Selector 中去除，并且很有可能使用多线程。\nSelectionKey # 上面我们见到了 OP_READ 和 OP_ACCEPT。除此之外，NIO 还有 OP_WRITE 和 OP_CONNECT 两种事件。可以认为每个事件代表\u0026quot;就绪\u0026quot;：例如当连接另一方传来数据时，连接处于\u0026quot;读就绪\u0026quot;状态，对应事件 OP_READ。因此，写就绪和连接就绪这两种事件并不常用。\n四种事件的值分别为 1、2、8、16，因此可以使用位操作来处理这些事件。例如：\nint intreastSet = OP_READ | OP_ACCEPT; if ((interest \u0026amp; OP_READ) == OP_READ) {} 相应地，SelecttionKey 也提供了一些处理这些信息的方法。\nboolean isAcceptable(); int interestOps(); // all interest ops int readyOps(); // ops that already triggered 还可以为每个 SectionKey 附加一个对象，以方便识别类似的对象。\nselectionKey.attach(theObject); // or SelectionKey key = channel.register(selector, SelectionKey.OP_READ, theObject); Object attachedObj = selectionKey.attachment(); Selector # 除了 select() 方法外，Selector 类同样提供了带有超时的阻塞方法和非阻塞，允许返回 0 的方法。\nint select(int timeout); int selectNow(); 如果在阻塞期间调用 Selector 的 wakeUp() 方法（当然，是在另一个线程里），线程会立刻放弃阻塞。在操作结束之后，需要使用 Selector.close() 方法。这将会使所有的 SelectionKey 都无效，但并不会关闭 Channel。\n异步 IO 设计 # 概述 # 阻塞 IO 和异步 IO 的区别是显而易见的。阻塞 IO 是一种成功的设计，它能够保证 IO 的可靠和简单。但在这种模式下，每个 IO 都需要单独的一个线程来处理。在 JVM 的默认参数下，32 bit 系统的一个栈为 320kB，64 bit 下更达到了 1MB，在高并发情况下这是完全无法接受的。线程池是解决这个问题的一种途径，但当我们面临大量低速长链接的时候，问题仍然没有被彻底解决。而这正是大规模互联网应用的常态。因此，异步 IO 成为了必然的选择。异步 IO 的最典型特征是，每一次检查不再是阻塞或获得整块数据，而是0或数据。这虽然解决了多线程的问题，却带来了另外一些需要解决的问题。\n异步 IO 首先要解决的问题是，怎样用一个线程处理许多连接。于是，我们有了 Selector，使用一个 Selector 来处理许多连接，以实现\u0026quot;阻塞直到有一个\u0026quot;的效果，而不需要去处理那些尚未读到数据的连接。于是，线程资源被充分地利用起来。\n第二个问题是，由于所有的操作都被立即返回，异步 IO 下读到的数据不总是完整的。甚至，在连续传输的情况下，几乎总是不完整的。于是，我们需要做两件事：\n判断当前的数据是否是完整的\n将不完整的数据暂存起来，以备下一次传输时拼合起来。\n于是，我们在 Selector 与 Channel 之间加入一个 Message Reader，用来处理这些工作。在工程化的实践中，我们可能希望这套系统能够处理各种不同的协议。因此，可能会接收一个 Message Reader 的工厂作为参数，以进行依赖注入。\nMessage Reader 的实现 # 前面我们看到，Message Reader 需要能够在内部的一个 Buffer 中存储不完整的 Message。显而易见，这个 buffer 的大小应该等于消息的最大值。但这时我们又遇到了之前说的内存不足的问题：百万级别的 1 MB buffer 意味着 1 TB 的 RAM 空间。因此，我们需要在这里使用可伸缩的（flexible）buffer。\n拷贝扩容 # 一种常见的方法是熟悉的拷贝扩容，也就是当 buffer 已满后将所有内容复制到一个更大的数组中去。在这种方式下，threshold 的 选取就是一个重要的问题。例如，假设一个系统的请求消息不大于 4 kB，传输的文件通常不大于 128 kB，更大的文件则没有规律性。那么，我们就会将 threshold 设置为 4kB 和 128 kB，将最终的内存占用控制在 GB 级别。\n追加扩容 # 另一种常见的方式是追加（append）扩容，方法是用一个列表将所有小的 buffer 片段集合起来，或者将一个大的数组分片，再用列表来管理分片。后者在内存模型上会更有利一些，但需要对并发量的准确判断。追加扩容的缺点也很明显，维护和读取都比较复杂。\n使用 TLV 消息 # 许多协议，包括 HTTP/2 在内，开始使用 TLV 格式的消息。TLV 指的是 Type-Length-Value 的元组。对于这类消息，我们可以在一开始就知道消息的长度，并为其开辟好内存空间，避免了上面的方式中对内存资源的浪费。\n当然，TLV 格式也有其缺点。对于很长的 TLV 消息，我们就需要很大的内存空间的预开辟，这也为 DoS 攻击提供了空间。一种解决方案是使用分段 TLV 的消息格式，但这并不能彻底解决问题。另一种方式是为消息设置超时时间。这样，服务器至少能够在一段时间的无响应后恢复。\n写不完整的消息 # 前面已经提到，非阻塞模式的通道并不能对一次 write() 实际写入的数据量做出保证，而是将写入的数据的字节数返回给调用者。于是，为了进一步解耦和提高效率，我们还需要在数据处理者和 Channel 同样准备一个 Message Writer，用来处理这个不稳定的输出过程。\n回过头来想，我们在这里并不想为每个连接都维护一个线程。因此，我们只希望对有消息可写的 Writer 进行处理。因此，我们使用这样一个过程：\n当 Message Writer 有消息可写时，才将其对应的 Channel 注册到 Selector。然后，服务器在空闲时检查 Selector 来获取可写的 Channel，并寻找其对应的 Writer 以写入数据。在 Writer 已经没有数据可写时，将 Channel 从 Selector 上解绑。\n集成 # 现在我们已经理清了输入和输出两个部分，现在我们从整个服务器的角度来思考。总的来说，一个服务器会执行这样一个循环：\n从 ServerSocket 中获取 Socket =\u0026gt; Select 读事件 =\u0026gt; 将接受的数据交给 Reader 来处理 =\u0026gt; 在核心部分处理 Reader 传来的完整数据 =\u0026gt; 将处理后的数据交给 Writer =\u0026gt; Select 写事件\n当然，这些功能还可以在多个线程内完成。\n"},{"id":36,"href":"/notes/in-depth-jvm/synchronization/","title":"Java Synchronization","section":"In-depth Understanding JVM","content":" Java 中的线程同步 # Java 中的 synchronized 关键字 # 对函数使用 synchronized 关键字时，如果是静态函数，相当于 synchronized(this.class) 代码块。反之，则相当于 synchronized(this) 代码块。\nsynchronized 代码块只能锁住对象。最经济的锁对象是 byte[0]，只需要三条字节码，比 new Object() 更少。锁住一个 static 变量，实际上相当于锁住一个类。锁住的对象必须是 private 的。否则，引用所指向的对象将能够被外部修改，破坏同步。\npublic class Synchronize { private String name; private static final byte[] sb = new byte[0]; private final byte[] b = new byte[0]; private Synchronize(String name) { this.name = name; } public static void print(String s) { System.out.println(\u0026#34;start\u0026#34; + s); try { Thread.sleep(1000); } catch (InterruptedException e) { } System.out.println(\u0026#34;stop\u0026#34; + s); } public void syncStatic() {//锁住一个静态变量 synchronized (sb) { print(name); } } public void syncMember() {//锁住一个实例变量 synchronized (b) { print(name); } } public static synchronized void syncClass() {//锁住整个类 print(\u0026#34;static\u0026#34;); } public synchronized void syncObject() {//锁住对象 print(name); } } 在字节码的实现中，方法的同步和代码块的同步采用了不同的两种方式：\npublic void syncMember(); descriptor: ()V flags: (0x0001) ACC_PUBLIC Code: stack=2, locals=3, args_size=1 5: astore_1 6: monitorenter //!!!获得对象b的monitor锁 7: aload_0 14: aload_1 15: monitorexit //!!!释放对象b的monitor锁 16: goto 24 public static synchronized void syncClass(); descriptor: ()V flags: (0x0029) ACC_PUBLIC, ACC_STATIC, ACC_SYNCHRONIZED //!!!常量池中增加了ACC__SYNCHRONIZED常量，同步工作由JVM完成 Code: stack=1, locals=0, args_size=0 0: ldc #6 // String static 2: invokestatic #5 // Method print:(Ljava/lang/String;)V 5: return LineNumberTable: line 21: 0 line 22: 5 轻量级锁和偏向锁 # synchronized 关键字实现的锁通过对象的 monitor 锁调用操作系统的 mutex 锁，执行过程中需要在内核态与用户态之间切换，因此这种方式的开销比较大，被称为重量级锁。JDK1.6 之后，引入了轻量级锁和偏向锁。\n锁可以从低级向高级膨胀，不能反向转化。轻量级锁适用于两个线程交替执行而不冲突的情况，此时轻量级锁可以节省性能损耗。当两个线程同时访问锁（两个线程\u0026quot;相撞\u0026quot;时），轻量级锁膨胀为重量级锁，以处理同步过程。此后锁不能再变回来。\n偏向锁适合只有一个线程正在运行的情况，可以进一步减少同步成本的消耗。偏向锁认为只有自己一个线程正在执行，因此直到竞争出现或者执行完成才释放锁。偏向锁可以膨胀为轻量级锁。\n这种处理锁的方式方式被称为乐观锁：发生碰撞后再进行处理。相应的，重量级锁属于悲观锁：无论如何，都要进行同步处理。\n轻量级锁 # 加锁 # 在代码进入同步块的时候，如果同步对象锁状态为无锁状态（锁标志位为\u0026quot;01\u0026quot;状态，是否为偏向锁为\u0026quot;0\u0026quot;），虚拟机首先将在当前线程的栈帧中建立一个名为锁记录（Lock Record）的空间。\n拷贝对象头中的 Mark Word 复制到锁记录中。\n拷贝成功后，虚拟机将使用 CAS 操作（Compare And Swap）尝试将对象的 Mark Word 更新为指向 Lock Record 的指针，并将 Lock record 里的 owner 指针指向对象 Mark Word。\n如果这个更新动作成功了，那么这个线程就拥有了该对象的锁，并且对象Mark Word的锁标志位设置为\u0026quot;00\u0026quot;，即表示此对象处于轻量级锁定状态。\n即：此时栈中有一个 Lock Record 段，其由两个部分组成：\nowner 指针，指向被锁的对象 从对象拷贝而来的 Mark Word，即内容为hashcode-age-01的段。与此同时，对象头部的 Mark Word 内容为一个 30bit 的指向此位置的指针，及锁标记位00. 如果这个更新操作失败了，虚拟机首先会检查对象的Mark Word是否指向当前线程的栈帧：\n如果是，就说明当前线程已经拥有了这个对象的锁，那就可以直接进入同步块继续执行。\n否则，说明多个线程竞争锁，轻量级锁就要膨胀为重量级锁，锁标志的状态值变为\u0026quot;10\u0026quot;，Mark Word中存储的变成指向重量级锁（互斥量）的指针，后面等待锁的线程也要进入阻塞状态。而当前线程便尝试使用自旋来获取锁。\n解锁 # 再次使用 CAS 操作将 Displaced Mark Word 换回，如果失败，则说明锁已经膨胀为重量级锁。\n偏向锁 # JVM 用一个 epoch 表示一个偏向锁的时间戳。\n获取 # 当一个线程访问同步块并获取锁时，会在对象头和栈帧中的锁记录里存储锁偏向的线程ID，以后该线程在进入和退出同步块时不需要花费CAS操作来加锁和解锁，而只需简单的测试一下对象头的Mark Word里是否存储着指向当前线程的偏向锁，如果测试成功，表示线程已经获得了锁，如果测试失败，则需要再测试下Mark Word中偏向锁的标识是否设置成1（表示当前是偏向锁），如果没有设置，则使用CAS竞争锁，如果设置了，则尝试使用CAS将对象头的偏向锁指向当前线程。\n撤销 # 偏向锁使用了一种等到竞争出现才释放锁的机制，所以当其他线程尝试竞争偏向锁时，持有偏向锁的线程才会释放锁。偏向锁的撤销，需要等待全局安全点（在这个时间点上没有字节码正在执行），它会首先暂停拥有偏向锁的线程，然后检查持有偏向锁的线程是否活着，如果线程不处于活动状态，则将对象头设置成无锁状态，如果线程仍然活着，拥有偏向锁的栈会被执行，遍历偏向对象的锁记录，栈中的锁记录和对象头的Mark Word，要么重新偏向于其他线程，要么恢复到无锁或者标记对象不适合作为偏向锁，最后唤醒暂停的线程。\n其他优化 # 适应性自旋（Adaptive Spinning） # 当线程在获取轻量级锁的过程中执行CAS操作失败时，是要通过自旋来获取重量级锁的。为了减少CPU资源的浪费，指定自旋的次数，如果还没获取到锁就进入阻塞状态。可以使用 JVM 参数 -XX:PreBlockSpin 来调整，默认是 10。JDK 1.6 以后使用了适应性自旋，如果自旋成功了，那么这个锁的平均时间可能并不长，使用自选获得锁的几率比较大，更加经济，下次自旋的次数会更多，如果自旋失败了，则自旋的次数就会减少。\n锁粗化（Lock Coarsening） # 将多次连接在一起的加锁、解锁操作合并为一次，将多个连续的锁扩展成一个范围更大的锁，避免在无冲突情况下反复加锁解锁的代价。不过，在代码编写时，还是应当把同步块限制的尽量小。\n锁消除（Lock Elimination） # 锁消除即删除不必要的加锁操作。利用代码逃逸技术，如果判断到一段代码中，堆上的数据不会逃逸出当前线程，那么可以认为这段代码是线程安全的，不必要加锁。对于通常的代码，编写者应当已经明白数据是否进行逃逸。但作为库被使用的代码中可能有许多不必使用的同步块，如已经被废弃的 StringBuffer。\n"},{"id":37,"href":"/notes/in-depth-jvm/memory-model/","title":"JVM Memory Model","section":"In-depth Understanding JVM","content":" Java 内存模型 # 这里的 Java 内存模型（Java Memory Model, JMM）是 JVM 规范试图提出的一种平台无关的内存模型，目的是既能够达到平台无关的效果，同时能够发挥缓存、多核等硬件模型的性能。\n在这个模型中，每个线程都有自己的工作内存，与用于存储所有变量的主内存区分。二者之间的关系大致相当于 CPU Cache 与主存的关系，工作内存中存储的是主内存中存储的变量的一份拷贝。线程所有的对变量的修改都必须在工作内存中进行。需要注意的是，这里的变量指的是一个引用（Reference），而其指向的对象位于 JVM 堆中，也就是位于主内存区域。\n如果和 JVM 内存布局相关联的话，工作内存大致上相当于虚拟机栈的一部分，主内存主要对应于 JVM 堆中的对象实例。\nJava 内存模型的经典描述 # lock（锁定）：作用于主内存的变量，把一个变量标识为一条线程独占状态。 unlock（解锁）：作用于主内存变量，把一个处于锁定状态的变量释放出来，释放后的变量才可以被其他线程锁定。 read（读取）：作用于主内存变量，把一个变量值从主内存传输到线程的工作内存中，以便随后的load动作使用 load（载入）：作用于工作内存的变量，它把read操作从主内存中得到的变量值放入工作内存的变量副本中。 use（使用）：作用于工作内存的变量，把工作内存中的一个变量值传递给执行引擎，每当虚拟机遇到一个需要使用变量的值的字节码指令时将会执行这个操作。 assign（赋值）：作用于工作内存的变量，它把一个从执行引擎接收到的值赋值给工作内存的变量，每当虚拟机遇到一个给变量赋值的字节码指令时执行这个操作。 store（存储）：作用于工作内存的变量，把工作内存中的一个变量的值传送到主内存中，以便随后的write的操作。 write（写入）：作用于主内存的变量，它把store操作从工作内存中一个变量的值传送到主内存的变量中。 相应地，对这些操作进行了顺序的规定：\n不允许read和load、store和write操作之一单独出现 不允许一个线程丢弃它的最近assign的操作，即变量在工作内存中改变了之后必须同步到主内存中。 不允许一个线程无原因地（没有发生过任何assign操作）把数据从工作内存同步回主内存中。 一个新的变量只能在主内存中诞生，不允许在工作内存中直接使用一个未被初始化（load或assign）的变量。即就是对一个变量实施use和store操作之前，必须先执行过了assign和load操作。 一个变量在同一时刻只允许一条线程对其进行lock操作，lock和unlock必须成对出现 如果对一个变量执行lock操作，将会清空工作内存中此变量的值，在执行引擎使用这个变量前需要重新执行load或assign操作初始化变量的值 如果一个变量事先没有被lock操作锁定，则不允许对它执行unlock操作；也不允许去unlock一个被其他线程锁定的变量。 对一个变量执行unlock操作之前，必须先把此变量同步到主内存中（执行store和write操作）。 Volatile 规则 # JMM 为 volatile 提供了一系列特殊的规则。这些规则主要包含两个语义：\n首先，任何一条线程对 volatile 变量的修改都会被其他线程立刻得知。也就是说，变量的值在各个线程之间是一致的。即使实际上某一瞬间是不一致的，在下一次使用之前，线程都会从主内存重新刷新变量的值。\n然而，这并不意味着这个变量是线程安全的，因为作用在变量上的操作仍然不是原子的。所以，即使变量值的变化能够立刻反映到其他线程，也可能出现操作的冲突。因此，对于全局计数器这一类变量，使用 synchronized 才是正确的选择。volatile 变量更适合全局开关的角色：\nvolatile boolea shutdownRequested; void shutdown() { shutdownRequested = true; } void doWork() { while (!shutdownRequested) { // actual work } } volatile 的另一个语义是禁止指令重排。普通的变量只能在所有涉及到赋值的时刻保证变量值修改的正确结果，这在单线程情况下已经足以保证程序正确运行了。然而在多线程情境下，可能会出现更多的问题：\nvolatile boolean initialized = false; // actual initialization... initialized = true; // in another thread while (!initialized) { sleep(); } doSth(); 这种情况下，另一个线程的 doSth() 需要等待第一个线程中的初始化结束并对其进行使用。如果不使用 volatile，第一个线程中的 initialized = true 赋值就有可能在初始化尚未结束时就进行。在单线程情况下这并不是问题，因为 JVM 的指令重排会保证接下来使用数据时的正确性。但多线程情况下，我们无法保证线程间指令的顺序，就有可能访问到尚未初始化完成的数据。这个特性直到 JDK 1.5 才正式完成，也是从这时开始，双锁的单例模式得以在 Java 中实现。\npublic class Singleton { private volatile static Singleton instance; public static Singleton getInstance() { if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } } 如果我们查看这段代码相关的编译结果，就能够看到，在赋值后执行了一个起内存屏障作用的指令 lock addl $0x0. (%esp)。这条指令的后半部分把寄存器加 0 没有实际作用，而 lock 的作用是将本 CPU 的 Cache 写入内存，同时引起其他 CPU 的 Cache 中的相同位置无效化。也就是说，所有的 CPU 需要再次使用这个变量都需要重新从主存中取得。对应到 JMM，这个操作就相当于让所有的线程再次使用这个变量都需要重新 read 和 load。\n同时，从指令重排的角度来讲，这个 lock 操作强制硬件完成所有这个变量依赖的运算结果。也就是起到了 \u0026ldquo;内存屏障\u0026rdquo; 的作用。\n可以确定的是，volatile 变量的读取操作基本上和普通变量一样，写操作则要略慢。不过，考虑到 JVM 还会为锁进行优化和消除，它不一定会比 synchronized 变量快很多。\n最后，我们回到 JMM 来看 volatile 相关的规则：\n对于 volatile 变量 V 和线程 T，只有在上一个操作是 load 时，才能执行 use。又考虑到 load 和 read 必须一起出现，也就是说，需要重新从主内存读取变量的值。 T 对 V 的 assign 操作和 store 操作必须对于变量 V 连续出现。再考虑到 store 和 write 的关系，也就是说每个线程的修改操作必须要写入到主内存去。 当一个线程修改两个变量时，若 use 或 assign 更早，那么对应的 store 或 write 也更早（禁止重排）。 例外情况和特性总结 # 对于 long 和 double 这样的 64 位数据，出于性能考虑，JVM 规范并不要求其原子性。不过在目前的 64 位商业实现中，是能够保证原子性的，尤其是 double，因为 CPU 为其提供了运算单元。JDK 9 中已经包含了与之相关的参数。\nJMM 的特征大致可以总结为：\n原子性：我们大致可以认为 JMM 中所有涉及变量的操作都是原子的。（long 和 double 类型的问题完全可以被忽略）。如果需要更大范围的原子操作，lock 和 unlock 操作也能完成。JVM 通过 monitorenter 和 monitorexit 字节码来完成这两件事，也就是 Java 代码中的 synchronized 关键字。 可见性（Visibility）是指在一个线程修改了变量之后，其他线程也能够观察到改变。volatile 变量相对于普通变量提供了更加可靠的可见性保证。 有序性（Ordering）Java 在线程内是完全有序的，而在跨线程的观察中是无序的。这是由指令重排功能和工作内存的设计决定的。 可以注意到，使用 synchronized 加锁是上述这些特性的万能解决方案。\n先行发生原则 # 经过上面的讨论，我们最终得到了一系列的先行发生（Happens Before）的规则。这些规则可以认为是 JMM 的一个等价的简化描述，可以用于判断线程安全性。\n程序次序规则 Program Order Rule：一个线程内，按照程序控制流顺序，前面的代码先行发生于后面的代码。 管程锁定规则 Monitor Lock Rule：一个 unlock 操作先行发生于时间上后来对同一个锁的 lock 操作。 volatile 变量规则 Volatile Variable Rule：一个 volatile 变量的写操作先行发生于后面对这个变量的读操作。 线程启动规则 Thread Start Rule：Thread 对象的 start() 方法先行发生于线程的所有操作。 线程终止规则 Thread Terminate Rule：线程的所有操作都先行发生于现成的终止，如 Thread.join() 的返回或 Thread.isAlive() 的返回假。 线程中断规则 Thread Interrupt Rule：对线程 interrupt() 方法的调用先行发生于线程内部的代码检测到中断。 对象终结规则 Finalize Rule：一个对象的初始化完成先于其 finalize() 方法的开始。 传递性 Transitivity 这些线性发生规则完全表示了 Java 中所有不需要加锁的情景。考虑一个最简单的情景：线程 A 调用了一个 setter，线程 B 调用了对应的 getter。使用上面的规则进行分析，会发现所有的规则都不适用。因此，即使调用 getter 的操作在时间上晚发生，也无法保证得到的结果是线程 A 刚刚设定的，整个程序线程不安全。解决办法也可以一定程度上从规则中看到：使用 synchronized 或 volatile，分别对应了第二和第三条规则。\nJava 线程 # Java 中的线程 # Windows 和 Linux 提供了一对一的线程模型，即 LWT 形式。同时，现代操作系统使用的都是抢占式的线程调度模型。在 Java 中，调用 Thread.yield() 可以让出线程时间，但在抢占式调度下没有什么办法来确定当前线程是否会被抢占，或者阻止抢占。\nJava 提供了十个级别的线程优先级（Thread.MIN_PRIORITY），但优先级并不是可靠的规定。例如，Windows 下只有七个系统优先级，而且 Windows 包括了基于线程状态改变优先级的机制。\n与操作系统类似，Java 的线程有五种互相转换的状态，加上定时等待，一共六种：\n操作系统提供的线程调度的运行和等待状态都被包括在了 Java 线程的 Running 状态中。 无限期等待（Waiting）的可能性包括： 无超时的 Object.wait() 无超时的 Thread.join() LockSupport.park() 限期等待（Timed Waiting）的可能性包括： Thread.sleep() 有超时的 Object.wait() 和 Thread.join() LockSupport.parkNanos() LockSupport.parkUntil() 线程安全 # 线程安全的数据 # 可以把数据 / 变量分为五类，其\u0026quot;线程安全程度\u0026quot;由强至弱排序：\n不可变 Immutable\n如 final 的基本类型，或 String、枚举类、以及 java.lang.Number 的一些子类，如 Double 和 BigInteger。不过，用于自增的 AtomicInteger 显然不是不可变的。技术上，只要将所有带有状态的变量全部声明为 final，这个类就是不可变的。\n绝对线程安全\n绝对线程安全的数据，其所有操作都被封装了同步能力，因此任何外部调用都不需要进行同步措施。这个定义实际上很难达成。例如，从一个集合中取得一个下标，删除这个元素，再次访问。这个过程中的每一个步骤都是线程安全的，但由于取出了一些信息使其脱离了数据结构的维护，整个操作仍然会发生线程安全问题。一些数据结构的 merge() 等方法就是为了解决这样的问题，但也只能解决其中常用的一部分。\n相对线程安全\n与上面的相对应，对于数据结构的每一个单独操作都是线程安全的，这也是我们通常使用的数据结构能够达成的水平，包括已经被舍弃的 Vector HashTable 和 其他 Concurrent 的集合。\n线程兼容\n这一类对象本身不是线程安全的，可以通过同步手段来保证线程安全，也就是我们用到的绝大部分集合。\n线程对立\n完全不兼容线程安全的例子，这种情况很少，Thread.resume() 和 Thread.suspend() 的共同使用是一个例子。无论是否进行同步，都有可能造成死锁。因此这两个方法已经被声明废弃了。\n互斥同步 # 常见的互斥同步方法包括临界区（Critical Section）、互斥量（Mutex）、信号量（Semaphore）等。在 Java 中，最常用的方式就是通过 synchronized 关键字。编译时，这个关键字会在修饰的代码块前后加入 monitorenter 和 monitorexit 指令，分别接收一个引用作为参数来指明操作锁的对象。这个过程是可重入的，对象上的锁包括一个计数器。\n由于 Java 上线程的操作是映射到操作系统的，sychronized 映射到操作系统的 mutex，因此对线程进行阻塞的过程需要通过系统调用来完成，这是一个重量级的操作。\n除此之外，也可以使用 java.util.concurrent.ReentrantLock，二者的基本功能相似。API 层面的的可重入锁加入了一些高级功能：\n等待可中断，即通过 tryLock(long timeout, TimeUnit unit) 方法实现了可超时的加锁 公平锁，即多个线程在等待同一个锁时，获得锁的顺序与申请锁的时间顺序相同。通过 ReentrantLock(boolean fair) 构造方法来实现。 锁绑定多个条件。一个 ReentrantLock 对象可以绑定多个 Condition 对象。相比之下，wait()/notify() 的模式就需要嵌套多个锁。 在 JAVA 1.6 之后，由于锁优化的完善，synchronized 的性能逐渐追赶上了可重入锁。因此，功能是选择使用锁的更重要的标准。\n非阻塞同步 # 互斥同步是一种典型的悲观锁，不能拿到锁的线程需要进行阻塞，这个代价相对较高，需要经过用户态与内核态的转换、维护计数器、检查所有被阻塞的线程等一系列操作。\n另一种可能的办法是乐观策略：先进行操作，如果发生了冲突再进行补救措施。随着硬件指令集的发展，\u0026ldquo;进行操作\u0026rdquo; 和 \u0026ldquo;检查是否有冲突\u0026rdquo; 这两个操作得以原子地完成，乐观策略才能实用。这类指令包括:\nTest - and - Set 测试并设置 Fetch - and - Increment 获取并增加 Swap 交换 Compare and Swap 比较并交换，即 CAS Load - Linked / Store - Condition 加载链接 / 条件存储，即 LL / SC 其中，后两条是现代处理器引入的指令，其目的和功能类似。\nCAS 有三个操作数，分别是内存位置 V、旧的预期值 A 和新值 B，仅当内存位置的数据与 A 相等时，才使用 B 去替换，并一定返回 V 处的旧值。在现代处理器上，这是一个原子操作，在 JDK 1.5 之后由 sun.misc.Unsafe 类中的 compareAndSwapInt() 和 compareAndSwapLong() 等方法提供，JIT 将方法调用过程省去，编译成一条原子指令。当然，Unsafe 不能正常调用，使用这些方法的例子包括 java.util.concurrent.atomic.AtomicInteger 中的 compareAndSet() getAndIncrement()等。\npublic final int incrementAndGet() { for(;;) { int current = get(); int next = current + 1; if (compareAndSet(current, next)) { return next; } } } 不过，CAS 存在 ABA 问题：如果一个变量曾经被改成 C，又改回 A，CAS 就会认为变量从未改变过。当然，这种情况通常不会影响同步。虽然 Java 提供了 AtomicStampedReference 来解决这个问题，如果真的需要，直接进行互斥同步通常就可以接受。\n无同步方案 # 同步只是实现线程安全的一种方式，线程安全不一定必须进行同步。\n可重入代码是其中一种天然线程安全的代码，可以在其执行的过程中任意插入其他代码甚至递归。典型的可重入代码是，完全不使用堆空间，所有状态量都由参数传入，即一个无状态的代码块。在函数式编程中这种代码很常见。\n使用 ThreadLocal 是另一种方式，通过将变量限制在线程内部，完全避免了线程冲突。在生产者-消费者模式中，通常会将一个资源在一个线程内消费完成。\n"},{"id":38,"href":"/notes/in-depth-jvm/memory-region/","title":"JVM Memory Regions","section":"In-depth Understanding JVM","content":" JVM 内存分区 # JVM 规范中定义了五个分区：PC，栈，本地栈，堆和方法区。在这里我们还加入了方法区中的运行时常量池和不属于 JVM 管理的直接内存。\n程序计数器 PC Program Counter # 线程私有 正常情况下指向运行的字节码，native 方法时为 0 JVM 规范中唯一一个未规定 OOM 的内存区域 栈 Java VM Stack # 大部分 JVM 将栈实现为可扩展的。当栈深度达到限制的最大值时，报 StackOverflowError。当已经没有内存来扩展时，报 OOM。\n栈帧 # 一个栈帧代表栈中的一个方法的信息。内容包括：\n局部变量表 操作数 动态链接 方法出口 其生命周期为一个方法从调用到返回的全过程。\n局部变量表 # 局部变量表保存八种基本类型的数据和对象的引用。以 slot 为单位，每个 slot 为 4 Byte。因此 double 类型占用两个 slot。\n本地方法栈 Native Method Stack # 与 Java 栈类似，本地方法栈是 native 方法的栈。JVM 规范中未做强制规定，HotSpot 虚拟机将 Java 栈和本地方法栈合二为一一同进行管理。\nJava 堆 Heap # Java 堆是我们最熟悉的内存区域，这个区域是在各个线程间共享的，将发生 GC 等过程。原则上，所有的对象都会被分配在这里。随着逃逸分析和 JIT 等技术的成熟以及栈上分配和标量替换等技术的广泛应用，有些对象不一定被分配在堆里。\n在堆内部，可以根据 GC 算法分为新生代和老生代，而新生代更加细致地划分为 Eden 空间，From Survivor 空间和 To Survivor 空间等。从内存分配的角度来看，可能划分出多个 Thread Local Allocate Buffer（TLAB）等。当然，这些分区并没有实质上的区别，划分的目的是进行不同策略的 GC。\n方法区 Method Area # 各个线程共享的内存区域，用于存储 Class Loader 加载的：\n类信息 常量 静态变量 JIT 编译后的代码 等内容。也被称为非堆（Non-Heap）。\n在 Hotspot 早先的版本，方法区与字符串常量池（Interned Strings）一起被实现为堆的 \u0026ldquo;永久代\u0026rdquo;。\n运行时常量池 Runtime Constant Pool # 运行时常量池是方法区的一部分，用来存储 Class 文件中定义的各种字面量和符号引用等。在旧版本中，字符串常量池就是一个典型的例子，说明我们可以在运行时向这个常量池动态地添加对象。\n直接内存 # 直接内存，顾名思义就是跳过 JVM 直接在系统内存中进行管理的区域。典型的例子是 Java 1.4 中引入的 NIO 中的 Buffer。通过 native 方法实现，将 Buffer 直接实现在系统中，能够避免在系统内存与 JVM 进程内存之间反复进行拷贝。显然，这个区域不会受到 JVM 参数的影响。当然，当整个系统的内存不足时，同样会发生 OOM。\nHotSpot 的去永久代 # 方法区的变化与永久代 # 永久代事实上是旧版本 Oracle JDK / OpenJDK 对 JVM 规范中方法区的一种实现。在 HotSpot 中，如果永久代发生了内存不足，则会触发 Full GC 以回收常量和加载的类，而 Full GC 通常是我们十分不想见到的。而且，对类进行卸载和回收的条件相当苛刻，再加上字符串常量池的大小，也导致这个位置的 OOM 仍然十分常见。\nJDK 6 中，方法区的 JIT 编译后代码被储存在 native 内存的 JVM-codeCache 部分，其余均为永久代 JDK 7 中，Symbol 的存储移动到了 native 内存，静态变量移动到了 java.lang.Class 对象的末尾，位于堆中。 JDK 8 中，彻底移除了永久代，将整个方法区转移到元空间 Metaspace。（-XX:MaxMetaspaceSize）这个区域位于 Native Heap 中，因此几乎没有容量的限制，除了 JVM 参数。 字符串常量 String Interning # 字符串常量池 # JDK 为八种基本类型和 String 提供了常量池。其中，字符串常量池在 JDK 7 中被从永久代移除。字符串进入常量池的方式有两种：\n使用双引号声明字符串 使用 String::intern 方法 其核心思想是，对于可能多次使用的字符串，将它们的引用指向同一个位于常量池中的字符串，以避免重复分配对象。\n在 native 部分，字符串常量池实际上是一个固定长度、使用拉链法的 HashMap。因此，当字符串常量数量超过了 Map 容量之后之后，性能就会开始下降。JDK 7u40 之前，这个值是 1009，之后提高到了 60013，基本上能够满足我们的需求。可以使用 -XX:StringTableSize 参数来调整。与一般的 HashMap 一样，容量达到存入数据的两倍时能够比较好地避免碰撞。\n更多实现细节 # 在 JDK 6 和 JDK 7 下分别执行同一段代码:\npublic static void main(String[] args) { String s = new String(\u0026#34;1\u0026#34;); s.intern(); String s2 = \u0026#34;1\u0026#34;; System.out.println(s == s2); String s3 = new String(\u0026#34;1\u0026#34;) + new String(\u0026#34;1\u0026#34;); s3.intern(); String s4 = \u0026#34;11\u0026#34;; System.out.println(s3 == s4); String s5 = new String(\u0026#34;1\u0026#34;) + new String(\u0026#34;1\u0026#34;); String s6 = \u0026#34;11\u0026#34;; s5.intern(); System.out.println(s5 == s6); } 结果分别为 false false false 和 false true false。在这里，我们就能一定程度上看到 JDK6 和 JDK 7 中实现的区别。\n从第一段代码开始。首先，我们知道的是，所有双引号声明的字符串都会在常量池中生成，而 new 操作符得到的对象则被分配在堆中。然后，当我们调用 intern 方法时，字符串将会被放在常量池中。问题是，当前我们使用的这个引用呢？\n实际上，这时我们 new 得到的 String 对象仍然存在，其位置位于一般的 Java 堆中。只是，其内容指向了常量池，即 JDK 6 中的永久代位置，或后来的堆中的常量池部分。相比之下，我们直接使用双引号声明的 s2 则是直接指向常量池中的对象。第三段代码也是同样的情况。\n那么，在第二段代码中，JDK 6 和 7 有什么区别呢？因为我们知道，JDK 7 之后。常量池已经被从永久代移到了一般的堆中，这意味着我们完全可以不再存储一个对象，而是直接在常量池中存储一个引用，这个引用和 s3 指向同一个对象。也就是说，常量池中存储的是一个指向 Java Heap 中某个对象的引用。\n而到了第三段代码，执行顺序的变化也导致了对象引用关系的变化。当然，Java 语言规范并没有规定 == 符号在对象上的关系。使用 equals 仍然是合适的做法。下面的图分别展示了 JDK 6 和 JDK 7 中的情况\n对象的创建 # JVM 中对象创建的流程 # 判断是否需要类加载 分配内存 初始化为全 0 设置对象头 执行 \u0026lt;init\u0026gt; 分配内存的细节 # 寻找可用内存空间 # 首先要考虑的是对象是否需要使用连续的内存空间。在 HotSpot 中，对象的空间始终是连续的。因此，当内存碎片和大对象的创建同时出现时，就可能需要很多的 GC 来移动对象腾出整块的空间。在 IBM J9 等 JVM 中可以使用不连续的空间，但相应地会提高性能代价。\n当使用带有 Compact 的 GC 器，即已用内存是连续的时，只需要维护一个指向已用与未使用部分分界线的指针并进行移动即可。这种方式叫做 Bump the Pointer。相应地，如果内存不连续，就需要使用 Free List 来寻找。\n多线程分配和 TLAB # 创建对象在虚拟机中是非常频繁的行为，因此线程冲突是十分常见的。有可能一个线程在指针位置创建一个对象时，同时另一个线程又使用了这个指针。解决这个问题的方案有两种，一种是通过 CAS 和失败重试等方式来保证更新操作的原子性，另一种是使用 TLAB（Thread Local Allocation Buffer）。\nTLAB 位于 Java 堆的 Eden 区（复制 GC 算法中允许分配新对象的区域）中，为每个线程分配一个。每个线程在创建对象时，只能在自己的 TLAB 中进行分配。这样，就从根源上避免了线程之间的分配冲突。TLAB 并不需要太大，用尽之后还可以继续分配新的区域。当然，分配新的 TLAB 需要进行线程间的同步，避免多个线程分配到同一个区域。这个特性可以通过 -XX:-UseTLAB 来设置。\n对象初始化 # 首先，JVM 需要将对象的所有位都设置为 0，这也是基本类型的默认值的来源。如果使用 TLAB，这个步骤也可以提前到 TLAB 的分配。\n对象头的主要内容包括对象所属的类、类的部分信息、对象的 HashCode、GC 年龄和锁的信息等。\n然后，JVM 会调用 \u0026lt;init\u0026gt; 方法。这个方法会在任何形式的对象创建时被调用，包括 new、反序列化、反射或 clone()，其主要作用是按照类编写者的意愿对对象的实例变量进行初始化。类似的是针对静态变量等进行初始化的 \u0026lt;cinit\u0026gt; 方法。\n具体来说，\u0026lt;init\u0026gt; 方法是被字节码中的 invokespecial 指令来调用，这个指令也被用在 super 关键字和私有方法的调用中。例如，new StringBuffer ：\nnew java/lang/StringBuffer ; create a new StringBuffer dup ; make an extra reference to the new instance ; now call an instance initialization method invokespecial java/lang/StringBuffer/\u0026lt;init\u0026gt;()V ; stack now contains an initialized StringBuffer. 在 super 关键字的应用中，例如：\npublic boolean equals(Object x) { return super.equals(x); } 则会得到：\naload_0 ; push \u0026#39;this\u0026#39; onto the stack aload_1 ; push the first argument (i.e. x) onto the stack ; now invoke Object\u0026#39;s equals() method. invokespecial java/lang/Object/equals(Ljava/lang/Object;)Z 对象的内存布局和访问 # 对于 HotSpot 这种使用连续内存分配对象的虚拟机，对象的内存布局比较简单，可以分为对象头、实例数据和 Padding 三部分。\n对象头 # 对象头（Object Header）结构：\n长度（32bit JVM 中） 内容 作用 1字宽（32bit） Mark Word 与锁有关 1字宽 Class Metadata Address 对象所属类型的指针 1字宽（仅数组） Array Length 数组长度 对象 Mark Word 的内容：\n锁状态 23bit 2bit 4bit 1bit（是否为偏向锁） 2bit（锁状态） GC 标记 空 空 空 空 11 无锁 对象hashCode（25bit） hashCode 对象分代年龄 0 01 偏向锁 线程id Epoch 对象分代年龄 1 01 轻量级锁 指向栈中锁记录的指针（30bit） 指针 指针 指针 00 重量级锁 指向互斥量的指针（30bit） 指针 指针 指针 10 数组类型多出一个数组长度域的原因是，VM 可以从对象的类型直接推断出对象的长度，却无法推断出数组的长度。此外，也不是所有的虚拟机都使用类型指针来指示类的信息。在 64 bit 的 JVM 下，除了字段的长度，还要考虑是否使用了指针压缩等技术。\n实例变量和 Padding # 接下来是实例变量的存储。通常，虚拟机选择将相同长度的数据（如 long 和 double，short 和 char）存放到一起，以避免内存空间的浪费。在这个前提之下，父类的实例变量会被存储在子类变量之前。如果开启了 CompactFields（默认启用）参数，则还可能将较短的变量插入到较长的变量之间的空隙。\n总之，实例变量部分的策略和 Padding 部分的存在都是为了进行内存空间的对齐。JVM 中，对象的起始位置是以 8 Byte 为单位进行对齐的。不足的部分就需要 空白的 Padding 来填充。\n访问对象的方法 # JVM 规范中只规定了对象指向引用，而没有规定通过引用寻找对象的具体方法。引用寻址的实现主要有两种：\n句柄访问 # 这种方式在 Java 堆中划分了另外一个部分作为句柄池。句柄池中的每一条记录包括指向堆中对象实例的指针和指向方法区中类型数据的指针。这种方法的优势是，在堆中移动对象的实际位置（GC 时十分常见）时，只需要修改句柄中的指针，而不需要回到栈上修改每一个引用的指针。\n直接指针访问 # 直接指针访问时，引用直接指向堆中的对象本身，对象内部包含指向方法区中类型数据的指针。这种方式的显著优势就是节省了一次寻址，鉴于对象访问十分频繁，这个优势也是十分可观的。我们前面在对象头中看到的类型数据指针就是这样的实现。\n制造 OOM # 堆溢出 # 堆溢出是最容易产生的异常，只要不停地创建\u0026quot;活着\u0026quot;的对象即可。\n// VM Args: -XX:MaxHeapSize=8M / -Xmx8m // VM Args: -XX:InitialHeapSize=8M / -Xms8m // VM Args: -XX:MaxNewSize=4M / -Xmn4m // -XX:+HeapDumpOnOutOfMemoryError public static void main(String args) { List\u0026lt;int[]\u0026gt; list = new ArrayList\u0026lt;\u0026gt;(); while(true) { list.add(new int[100000]); } } 通过 Heap Dump，可以查找对象的 GC Root，以排查内存泄漏的原因。否则，就要通过调整参数来避免。\n栈溢出 # 在 HotSpot 中并不区分虚拟机栈和本地方法栈。栈部分的溢出有两种形式，StackOverflow 和 OutOfMemory。对于爆栈，只需要进行大量的递归即可。相关的参数为 -Xss128k，等价于 -XX:ThreadStackSize=128k。当每个栈帧的本地变量表较大时，爆栈时栈的深度相应就更小。\n在单线程情况下，无论栈帧太大还是栈内存不足，抛出的都是 StackOverflowError。通常这个深度能够达到 1000 ~ 2000 的级别。OOM 则是出现在创建大量线程的情形，不过这时并不是栈的内存不足，而是无法再分配一个新的栈。\n方法区溢出 # 前面我们知道，JDK 6 之前的永久代溢出是常见的，只需要通过大量的 String.intern，就能够让字符串常量池产生 OOM。而在 JDK 7 中，常量池转移到了堆中，且只保存一些引用，因此无法再发生这类 OOM。\n另一种方法区 OOM 的可能性是加载大量的类。这种情况除了加载 ClassPath 中的类之外，还可能是加载大量的 JSP、反射机制的 GeneratedConstructorAccessor、动态代理、OSGi 或 CGLib 操作字节码等方式。无论如何，在 Hibernate、Spring 等框架中，这类情况并不少见。\n在 JDK 8 中，随着\u0026quot;去永久代\u0026quot;，与之相关的参数变成了 -XX:MetaspaceSize 和 -XX:MaxMetaspaceSize。而在之前的版本中是 -XX:PermSize 和 -XX:MaxPermSize。其中前一个参数都是初始大小。JDK 8 中，默认将 Metaspace 的最大值设为了系统内存，因此不会发生溢出的情况。不过，默认的初始值比较小，扩展的过程会带来一些额外的 GC。\n直接内存溢出 # 直接内存溢出只有两种可能，使用 Unsafe 方法或使用 NIO 的 DirectByteBuffer。\n不过，JDK 的作者为此进行了一定的保护：DirectByteBuffer 并没有真的向系统申请内存。它只是发现内存不足以分配，于是手动抛出了异常。另一种方式 Unsafe 方法则要求只有 Bootstrap CLassLoader 加载的类的调用才能返回实例\u0026mdash;\u0026mdash;也就是说只希望 JDK 提供的类来访问。我们可以通过反射来绕过这个规定，当然，并不推荐这样做。\n// VM Args: -Xmx20M -XX:MaxDirectMemorySize=10M public static void main(String[] args) throws Excecption { Field unsafeField = Unsafe.class.getDeclaredFields()[0]; unsafeField.setAccessible(true); Unsafe unsafe = (Unsafe) unsafeField.get(null); while(true) { unsafe.allocateMemory(1024 * 1024); } } 直接内存的 OOM 的特征是，不会在堆 Dump 中发现异常，因此，如果使用了 NIO，就要把这个因素考虑在内。\n"},{"id":39,"href":"/pl/lambda/","title":"Lambda Calculus and Y Combinator","section":"Programming Languages","content":" Lambda 演算与 Y 组合子 # λ 演算 # λ 项 # 只有三种有效的 λ 项：\n一个变量 $x$ 一个抽象 $\\lambda f.\\lambda x.x$，大致上等同于 Python 中的 lambda f, x: x 函数的应用 $ts$，大致上等同于 Python 中的 t(s) α-等价 # 在一个抽象中，变量的名字并不重要。例如 $\\lambda x.x$ 和 $\\lambda y.y$ 是 α-等价的。\n描述这种变换的一种记法是使用：$t[x:=r]$，表示在 $t$ 中将所有的 $x$ 重命名为 $r$。于是有：\n$x[x:=r]=r$，将 $x$ 替换为 $r$ $y[x:=r]=y$，$y$ 中不包括 $x$，无需替换 $(ts)[x:=r]=(t[x:=r])(s[x:=r])$，对应用，将两部分分别替换 $\\lambda x.t[x:=r]=\\lambda x.t$，替换前后并没有区别（α-等价） $\\lambda y.t[x:=r]=\\lambda t.t[x:=r]$ 自由变量与约束变量 # 自由变量被定义为这样的集合：\n对于变量 $x$，其自由变量集合仅包括 $x$ 对于抽象 $\\lambda x.t$，其自由变量集合为 $t$ 的自由变量集合去掉 $x$。 对于应用 $ts$，其自由变量为 $t$ 和 $s$ 自由变量集合的并集。 定义这些的目的是在进行β-归约时避免变量名的冲突。不严谨地说，自由变量就是\u0026quot;可以被替换\u0026quot;的变量集合。\nβ-归约 # β-归约表示，在应用函数时，可以直接对结果中相应的变量进行替换。即，$(\\lambda x.t)s=t[x:=s]$。\n在无类型 λ 演算中，有些形式无法被化到最简（称范式），如：\n$$ (\\lambda x.xx)(\\lambda y.yy) = (\\lambda y.yy)(\\lambda y.yy) $$\n甚至，对于 $\\lambda x.xxy$，反而会越来越长（$y$ 越来越多），顺序也会影响归约会否进入死循环。好在，有两个定理解决了这个问题：\nChurch-Rosser 定理：如果一个 λ 项有范式，那么这个范式（在α-等价的意义上）是唯一的。 对一个 λ 项，始终归约其最左侧最外侧的可约式，总能得到β-范式（如果存在）。 η-变换 # η-变换并不是必需的，实际上它是前两者的一个推论：$\\lambda x.f x=f$，其中 $x$ 不是 $f$ 中的自由变量。\n邱奇数 # 邱奇数是为 λ 演算而生的数的表示方式，同样基于皮亚诺公理（使用一个基础，即0或1，以及一个后继的定义来描述自然数）。我们令：\n$$ \\begin{align*} \\operatorname{zero} \u0026amp;= \\lambda f.\\lambda x.x \\\\ \\operatorname{succ} \u0026amp;= \\lambda n.\\lambda f.\\lambda x.f(n\\ f\\ x) \\end{align*} $$\n记得柯里化的原则：$n\\ f\\ x=n(f)(x)$。于是，就能得到：\n$$ \\begin{align*} \\operatorname{one}\u0026amp;=\\operatorname{succ\\ zero}\\\\ \u0026amp;=(\\lambda n.\\lambda f.\\lambda x.f(n\\ f\\ x))(\\lambda g.\\lambda y.y)\\\\ \u0026amp;=\\lambda f.\\lambda x.f\\ x\\\\ \\operatorname{two}\u0026amp;=\\lambda f.\\lambda x.f(f\\ x)\\\\ \\operatorname{three}\u0026amp;=\\lambda f.\\lambda x.f(f(f\\ x)) \\end{align*} $$\n在这个定义中，$f$ 的次数就意味着自然数的值。因此可以有：\n$$ \\begin{align*} \\operatorname{plus}\u0026amp;=\\lambda m.\\lambda n.m\\ \\operatorname{succ}\\ n\\\\ \\operatorname{plus\\ one\\ two}\u0026amp;= (\\lambda g.\\lambda y. g y) (\\lambda n.\\lambda f.\\lambda x f(n f x))\\ \\operatorname{two}\\\\ \u0026amp;=(\\lambda y.(\\lambda n.\\lambda f.\\lambda x.f(n\\ f\\ x))y)\\ \\operatorname{two}\\\\ \u0026amp;=(\\lambda y.(\\lambda f.\\lambda x.f(y\\ f\\ x)))\\ \\operatorname{two}\\\\ \u0026amp;=(\\lambda n.\\lambda f.\\lambda x. f(n\\ f\\ x))\\ \\operatorname{two}\\\\ \u0026amp;=\\operatorname{succ\\ two}\\\\ \\end{align*} $$\n同理，有：\n$$ \\begin{align*} \\operatorname{mult}\u0026amp;=\\lambda m.\\lambda n.\\lambda f.n(m\\ f)\\\\ \\operatorname{exp}\u0026amp;=\\lambda m.\\lambda n.n\\ m\\\\ \\operatorname{pred}\u0026amp;=\\lambda n.\\lambda f.\\lambda x.n(\\lambda g.\\lambda h.h(g\\ f))(\\lambda u.x)(\\lambda u.u)\\\\ \\operatorname{sub}\u0026amp;=\\lambda m.\\lambda n.m\\ \\operatorname{pred}\\ n \\end{align*} $$\n邱奇逻辑 # $$ \\begin{align*} \\operatorname{true}\u0026amp;=\\lambda a.\\lambda b.a\\\\ \\operatorname{false}\u0026amp;=\\lambda a.\\lambda b.b\\\\ \\operatorname{and} \u0026amp;= \\lambda p.\\lambda q.p\\ q\\ p\\\\ \\operatorname{or} \u0026amp;= \\lambda p.\\lambda q.p\\ p\\ q\\\\ \\operatorname{if} \u0026amp;= \\lambda p.\\lambda a.\\lambda b.p\\ a\\ b \\end{align*} $$\n更进一步，可以基于这些进行谓词逻辑的运算：\n$$ \\begin{align*} \\operatorname{iszero}\u0026amp;=\\lambda n.n (\\lambda x.\\operatorname{false}) \\operatorname{true}\\\\ \\operatorname{leq}\u0026amp;=\\lambda m.\\lambda n.\\operatorname{iszero} (\\operatorname{sub}m\\ n)\\\\ \\operatorname{eq}\u0026amp;=\\lambda m.\\lambda n. \\operatorname{and} (\\operatorname{leq}m\\ n)(\\operatorname{leq}n\\ m) \\end{align*} $$\nY 组合子 # 接下来考虑一个递归函数：\n$$ \\operatorname{fact} = \\lambda n.\\operatorname{if}(\\operatorname{iszero}\\ n)\\ \\operatorname{one}\\ (\\operatorname{mult}\\ n (\\operatorname{fact}(\\operatorname{pred}\\ n)) $$\n直接进行递归是不行的，这里只是一个简单的演算，$\\operatorname{fact}$ 这个名字，甚至所有的名字都是不必要的。一种方式是这样的：\n$$ \\begin{align*} \\operatorname{fact1}\u0026amp;=\\lambda f.\\lambda n.\\operatorname{if}(\\operatorname{iszero}\\ n)\\ \\operatorname{one}\\ (\\operatorname{mult}\\ n (f\\ f(\\operatorname{pred}\\ n))\\\\ \\operatorname{fact}\u0026amp;=\\operatorname{fact1}\\ \\operatorname{fact1} \\end{align*} $$\n通过这种方式，使用 $f\\ f$来代替原来的递归，就能做到调用自身。不过，这样做仍然不够优雅。因此，Haskell 和图灵分别发现了两个不动点组合子（常用的是前者）：\n$$ \\begin{align*} \\operatorname Y\u0026amp;=\\lambda f.(\\lambda x.(f(x\\ x))(\\lambda x.(f(x\\ x))))\\\\ \\operatorname\\Theta\u0026amp;=(\\lambda x.\\lambda y.(y (x x y))) (\\lambda x.\\lambda y.(y (x x y))) \\end{align*} $$\n由于其前半部分和后半部分相同，经常也写作：\n$$ \\operatorname Y=\\lambda f.(\\lambda g.g\\ g)(\\lambda x.f(x\\ x)) $$\n经过简单的演算，可以得到：\n$$ \\begin{align*} \\operatorname Y\\ g\u0026amp;=\\lambda f.(\\lambda x.(f(x\\ x))(\\lambda x.(f(x\\ x))))\\ g\\\\ \u0026amp;=(\\lambda x.(g(x\\ x)))(\\lambda y.(g(y\\ y)))\\\\ \u0026amp;=g\\ ((\\lambda y.(g(y\\ y)))(\\lambda y.(g(y\\ y))))\\\\ \u0026amp;=g\\ (\\operatorname Y\\ g) \\end{align*} $$\n除了符合函数不动点的定义之外，更重要的是在程序中它可以\u0026quot;再次调用函数\u0026quot;，直到抵达递归中的结束条件。需要注意的是这两种形式都使用于 call-by-name 的求值策略，传值调用下需要稍微修改一下形式：\n$$ \\begin{align*} \\operatorname Z\u0026amp;=\\lambda f.(\\lambda x.f(\\lambda y.x\\ x\\ y))(\\lambda x.f(\\lambda y.x\\ x\\ y))\\\\ \\operatorname\\Theta_V\u0026amp;=(\\lambda x.\\lambda y.(y(\\lambda z.x\\ x\\ y\\ z)))(\\lambda x.\\lambda y.(y(\\lambda z.x\\ x\\ y\\ z))) \\end{align*} $$\n"},{"id":40,"href":"/cs/linux-io-multiplex/","title":"Linux IO Multiplexing","section":"Computer Science","content":" Linux IO 多路复用 # 同步/异步 阻塞/非阻塞 # 概念 # 《UNIX Network Programming》中给出了五种 IO 模型：\n同步阻塞 IO\n同步非阻塞 IO\nIO 多路复用\n信号驱动 IO\n异步 IO\n在考虑 IO 模式之前，要先回顾一些关于操作系统的知识。在程序调用系统调用（System Call）时，程序转而进入内核态，在内核态中完成 IO 操作，并将 IO 的数据从内核空间的 buffer 拷贝到用户空间的 buffer 中。回到用户态之后，程序将可以从 buffer 中取得数据。\n同步阻塞 IO # 对于同步阻塞 IO，在调用系统的 recvfrom 调用之后，进入内核态，线程阻塞。直到系统内核接收到数据并把数据拷贝到用户空间 buffer 全部完成后，才能从阻塞中恢复。\n同步非阻塞 IO # 在非阻塞模式的 IO 中，如果调用 recvfrom 时系统尚未收到对应的数据，则不会阻塞，而是直接返回 EWOULDBLOCK 错误。如果调用时已经收到了数据，才会在拷贝 buffer 的过程中阻塞。通常来说这个时间是短暂而稳定的，因此这种 IO 模式完全可以认为是非阻塞的。\n多路 IO 复用 # select poll epoll 都属于这一类。这种 IO 模式并没有实现真正的异步 IO，却能够获得异步 IO 的一些优点。\n例如，假设我们现在需要监听多个端口的 UDP 数据。在同步阻塞 IO 下，我们需要为每个端口的监听准备一个线程，这些线程全部阻塞住，有数据时再进行处理。这在高并发场景下会带来海量的 RAM 需求。\n然后我们考虑在同步非阻塞 IO 下应该怎样处理。一个比较可行的方法是，在一个线程内对所有的 IO 进行轮询。如果返回错误，就访问下一个监听端口。如果有数据，则处理数据。这种方法十分可行，但编码复杂，也存在一定程度的性能浪费。\n多路 IO 复用可以认为是替我们完成了上述的过程（当然不完全一样）。例如 select，就是阻塞直到所有被监听的事件中有一个获得数据，就立刻返回。这样，我们就能在一个线程之内处理许多个 IO，这就叫做多路 IO 复用。\n信号驱动 IO # 信号驱动的 IO 并不需要用户程序去查询是否有 IO 数据到达，而是让用户程序在系统中注册。当数据到达时，内核反过来通知用户程序，用户程序阻塞并开始进行 buffer 拷贝。这种形式的 IO 比较少见。\n异步 IO # 异步 IO 在调用时立刻返回用户态，但并不得到结果。直到系统得到数据并拷贝到 buffer 之后，系统通知客户程序，客户程序可以开始处理数据。\n遗憾的是，Linux 尚未实现真正异步的网络 IO。实际情况下，程序需要阻塞在拷贝 buffer 过程。在 Windows、Solaris 等系统的 IOCP 模型下可以实现真正的异步。\nselect 与 poll 的问题 # Linux 的 wakeup callback 机制 # 在介绍 IO 复用之前需要首先介绍 wakeup callback 机制。Linux 为每个 socket 维护一个 sleep_list，列表中存储 wait_entry，存储监听该 socket 的进程信息。\n调用 select poll epoll_wait，陷入内核，查看监听的 socket 是否有事件。如果有，就返回处理。\n如果没有，为当前进程建立一个 wait_entry 节点，插入到所监控 socket 的 sleep_list 中。\n进入循环，阻塞\n当 socket 事件发生时，遍历 socket 对应的 sleep_list，调用其 callback 函数。通常会唤醒对应的进程进入 CPU 就绪队列。\n队列遍历完成或遇到排他的 wait_entry，停止遍历。\nselect # 一个进程可以通过 select 监听多个 socket，并阻塞。在所有监听的事件中有任何一个发生时，进程从阻塞中恢复，我们可以遍历所有监听的事件来查看到底是哪一个可用。\n回到上面的 wakeup callback 机制。要达到这样的效果，我们需要把这个进程插入到所有监听的 socket 的 wait_list 中去。当其中任何一个有事件发生时，进程就会被唤醒。\nint select( int nfds, fd_set *restrict readfds, fd_set *restrict writefds, fd_set *restrict exceptfds, struct timeval *restrict timeout ); 这里的 fd_set 的实质内容是一个 long int。可以看到，select 将三类事件分开传入。\n调用时，程序将会把要监听的 fd_set 文件标识符拷贝到内核态。可以想到，这时系统会首先先检查所有的监听事件是否有发生的。如果有，就可以立刻进行返回。如果所有事件都没有，就进入阻塞状态。当然，还可以设置超时时间。\n随后，当事件发生时，我们需要重新遍历一遍所有的监听事件，因为我们不知道是否还有其他事件同时发生。收集到所有的事件之后，我们才可以返回用户态。\nselect 存在的问题：\n将 fd_set 拷贝到内核态的开销。\n为了减少拷贝 fd_set 的影响，select 的集合大小在内核中限制为 1024。对于高并发的情形，仍然需要做很多额外的工作。\n得到事件通知后，我们仍然需要遍历整个列表来寻找究竟是哪一个发生了事件。\npoll # poll 的出现解决了 select 的 1024 限制问题。函数原型如下：\nstruct pollfd { int fd; /* File descriptor to poll. */ short int events; /* Types of events poller cares about. */ short int revents; /* Types of events that actually occurred. */ }; int poll(struct pollfd *fds, nfds_t nfds, int timeout); poll 不再使用 fd_set，而是把要监听的事件封装到 pollfd 中去，减少了参数的数量。\n然而，所有的文件描述符仍然需要拷贝到内核态，并且当描述符超过 1024 个之后，性能仍然会下降。也就是说，poll 并没有解决 select 的根本问题。\nepoll # epoll 的使用示例 # int epoll_create(int size); int epoll_ctl( int epfd, int op, int fd, struct epoll_event *event ); int epoll_wait( int epfd, struct epoll_event *events, int maxevents, int timeout ); struct epoll_event ev; int main() { // these functions return -1 while error occurs epfd = epoll_create(argc - 1); fd = open(argv[j], O_RDONLY); epoll_ctl(epfd, EPOLL_CTL_ADD, fd, \u0026amp;ev) ready = epoll_wait(epfd, evlist, MAX_EVENTS, -1); for (int j = 0; j \u0026lt; ready; j++) { if (evlist[j].events \u0026amp; EPOLLIN) { // process event } } } 解决集合拷贝问题 # 回顾 select 和 poll 我们会发现，每次调用都需要重新将整个文件描述符集合传入内核态。然而实际情况下，这个集合变化并不频繁，我们的服务器通常固定监听少数几个端口。即使变化，整个集合也很少会全部变化，而是少数的增加和删除。因此，我们首先可以将这个过程剥离成为另一个函数：epoll_ctl。也就是说，一次拷贝，多次使用。然后，我们设立几个操作：EPOLL_CTL_ADD EPOLL_CTL_MOD EPOLL_CTL_DEL。这样，我们在拷贝上的开销就减少到了最低。\n不过，这个过程还涉及到对描述符集合的 CRUD。由于我们已经突破了 1024 的限制，在这里使用一个高效的数据结构就存在必要。在 Linux 2.6.8 之前，这里使用的是 Hash。因此，epoll_create 的原型里包含了初始容量。其后，Linux 改用了红黑树来处理这个问题。因此，这个参数现在没有任何作用。\n其次，在事件就绪后，我们还需要回头再次把文件描述符拷贝出来。对于这个问题，epoll 的解决办法是将用户空间和内核空间的地址映射到同一块物理内存上去。这样，用户程序就无需拷贝可以直接访问。\n遍历 fd 集合 # select 的另一个问题是经常需要遍历整个集合。面对这个问题，epoll 拿出的解决办法是使用一个中间层。\n在 select 中，我们需要在所有 socket 的睡眠队列中插入对应的进程的标识。相比之下，epoll 将进程睡眠在 ready_list 上，而将操作 ready_list 的操作绑定在 socket 的睡眠队列上。这样，当 socket 发生事件时，所对应的事件就被加入到 ready_list 中。于是，在从阻塞中返回之前，我们只要看遍历 ready_list，而不再需要遍历所有监听的事件。总的来看，epoll 把对于整个兴趣列表的遍历变成了对所有已经有事件就绪的兴趣的列表的遍历。\nepoll 的边缘触发与水平触发 # 概念 # 在边缘触发情况下，信号仅在 buffer 状态变化时发出。而在水平触发的情况下，在整个 buffer 的状态，信号都会持续发出。\n以 epoll 的读取就绪事件为具体例子，在 ET 模式下，仅当数据到达 buffer 时，该 socket 对应的事件被加入到 ready_list 以供读取。而在 LT 模式下，只要 buffer 不为空，这个 socket 对应的事件就会保持在 ready_list。\n区别 # 显而易见，LT 模式下的触发和遍历都要更多一些，看起来性能会更弱一些。不过，在良好网络状况的状态下，如果一个 socket 紧接着就得到了另一组数据，那么相比 ET 模式，我们就省去了一次重复的复制过程。因此，两种方式的性能区别实际上并不明显。\n考虑我们希望多次读取一个 socket 的情况。显然，我们不能在一次循环中直接调用两次 recv，因为可能会在这里阻塞。然而，如果在 ET 模式下再次调用 epoll_wait，由于 ET 模式没有被触发，无法再次得到这个 socket。总之，在阻塞模式下 ET 模式会遇到很多问题。因此，阻塞模式下不能使用 ET。如果要使用 ET，必须使用非阻塞模式，在一次循环中将所有数据全部读取出来。\n相比之下，在 LT 模式下，我们可以在一次循环中只处理一个请求，因为即使没有处理，下一次 epoll_wait 也会将其取出，不会发生饿死的情况。\n"},{"id":41,"href":"/notes/","title":"Notes on Books","section":"czdm75 Blog","content":" Notes on Books # Core Java for Impatients Designing Data-Intensive Applications In-depth Understanding JVM Introduction to Algorithms Programming in Scala "},{"id":42,"href":"/notes/programming-scala/","title":"Programming in Scala","section":"Notes on Books","content":" Programming in Scala # 1. Basics 2. Functions 3 .Inheritance, Package, Assertion 4. Pattern Matching, Collections 5. Generics, Abstract, Implicits 6. Collections, Extractor, etc "},{"id":43,"href":"/pl/","title":"Programming Languages","section":"czdm75 Blog","content":" Programming Languages # Java NIO Internal Java NIO Usage Lambda Calculus and Y Combinator Scala: Currying, Partially Applied, Partial Scala: Monad, from Scala Perspective "},{"id":44,"href":"/pl/curry/","title":"Scala: Currying, Partially Applied, Partial","section":"Programming Languages","content":" Scala 中的柯里化、偏函数与部分应用函数 # 柯里化（Currying）、部分应用函数（Partially Applied Function）和偏函数（Partial Function）是 Scala 中三个非常容易混淆的元素。在 Groovy 中，部分应用函数也被称作了柯里化。实际上，二者的作用十分相似，思维上却有微妙的差异。\n在传统面向对象方法中涉及到工厂或模板的场合，通常正是柯里化和部分应用函数发挥作用的地方。另一种常见的场合是，在一系列代码中需要为一个函数绑定某一个固定的参数。在传统方式中，我们可能会抽象一个函数出来。但在函数式范式中，我们可以直接将这个\u0026quot;临时函数\u0026quot;绑定在一个变量中。\n概念上的区别 # 简单来说，柯里化的结果是\u0026quot;一串函数中的下一个\u0026quot;，而部分应用函数的结果是一个\u0026quot;减少了参数的函数\u0026quot;。对于函数 f(x, y, z)，其完全柯里化的结果是 f(x)(y)(z)。然后，我们将参数应用进去，例如规定 x 的值，得到的结果将是 g(y)(z)。反过来，对于部分应用函数，其结果将是 g(y, z)。\n偏函数之所以会被混淆则是因为名字过于相似。偏函数的\u0026quot;Partial\u0026quot;意味着其只能处理其所接受的参数的一部分。这种情况常见于模式匹配。偏函数可以被连接起来，上一个函数无法处理的参数被传递至下一个函数去处理。\nGroovy \u0026amp; Clojure # def volume = {h, w, l -\u0026gt; h * w * l} def area = volume.curry(1) def lengthPartialApplied = volume.curry(1, 1) def lengthCurried = volume.curry(1).curry(1) Groovy 的这些操作实际上是部分应用而不是柯里化。不过，这两种操作毕竟是十分相似的。而且，Groovy 也允许我们返回一个函数，以进行函数的复合。\ndef composite = { f, g, x -\u0026gt; return f(g(x)) } def func = composite.curry(func1, func2) Clojure 中的情况略微类似：\n(def substract-from-hundred (partial - 100)) (substract-from-hundred 10) ; same as (- 100 10) results 90 (substract-from-hundred 10 20) ; same as (- 100 10 20) results 70 Clojure 原生同样只提供了部分应用。同样地，用它来实现柯里化也很容易。\nScala # Scala 通过对多个参数列表的支持提供了良好的柯里化功能：\ndef modN(n: Int)(x: Int) = x % n == 0 modN(5)(25) // true (1 to 10).toList.filter(modN(5)) // List(5, 10) 同时，Scala 也提供对部分应用的支持。针对同样的场景，可以有：\ndef modN(n: Int, x: Int) = x % n == 0 (1 to 10).toList.filter(modN(5, _)) // List(5, 10) Scala 中的偏函数实际上和另外二者关系不大，但名字使其很容易混淆。偏函数通常随着不完全的模式匹配出现。例如，集合的 map 和 collect 方法：\nList(1, 2, \u0026#34;a\u0026#34;) map {case i: Int =\u0026gt; i + 1} // scala.MatchError: a (of class java.lang.String) // at .$anonfun$res17$1(\u0026lt;console\u0026gt;:12) // at scala.collection.immutable.List.map(List.scala:286) // ... 28 elided List(1, 2, \u0026#34;a\u0026#34;) collect {case i: Int =\u0026gt; i + 1} // List(2, 3) 抛出的 MatchError 意味着这个模式匹配无法处理 String 类型的对象。而在 collect 方法中，我们选择了将偏函数无法处理的部分直接抛弃。设计上来说，map 接收的参数类型为 A =\u0026gt; B，collect 接收的参数类型为 PartialFunction[A, B]。PartialFunction Trait 提供了 isDefinedAt 方法来判断是否能够处理。Scala 还提供了 andThen orElse 这样的方法来将偏函数连接起来。\ndef f: PartialFunction[Any, Int] = { case i =\u0026gt; 0 } def func: PartialFunction[Int, Int] = { case i if i \u0026gt; 0 =\u0026gt; i + 1 } List(-1, 1) map (func orElse f) // List(0, 2) "},{"id":45,"href":"/pl/monad/","title":"Scala: Monad, from Scala Perspective","section":"Programming Languages","content":" 从 Scala 视角看 Monad # 群 Group # 群是由一个集合与一个二元运算组成，满足四个性质：封闭性、结合律、单位元和逆元。以整数集 $\\mathbb{Z}$ 与加法运算组合起来的群为例：\n封闭性：所有集合内的元素经过二元运算得到的结果仍然在这个集合内。即，任意两个整数相加的结果仍为整数。 结合律：加法结合律 $(a+b)+c=a+(b+c)$。 单位元 / 幺元：存在一个元素与任意元素运算结果仍为那个元素。即，0 加任何整数结果都为那个整数。 逆元：对任意一个元素，总存在一个元素使得二者相运算结果为单位元。在这里即相反数。 如果省略逆元要求，则为一个幺半群（独异点，Monoid）。如果再省略单位元，则为一个半群（Semigroup）。从上面的结论可知，自然数与加法为一个幺半群，正整数与加法为一个半群。\n群的性质落实到程序设计中形成了一定操作的可能性。因为集合在运算上的封闭，op 的参数和返回值是同样的类型。基于集合的归约操作也依赖运算的性质。因为运算满足结合律，所以归约操作可以并行化。因为幺半群的操作有幺元，我们可以从一个起始点进行归约，也就是 Scala 中的 reduce 和 fold 方法。\n仍然以上面的加法为例。在求一个整数集合的和时，我们可以将集合拆分进行分布式计算，这是因为结合律。如果不使用 reduce 而使用 fold ,则需要一个初始值。这个初始值理所当然地就是 0，即运算的幺元。\n从程序设计的角度来讲，幺半群可以用这样的形式表示。其封闭性通过类型参数保证，而幺元的正确性则需要我们自己来保证。\ntrait Monoid[A] { def op(a1: A, a2: A): A def zero: A } val intAddMonoid = new Monoid[Int] { def op(a1: Int, a2: Int) = a1 + a2 def zero = 0 } val intMultiplyMonoid = new Monoid[Int] { def op(a1: Int, a2: Int) = a1 * a2 def zero = 1 } val stringMonoid = new Monoid[String] { def op(a1: String, a2: String) = a1 + a2 def zero = \u0026#34;\u0026#34; } def listMonoid[A] = new Monoid[List[A]] { def op(a1: List[A], a2: List[A]) = a1 ++ a2 def zero = Nil } def optionMonoid[A] = new Monoid[Option[A]] { def op(a1: Option[A], a2: Option[A]) = a1 orElse a2 def zero = None } 范畴 Category # 范畴有三个条件：一个范畴由一系列物件 Object 构成的类 $ob(C)$ 和物件之间的态射组成的类 $hom(C)$ 构成。每一个态射是一个物件指向另一个物件的保持结构的一种关系。态射可以复合，就是说，如果对物件 $a,b,c$，有 $f:a\\rightarrow b,g:b\\rightarrow c$，那么存在 $g\\circ f:a\\rightarrow c$。一个范畴还满足两条公理：\n这些态射满足结合律。$f\\circ(g\\circ h)=(f\\circ g)\\circ h$。 范畴存在单位元。即，对每一个物件 $x$，都有 $f:x\\rightarrow x$。 可以看到，范畴的定义和群有些相似，但比群要更抽象，条件更少。一系列物件不一定是一整个类，也可以是一个一般的集合（称小范畴）；可以是一个很具体的对象，也可以是抽象的类型或集合。一系列态射也很泛化，可以是各种类型的态射，每个态射间也不一定非常相似。\n例如，所有的群及群同态构成一个范畴 $Grp$。所有的集合及集合之间的全函数构成一个范畴 $Set$。所有预序关系（满足自反、传递）通过单调函数构成一个范畴 $Ord$。\n一个幺半群是一个小范畴。其中只有一个物件，即幺半群中的集合，态射当然也只有一个单位态射，由幺半群的集合的元素给出，态射的复合由运算符给出。\n一个有向图也是一个小范畴，其物件是图中的顶点，态射是有向图中的边。虽然并不是任意两个物件都能连接，但有向图的路径的确能够复合，满足结合律，也拥有单位态射。\n范畴的概念之所以难以理解，可能是因为其太过于\u0026quot;高阶\u0026quot;。时刻需要记住的是，范畴是一个抽象程度非常高，限制非常少的概念。\n落回到类型系统上。如果我们把一个函数看作一个类型到另一个类型的态射，那么一系列类型及它们之间的函数就是一个范畴。例如，整数类型，字符串类型和 toString 函数，加上两种类型的 identity 函数，构成一个范畴。\n函子 Functor # 这样做的目的是引入下一个概念：函子。函子是范畴到范畴之间的映射，也可以解释为小范畴范畴中的态射。花点时间思考一下这个概念：这个范畴中的物件为小范畴，态射为小范畴到小范畴的态射，也就是范畴到范畴之间的映射。函子被这样定义：\n设 $C,D$为范畴。\n对于 $C$ 中的每一个对象，函子 $F$，或者说这两个范畴之间的映射 $F$ 将 $\\forall X \\in C$映射至 $F(X)\\in D$。 对每个态射 $\\forall f:X\\rightarrow Y\\in C$ 映射至 $F(f):F(X)\\rightarrow F(Y)\\in D$。 $\\forall X\\in C$，很自然地，有 $F(\\mathbf{id}_X)=\\mathbf{id}_{F(X)}$。 $\\forall f: X\\rightarrow Y\\in C,g:Y\\rightarrow Z\\in C$，有 $F(g\\circ f)=F(g)\\circ F(f)$。 最后，由一个范畴映射到它本身的函子称作自函子。以所有的小范畴为物件，以函子为态射，也可以组成一个范畴，称小范畴范畴。自函子即是其中的单位态射。\n再次落回到类型系统上来。如果我们将一系列函数连接起来的类型称为一个范畴，那么函子就可以是这样的两个范畴之间的映射。依然是上面的例子，一个范畴由：\nInt String def toString(i: Int): String = i.toString def identity(i: Int): Int = i def identity(s: String): String = s 组成，另一个范畴可能由：\nList[Int] List[String] def mapToString(ints: List[Int]): List[String] = ints map toString def identity(ints: List[Int]): List[Int] = ints def identity(strs: List[String]): List[String] = strs 那么，这两个范畴可以用一个函子进行映射。前者可以抽象为 Identity 的范畴，后者抽象为 List 的范畴，而这个函子把 Int 和 String 装进列表，并将对应的态射映射为对应的集合版本。\n可以看到，我们真正要做的重要的事情是，把 toString 映射成 mapToString。不过，如果用代码来实现的话，我们就没有必要用 Int 和 String 来限制自己了。一个 List 的函子可以描述为：\nobject ListFunctor { def map[A, B](fa: List[A], f: A =\u0026gt; B): List[B] = fa map f } // 使用函子 ListFunctor.map[Int, String](List(1, 2, 3), _.toString) ListFunctor.map(List(1, 2, 3), (i: Int) =\u0026gt; i.toString) 在上面的调用中，参数 f 就是我们要映射的范畴中的态射。如果我们再将函子抽象一下的话，就需要使用高阶类型：\ntrait Functor[F[_]] { def map[A,B](fa: F[A], f: A=\u0026gt;B): F[B] } 这样，无论 F 是 Option Array 还是其他类型，我们都可以为它们定义适合的 map 函数。当然，这里并没有为范畴中的物件做映射。在我们这个例子里，这件事由List.apply 完成。这个通用的函子 Trait 能够将类型 A B 所在的范畴映射到 F[A] F[B] 所在的范畴。\n剩下的问题是，为每一种类型映射（通常是泛型）都定义一个函子似乎太过麻烦。实际上，我们这里的 map 函数和 List 类里自带的 map 并没有什么区别。也就是说，List 这样的自带 map 函数，能够将一个范畴里类型间的态射转换成另一个范畴里类型间的态射（这里是从 Int =\u0026gt; String 映射为 List[Int] =\u0026gt; List[String]）就是一个函子。Scala 里的很多容器都是函子，这些容器通常也直接用 apply 方法提供了类型的映射。对于态射的组合，我们则可以通过链式的 map 或者对参数使用 andThen 来解决。\n// 并非真实的标准库 object List{ def apply(elem: A) = new List(elem) //映射物件 } class List[A] { def map[B](f: A =\u0026gt; B): List[B] = ... //映射态射 } List(1).map(_.toString) List(1).map((i: Int) =\u0026gt; i.toString) 函子与逆变、协变 # 函子相关的还有两个概念：协变函子和反变函子（逆变函子）。上面描述的即为协变函子，反变函子则是将态射的方向反转，即：\n$\\forall f:X\\rightarrow Y,F(f):F(Y)\\rightarrow F(X)$ $\\forall f: X\\rightarrow Y\\in C,g:Y\\rightarrow Z\\in C,F(g\\circ f)=F(f)\\circ F(g)$ 即，反变函子将态射的方向反转过来。\n我们已经知道容器是一个典型的函子。如果我们将一个范畴中类型的继承关系作为态射，那么函子对继承关系的映射的结果应当是这些类型的对应容器类的继承关系，于是我们就能够联想到我们熟悉的泛型类型的逆变和协变。显然，容器是协变函子意味着容器是协变的，容器是逆变函子意味着容器是逆变的。上面的 List 就是一个协变的容器：\nclass List[+A] { def map[B](f: A =\u0026gt; B): List[B] = ... } 在 TypeLevel 提供的 cats 库中，提供了逆变函子的定义，其对应的 contramap 函数定义如下：\ndef contramap[A, B](fa: F[A])(f: B =\u0026gt; A): F[B] 这个函数将 A =\u0026gt; B 映射为 F[B] =\u0026gt; F[A]，典型的例子如 Ordering.on ：\ndef on[U](f: U =\u0026gt; T): Ordering[U] = new Ordering[U] { def compare(x: U, y: U) = outer.compare(f(x), f(y)) } 这个函数将 U =\u0026gt; T 映射为 Ordering[T] =\u0026gt; Ordering[U]。\n自函子范畴上的幺半群 # 从 $C$ 到 $D$ 的函子范畴，记作 $[C, D]$，是以所有的协变函子 $F:C\\rightarrow D$ 为对象，以函子之间的自然变换为态射的范畴。如果 $C,D$ 是同一个范畴，那么函子 $F$ 就是自函子，这个范畴也就是自函子范畴。\n如果 $C,D$ 是范畴，$F,G$ 是二者之间的函子。那么 $\\forall X\\in C$，给出一个在 $D$ 的对象间的态射： $\\eta_X:F(X)\\rightarrow G(X)$，称为 $\\eta$ 在 $X$ 处的分量，使得 $\\forall f:X\\rightarrow Y\\in C$，有 $\\eta_Y\\circ F(f)=G(f)\\circ\\eta_X$。\n解释这个概念时常用 Haskell 中的概念 Hask 范畴。这个范畴的物件是 Haskell 中所有的类型，态射是所有的全函数，态射的复合类似于 andThen。对应到 Scala，也就类似于我们上面的 Int String List[Int] List[String] List[List[Int]] 等等所有类型组成的范畴。于是，我们上面的 ListFunctor OptionFunctor 等等函子就全部都是 Hask 范畴上的自函子。在这个范畴上看自然变换，例如，取 $X$ 为 Int $Y$ 为String $F$ 为 List $G$ 为 Option，那么相应地，$f$ 为 Int.toString，$F(f)$ 和 $G(f)$ 就是 List[Int].map(_.toString) 和 Option[Int].map(_.toString)。于是，自然变换 $\\eta$ 就是一个从 List 容器到 Option 容器的映射 List[T] =\u0026gt; Option[T]。\n于是，以自函子为物件、以自然变换为态射可以构成一个范畴，也就是自函子范畴。这时，我们在范畴这个概念的基础上又抽象了一层。\n那么，我们希望这个范畴具有什么样的性质，或者说我们希望这些态射是什么样的呢？在这里，对于函子 $M$，我们希望单位态射 $\\eta:1_H\\rightarrow M$，（这里的 $1_H$ 表示 范畴中的物件 $H$ 的单位态射）态射的复合 $\\mu：M\\circ M\\rightarrow M$。符合这样性质的自函子就是单子 Monad。（由于单子是一个自函子，所以只需要一个态射及其复合即可定义它。）显然，这个复合操作完美地复合封闭性、结合律和单位元。因此，单子是一个自函子范畴上的幺半群。\n用 Monad 解决回调地狱 # 考虑一个简单的除法函数：\ndef safeDiv(a: Double, b: Double) = b match { case 0 =\u0026gt; None case _ =\u0026gt; Some(a / b) } 如果我们想对一系列数字做链式的除法，会发生什么？可能有这样的两种实现：\nsafeDiv(6, 3) match { case None =\u0026gt; None case o =\u0026gt; safeDiv(2, o.get) } safeDiv(6, 3).map(safeDiv(2, _)) // type: Option[Option[Double]] 注意这里的 Option 是一个函子，在第二种实现中，我们使用函子的 map 解决了 safeDiv 无法适用于 Option 类型的问题。显而易见，无论哪一种体验都不会太好。所以，我们可以定义这样一个方法，将我们从多层嵌套的 Option 中解救出来：\ndef joinOption[T](o: Option[Option[T]]) = o match { case None =\u0026gt; None case _ =\u0026gt; o.get } joinOption(safeDiv(6, 3).map(safeDiv(1, _))) // type: Option[Double] implicit class JoinableOption[T](o: Option[Option[T]]) { def join = o match { case None =\u0026gt; None case _ =\u0026gt; o.get } } safeDiv(6, 3).map(safeDiv(2, _)).join.map(safeDiv(1, _)).join // type: Option[Double] 这里的 join 就实现了我们上面提到的态射的复合：$M\\circ M\\rightarrow M$。也就是说，一个 Option 紧接着一个 Option 的结果仍然是一个 Option。因此，Option 是一个单子。\n我们对这个 join 还是比较满意的。那么，按照上面的例子，如果这里的容器是 List，join 应该怎样实现呢？这个函数应当把两层嵌套的 List 变成单层的 List，也就是我们熟悉的 flatten。flatten 和 map 结合起来，我们发现实际上这种情况需要的是 flatMap。\ndef explode3(i: Int) = List(i, i, i) List(1, 2, 3).map(explode3).flatten // type: List[Int] List(1, 2, 3).flatMap(explode3) // type: List[Int] safeDiv(6, 2).flatMap(safeDiv(2, _)) // type: Option[Double] 同时，我们也印证了之前的想法：flatMap 的抽象是一种 Monad。在 Haskell 中，则是实现 Join，即 \u0026gt;\u0026gt;=（bind）算符，即成为一个 Monad。要定义一个严格的 Monad，我们通常需要它拥有两种操作：\ntrait Monad[M[_]] { def unit[A](a: A): M[A] //identity def join[A](mma: M[M[A]]): M[A] } // or, another expression trait Monad[M[_]] { def unit[A](a: A): M[A] def flatMap[A, B](fa: M[A])(f: A =\u0026gt; M[B]): M[B] } // or, another way trait Monad[M[_]] { def unit[A](a: A): M[A] def compose[A, B, C](f: A =\u0026gt; M[B], g: B =\u0026gt; M[C]): A =\u0026gt; M[C] } 无论哪一种，都能够体现 $M\\circ M\\rightarrow M$。\n或者说，对于我们一直使用的容器类型的例子，取 M 为 List，这个容器应该具有：\nobject List[T] { def apply(elem: T): List[T] = ??? } class List[T] { def flatMap[B](f: T =\u0026gt; List[B]): List[B] = ??? } 当然，真实的 Scala 库使用了更加泛用的写法。\n更重要的是，在 Haskell 和 Scala 中都为 Monad 提供了语法糖（do 和 for），以优雅地解决这里的拆包问题。\nval o = Some(Some(Some(1))) for { x \u0026lt;- o; y \u0026lt;- x; z \u0026lt;- y } yield z // results Some(1), in order to keep structure like o for { x \u0026lt;- o; y \u0026lt;- x; z \u0026lt;- y } println(z) // results 1, for z is an Int(1) 注意，这里 z 的值是 Int 1，但如果使用 yield，将会产生一个 Some(1)。这是因为 yield 的通常做法是从一个集合生成另一个集合，所以进行了一次打包。如果用 List 能够描述得更清楚：\nval l = List(List(1, 2), List(3, 4)) for {x \u0026lt;- l; y \u0026lt;- x} yield y // results List(1, 2, 3, 4) Haskell 中的语法糖也类似：\nsafeDiv a b \u0026gt;\u0026gt;= (\\x -\u0026gt; safeDiv c x \u0026gt;\u0026gt;= (\\y -\u0026gt; safeDiv d y \u0026gt;\u0026gt;= (\\z -\u0026gt; safeDiv e z))) al = do x \u0026lt;- safeDiv a b y \u0026lt;- safeDiv c x z \u0026lt;- safeDiv d y safeDiv e z Functor, Applicative, Monad # 现在我们从另一个方向来理解 Monad 等概念。在前面的例子中，我们都是使用的\u0026quot;容器\u0026quot;来理解，但实际情况中完全可以不只是容器。Haskell 世界常用上下文 Context 来表示这个概念。在 Scala 世界中，也不仅仅容器可以接收类型参数。首先回顾一下 Functor 和 Monad，在下面的描述中，我们都省略比较显而易见的 unit 的定义：\n函子解决了这样一个问题，将一个完全 Context 外（即接收 Context 外值并返回 Context 外的值）的函数应用在一个 in Context 的值上（例如容器中的值），得到一个 in Context 的值，表现为 map 方法。如果将 Context 看做一个盒子，那么就相当于在 map（Haskell 中的 fmap）的内部，将盒子中的值拿出来，应用函数，再装回同样的盒子里去。\ndef map[A, B](a: F[A])(f: A =\u0026gt; B): F[B] // A =\u0026gt; B ➡️ F[A] =\u0026gt; F[B] 单子解决了这样一个问题，将一个本来接收 Context 外的值，而返回 in Context 值的函数，应用到一个 in Context 的值上，并返回一个\u0026quot;单层的\u0026quot; in Context 的值。如果使用盒子比喻的话，那么相当于减少了盒子的层数。\ndef join[A](a: M[M[A]]): M[A] // flatten def flatMap[A, B](fa: M[A])(f: A =\u0026gt; M[B]): M[B] // map + join // A =\u0026gt; M[B] ➡️ M[A] =\u0026gt; M[B] def compose[A, B, C](f: A =\u0026gt; M[B], g: B =\u0026gt; M[C]): A =\u0026gt; M[C] // unit + map + join // A =\u0026gt; M[B] =\u0026gt; M[M[C]] ➡️ A =\u0026gt; M[C] 我们之前已经知道单子是一个函子。因为只需要满足单子所需的条件，就已经是一个函子，所以，map 完全可以通过 flatMap 来实现：\ndef map[A, B](fa: F[A])(f: A =\u0026gt; B): F[B] = flatMap(fa)((a: A) =\u0026gt; unit(f(_))) 现在我们引入另外一个概念，可应用函子 Applicative。数学的角度上并不常提起它，编程的角度上则重要一些。\n可应用函子解决了这样一个问题：将一个 in Context 的函数，应用到一个 in Context 的值上。也就是：\ndef apply[A, B](fab: F[A =\u0026gt; B])(fa: F[A]): F[B] // F[A =\u0026gt; B] ➡️ F[A] =\u0026gt; F[B] def map2[A, B, C](fa: F[A], fb: F[B])(f: (A, B) =\u0026gt; C): F[C] apply 函数非常符合我们的定义，map2 则不是非常直观。更广泛地说，任意数量参数的 map 都属于 Applicative 的管辖范围。通过柯里化，我们能够从 apply 构造 map2。在柯里化的语境下，(A, B) =\u0026gt; C 同时也是一个 A =\u0026gt; (B =\u0026gt; C)。所以有：\ndef map2[A, B, C](fa: F[A], fb: F[B])(f: (A, B) =\u0026gt; C): F[C] = { val fb2c: F[B =\u0026gt; C] = apply(unit(f))(fa) // apply[A, B] val fc = apply(fb2c)(fb) // apply[B, C] fc } 可以看出的是，在面向对象的世界中可应用函子并不常见，一个 apply 形式的场景是从高阶函数中获得一个用 Option 包裹起来的函数，并应用到另一个 option 值上。相比之下，map2 的用法要常见得多，但经常是将多个容器 zip 起来以元组作为参数列表，再利用 lambda 表达式将 map2 变成 map。\n这时回头看，我们发现单子同时也是一个可应用函子，因为 map2 完全可以用 flatMap 实现：\ndef map2[A, B, C](fa: F[A], fb: F[B])(f: (A, B) =\u0026gt; C): F[C] = flatMap(fa)(a =\u0026gt; map(fb)(b =\u0026gt; f(a, b))) 回到 Scala 的定义中，我们得到了：\ntrait Functor[F[_]] { def unit[A](a: A): F[A] def map[A, B](f: A =\u0026gt; B): F[B] } trait ApplicativeFunctor[F[_]] extends Functor { def apply[A, B](f: F[A =\u0026gt; B])(fa: F[A]): F[B] } trait Monad[M[_]] extends ApplicativeFunctor { def join[A](mma: M[M[A]]): M[A] } 用 Monad 隔离副作用 # IO 是 Haskell 中的一个 Monad（Haskell 选择将 Monad 显式地定义出来）。具体来说，Haskell 中的Monad 是一个 type class，可以暂时简单地理解为类似 interface 或 trait 的对方法的抽象，只是更加灵活。为了保证函数的纯正性，如果参数中包括 IO，Haskell 就要求返回值必须存在 IO。因此这样的\u0026quot;伪装\u0026quot;变得不可能：\nChar -\u0026gt; Char = (Char -\u0026gt; IO Char) . (IO Char -\u0026gt; Char) 这里的 . 表示函数的复合，即我们之前说的态射的复合，大致相当于 Scala 中的 andThen。\n通过对 IO 的这种要求，Haskell 保证将纯函数和非纯函数区分开来。不过，由于 IO Char 和 Char 并不一样，我们需要一种方式让真对 Char 的函数能够作用在 IO Char 上。这样，这种情况就十分类似于我们之前的 in Context 问题了。\n在 Haskell 的 Monad 中，unit 操作叫做 return，join 或 flatten 操作叫做 \u0026gt;\u0026gt;=（bind）。让我们先从 IO Monad 的实际使用开始：\ngetChar :: IO Char putChar :: Char -\u0026gt; IO () echo = (getChar \u0026gt;\u0026gt;= putChar) :: IO () 将这个过程翻译成对等的 Scala，大致相当于：\ndef getChar: IO[Char] = Some(\u0026#39;a\u0026#39;) // IO[Char] def putChar(c: Char): IO = { println(c); None } // Char =\u0026gt; IO def echo = getChar flatMap putChar // IO 因为 Haskell 中无法对 IO \u0026ldquo;拆包\u0026rdquo;，所以包含了 IO 的代码块就会始终包含着 IO。同时，通过 Monad，我们能够把不在 IO Context 内的函数应用到 IO 的变量身上。\n所以，Haskell 对副作用的处理总结起来是这样的：将 IO 作为一个\u0026quot;标签\u0026quot;打在过程上，并且凡是和副作用有关的上层函数都会被打上这个标签，当遇到普通的纯函数时，通过函子的特性将函数应用在实际的变量上，并在结果中保留 IO 标签。特殊情况是，当遇到另外一个同样有副作用的函数时，就会出现两个 IO \u0026ldquo;标签\u0026rdquo;。这时，通过 Monad 的特性将 IO 标签限制到一个。同时，由于 Monad 没有定义 IO -\u0026gt; ()，所以打上 IO 标签的函数永远不可能变回纯函数。\n"},{"id":46,"href":"/distributed/spark-rdd/","title":"Spark RDD Programming","section":"Distributed Systems","content":" Spark RDD 编程 # 使用数据集 # 并行化集合 # 可以使用 Spark Context 的方法对数组进行并行化，以在其上并行地进行操作。\nval data = Array(1, 2 ,3, 4, 5) val distData = sc.parallelize(data) distData.reduce((a, b) =\u0026gt; a + b) sc.parallelize(data, 10) // 10 partitions 进行并行化的一个重要参数就是将集合进行切分的分区（Partition）数量。通常，比较好的数字是每个逻辑核心 2 ~ 4 个分区，Spark 会自动进行划分。如上，也可以手动指定分区的数量。\n外部数据集：文本文件 # Spark 可以为任何被 Hadoop 支持的存储方式上创建分布式的数据集，包括 HDFS，Cassandra，Hbase，Amazon S3，以及本地存储等等。它也支持文本文件或 Hadoop 的各种 InputFormat。\n文本文件的数据集可以使用 Spark Context 的 textFile 方法来读取。这个方法接收一个文件的 URI，并作为行的集合读取进来。\nval distFile = sc.textFile(\u0026#34;data.txt\u0026#34;) // distFile: org.apache.spark.rdd.RDD[String] = data.txt MapPartitionsRDD[10] at textFile at \u0026lt;console\u0026gt;:26 distFile.map(line =\u0026gt; line.length).reduce((a, b) =\u0026gt; a + b) // full length of file 如果读取本地文件系统的文件，那么集群的所有结点都必须拥有读取这个文件的权限。可以把这个文件拷贝到所有结点的同一位置，或者将文件挂载到网络上来共享。 Spark 的所有基于文件的输入方法，包括这里的 textFile，都支持目录、通配符和 gz 压缩文件。 textFile 方法也可以接收一个参数，来控制文本文件的分区数量。Spark 默认为文件的每个 Block 创建一个分区，在 HDFS 上默认是 128 MB。你可以传入一个更大的值使分区的数量更多，但不能少于 block 的数量。 其他数据格式 # sc.wholeTextFiles 可以读取一个包含许多小文本文件的目录，将每一个以 filename, content 元组返回。相比之下，textFile 是以行为单位进行读取。这个方法也提供了第二个参数来控制分区的最少数量。 对于 Hadoop 生成的二进制键值对形式的 Sequence 文件，使用 sc.sequenceFile[K, V] 来读取。其中，K 和 V 都需要是 Hadoop 的 Writable 的子类，例如 IntWritable 和 Text。Spark 允许使用原生类型来代替几种常见的 Writable，例如对于一个 IntWritable 和 Text 的序列文件，也可以使用 sequenceFile[Int, String]，Spark 会进行自动转换。 对于其他的 Hadoop InputFormat，可以使用 sc.hadoopRDD 方法来读取。它接收一个任意的 JobConf 和 InputFormat 类，key 类和 value 类。对于新的 MapReduce API（org.apache.hadoop.mapreduce），使用 sc.newAPIHadoopRDD。 rdd.saveAsObjectFile 和 sc.objectFile 可以保存和读取一个 RDD，使用的是简单的序列化 Java 对象。虽然这样不算高效，但足够简单。 RDD 操作 # Transformations # 转换操作将一个数据集转换为另一个数据集，主要包括各种 map filter，针对键值对的 reduceByKey groupByKey sortByKey aggregateByKey，对集合进行转换的 union join，以及处理分区数量的 repartition coalesce 等方法。这些操作都会是懒加载的。也就是说，在遇到 Action 之前，Spark 都只记录操作而并不真正计算。\n这样的缺点是，对于不同的 action，可能需要重新计算转换过程。因此，可以使用 persist 和 cache 方法将 RDD 持久化到内存中。当然，也可以持久化到磁盘，或复制到多个节点等。\nval lines = sc.textFile(\u0026#34;data.txt\u0026#34;) val lineLengths = lines.map(s =\u0026gt; s.length).persist() val totalLength = lineLengths.reduce((a, b) =\u0026gt; a + b) Action # Action 通常会将集合转换为一个非集合的值，或者将集合收集回 Driver。包括 reduce collect first take countByKey 等取得元素或计算得到值的方法，以及各种 saveAsTextFile 类似的保存方法。\n同时，Spark 也提供了一些方法的异步版本。例如 foreachAsync 返回一个 FutureAction，而不会阻塞在运算过程中。\n传递函数 # 向 map 这一类方法传递函数的方式有几种。可以和上面一样使用匿名函数，也可以传递已有的方法，例如单例对象中的方法。\nobject MyFunctions { def func1(s: String): String = { ... } } rdd.map(MyFunctions.func1) 也可以传递对象中的实例方法。这时，这个对象就会被发送到整个集群。\nclass MyClass { val field = \u0026#34;Hello\u0026#34; def doStuff(rdd: RDD[String]): RDD[String] = rdd.map(x =\u0026gt; field + x) } 显然，将这个 MyClass 对象发送给整个集群很可能是不太合适的。因此，更好的方法是把这一类变量保存下来。\ndef doStuff(rdd: RDD[String]): RDD[String] = { val field_ = this.field rdd.map(x =\u0026gt; field_ + x) } 这样，就只有 field_ 需要被发送到整个集群。\n理解闭包 # var counter = 0 sc.parallelize(data).foreach(x =\u0026gt; counter += x) // WRONG 显然，这样的代码是存在冲突的，其结果是不确定的。在每一个 task 执行之前，Spark 都要计算这个任务的闭包（Closure），也就是这个过程所需要访问的外部变量和函数。闭包会被序列化并发送到集群的每一个 executor。\n因此，所有的 executor 处理的都是其内部自己的 counter 变量的副本，而不是我们定义的 Driver 里的 counter。当然，在 Local 模式下，如果 executor 和 Driver 使用同一个 JVM，这个值是有可能变化的。但无论如何都不应该使用这种方法。\n这种时候，适合使用累加器 Accumulator。Spark 会为这个累加器提供安全更新变量的机制。具体做法我们会在后面讨论。\n类似的情况还出现在打印 RDD 的元素时。由于同样的原因，直接使用 foreach 进行打印，会在集群的每一台节点上分别打印其正在处理的元素，这显然不是我们想要的。正常的方法是使用 collect 把 RDD 收集回到 Driver 上再进行打印。如果数据集很大，不能放在一台机器上，更好的方法是使用 take 取出一部分来检查。\nrdd.collect().foreach(println) rdd.take(100).foreach(println) 使用键值对 # 如果 RDD 的泛型类型是一个二元元组（Tuple），那么 RDD 会将其作为键值对来处理。例如，对于简单的 LineCount 程序：\nsc.textFile(\u0026#34;data.txt\u0026#34;).map(s =\u0026gt; (s, 1)) .reduceByKey((a, b) =\u0026gt; a + b) .sortByKey().collect() 与 Java 一样，由于键值对的操作中需要对元素进行比较，key 必须要提供匹配的 hashCode 和 equals 方法。\nShuffle # Background # Shuffle 是 Spark 重新分配数据的一种方式。这个过程通常需要在结点之间互相交换数据，因此这个过程相对代价较高。\n以 reduceByKey(func) 为例。这个方法返回一个 RDD，其内容为一些形如 (key, reducedValue) 的 Tuple。虽然操作完成后各个分区的内容是确定的，分区之间的顺序也是确定的，但分区内部元素的顺序并不确定。可以使用几种方法：\nmapPartitions 来对每个分区分别进行排序 repartitionAndSortWithinPartitions 直接完成重分区和排序 sortBy 来使整个 RDD 有序。 能够触发 Shuffle 的操作包括 rePartition coalesce，除 counting 之外的各种 ByKey 操作，以及 join 和 cogroup。\n性能影响 # Shuffle 操作主要的代价涉及到磁盘 IO、数据序列化、网络 IO 三个部分。这时，Spark 系统会建立一批 map task 和 reduce task。这两个名词来自 MapReduce，和 Spark 的两个方法没有关系。\n在系统内部，每个独立的 map task产生的结果都被保存在内存里，直到放不下为止。然后，这些数据会根据目标分区进行排序并写到磁盘上的一个文件。最后，reduce task 读取磁盘，将数据恢复。\n一些 Shuffle 方法会消耗大量的堆内存，因为它们需要采用一些数据结构来组织这些数据。例如，reduceByKey 和 aggregateByKey 会在 map task 过程中创建这些数据结构，而另一些其他 ByKey 操作则会在 Reduce 端创建这些数据结构（如 sortByKey）。当内存不足时，就会需要大量的磁盘 IO 和 GC 操作。\n此外，这个过程还会在磁盘上产生大量的中间文件，有些类似于 Hadoop 的形式。在 Spark 1.3 之后，这些文件会一直保留到 RDD 被 GC，无法再使用为止。这是为了能够方便地计算 RDD 的\u0026quot;血统\u0026quot;（lineage）。那么，如果 RDD 被使用的时间很长，或者 GC 不频繁，这些文件就会占据相当多的磁盘空间。临时存储路径定义在 SparkContext 的 spark.local.dir 属性。\nRDD 持久化 # 首先看 RDD.scala源码与持久化相关的部分：\nprivate def persist(newLevel: StorageLevel, allowOverride: Boolean): this.type = { // TODO: Handle changes of StorageLevel if (storageLevel != StorageLevel.NONE \u0026amp;\u0026amp; newLevel != storageLevel \u0026amp;\u0026amp; !allowOverride) { throw new UnsupportedOperationException(\u0026#34;...\u0026#34;) } // If this is the first time this RDD is marked for persisting // register it // with the SparkContext for cleanups and accounting. Do this only once. if (storageLevel == StorageLevel.NONE) { sc.cleaner.foreach(_.registerRDDForCleanup(this)) sc.persistRDD(this) } storageLevel = newLevel this } def persist(newLevel: StorageLevel): this.type = { if (isLocallyCheckpointed) { persist(LocalRDDCheckpointData.transformStorageLevel(newLevel), allowOverride = true) } else { persist(newLevel, allowOverride = false) } } def persist(): this.type = persist(StorageLevel.MEMORY_ONLY) def cache(): this.type = persist() 可以看到，persist 和 cache 实际上调用的是同一个过程。调用这个方法之后，RDD 并不会被立刻计算，它仍然需要 Action 才能真正触发计算。只不过，计算完成后并不会被清除，而是保留在内存中以备未来使用。\nSpark 的缓存是容错的：如果某个分区丢失了，仍然可以再次计算出来。这种情况会出现在内存不足的时候，我们在上面使用 sc.cleaner.foreach(_.registerRDDForCleanup) 就是为了允许 Spark 清除这个缓存。Spark 会以 LRU 的方式管理这些缓存。当然，也可以手动调用 unpersist。\n每个 RDD 都有不同的 Storage Level。默认使用的是 MEMORY_ONLY，这意味着缓存只以 Java 对象形式存在于内存中。这意味着，我们认为重新计算数据的代价比磁盘 IO 和反序列化更小（通常并非如此）。\nStorage Level Meaning MEMORY_ONLY 仅内存 MEMORY_AND_DISK 内存和磁盘 MEMORY_ONLY_SER 在内存中以序列化后的 byte[] 形式存在 MEMORY_AND_DISK_SER DISK_ONLY MEMORY_ONLY_2 and etc. 为每个分区在集群上建立两个副本 OFF_HEAP 仅内存序列化，使用 off-heap 内存（需启用） Python 中能够使用的 Level 有所不同。\n在 Shuffle 操作中，Spark 会自动使用持久化，即使我们不去主动指定。这样做的目的是，在某个节点运行失败时，不需要计算所有的输入数据。总之，只要我们可能重复使用同一个 RDD，最好就对其进行持久化。\n共享变量 # 上面我们已经提到，在 map 等方法中使用的变量，需要被复制到整个集群的所有节点中去。并且，这些变量的更新并不会被传回 Driver。为了解决这个问题，Spark 提供了广播变量和累加器两种解决办法。\n广播变量 # 广播变量允许我们将一个只读的变量缓存到每一台机器上去。例如以一种比较高效的方式将一份相对稍大的数据集给每一个节点一个副本。Spark 会尝试使用相对高效的算法来降低传输代价。\nSpark 的操作会被 Action 和 Shuffle 划分为很多个 Stage。Spark 会自动将每个 Stage 内任务所需要的公共数据进行广播，方式是通过序列化。这意味着，在许多个 Stage 都需要使用同一份数据时，或者使用这些反序列化的数据非常重要时，显式创建广播变量的效果才会比较好。\nval broadcastVar = sc.broadcast(Array(1, 2, 3)) // broadcastVar: org.apache.spark.broadcast.Broadcast[Array[Int]] = Broadcast(0) broadcastVar.value // res0: Array[Int] = Array(1, 2, 3) 创建之后，变量 v 只需要被传输一次就足够了。另外，为了保证所有结点都能取到相同的值，变量 v 在广播后不应该再被修改。\n累加器 Accumulator # 累加器常常用来实现计数器或求和。Spark 原生支持数值型 long 和 double 的累加，而且我们可以实现自己的累加器类型。建立之后，每个任务都可以对累加器进行加操作，但只有 Driver 能够读取值。\n累加器甚至会出现在 Web UI 当中（因为其作为计数器的作用）。Tasks 表中将会显示每个任务对累加器的修改，而其最终值将会出现在 Web UI 上面。\nval accum = sc.longAccumulator(\u0026#34;My Accumulator\u0026#34;) // accum: org.apache.spark.util.LongAccumulator = LongAccumulator(id: 0, name: Some(My Accumulator), value: 0) c.parallelize(Array(1, 2, 3, 4)).foreach(x =\u0026gt; accum.add(x)) // 10/09/29 18:41:08 INFO SparkContext: Tasks finished in 0.317106 s accum.value // res2: Long = 10 data.map { x =\u0026gt; accum.add(x); x } // now accum is still 0 我们可以通过继承 AccumulatorV2 类来实现自己的累加器，只需要实现一些方法。\nclass VectorAccumulatorV2 extends AccumulatorV2[MyVector, MyVectot] { private val myVector: MyVector = MyVector.createZeroVector override def isZero: Boolean = ??? override def copy(): AccumulatorV2[MyVector, MyVector] = ??? override def reset(): Unit = ??? override def add(v: MyVector): Unit = ??? override def merge(other: AccumulatorV2[MyVector, MyVector]): Unit = ??? override def value: MyVector = ??? } 累加器也允许返回值类型与添加的元素类型不一致。\n累加器的更新只发生在 Action 操作中。或者说，对累加器的更新只会在 Action 处被同步到累加器对象。因此，带有累加器的操作仍然是懒加载的。\n"},{"id":47,"href":"/distributed/spark-sql/","title":"Spark SQL Programming","section":"Distributed Systems","content":" Spark SQL Programming # Basic # DataFrame # # standalone from pyspark.sql import SparkSession spark = SparkSession \\ .builder \\ .appName(\u0026#34;Python Spark SQL basic example\u0026#34;) \\ .config(\u0026#34;spark.some.config.option\u0026#34;, \u0026#34;some-value\u0026#34;) \\ .getOrCreate() # in pyspark repl spark = SQLContext(sc) # json file content: # {\u0026#34;name\u0026#34;:\u0026#34;Michael\u0026#34;} # {\u0026#34;name\u0026#34;:\u0026#34;Andy\u0026#34;, \u0026#34;age\u0026#34;:30} # {\u0026#34;name\u0026#34;:\u0026#34;Justin\u0026#34;, \u0026#34;age\u0026#34;:19} df = spark.read.json(\u0026#34;examples/src/main/resources/people.json\u0026#34;) # missing value is null df.show() df.printSchema() df.select(\u0026#34;name\u0026#34;).show() # prints a column of data df.select(df[\u0026#39;name\u0026#39;], df[\u0026#39;age\u0026#39;] + 1).show() # prints two columns, one of\u0026#39;em is operated df.filter(df[\u0026#39;age\u0026#39;] \u0026gt; 21).show df.groupBy(\u0026#34;age\u0026#34;).count().show() SQL Queries # # temp view df.createOrReplaceTempView(\u0026#34;people\u0026#34;) spark.sql(\u0026#34;SELCT * FROM people\u0026#34;).show() # global temp view df.createGlobalTempView(\u0026#34;people\u0026#34;) spark.newSession().sql(\u0026#34;SELECT * FROM global_temp.people\u0026#34;).show() Schema # Infering Schema # from pyspark.sql import Row # reading text file as RDD # text file content: # Michael, 29 # Andy, 30 # Justin, 19 lines = sc.textFile(\u0026#34;examples/src/main/resources/people.txt\u0026#34;) parts = lines.map(lambda l: l.split(\u0026#34;,\u0026#34;)) people = parts.map(lambda p: Row(name=p[0], age=int([[1]]))) # RDD[Row] schemaPeople = sql.createDataFrame(people) # transform into sql dataframe schemaPeople.createOrReplaceTempView(\u0026#34;people\u0026#34;) teenNames = spark.sql(\u0026#34;SELECT name FROM people WHERE age \u0026gt;= 13 AND age \u0026lt;= 19\u0026#34;) teenNames.rdd.map(lambda p: \u0026#34;name: \u0026#34; + p.name).collect() # RDD[String] Programatically Specifying the Schema # Create an RDD of tuple or lists. (like RDD[Tuple2]) Create the schema as a StructType matching these tuples Apply the schema via createDataFrame from pyspark.sql.types import * people = sc.textFile(\u0026#34;examples/src/main/resources/people.txt\u0026#34;) \\ .map(lambda l: l.split(\u0026#34;,\u0026#34;)) \\ .map(lambda p: (p[0], p[1].strip())) # RDD[(String, String)] schemaString = \u0026#34;name age\u0026#34; # cloumn names string fields = [ StructField(field_name, StringType(), True) for field_name in schemaString.split() ] # One StructField is One field in structType # class StructField(name, dataType, nullable=True, metadata=None) # DataType is base class of datatypes, e.g. StringType, BinaryType, etc. schema = StructType(fields) schemaPeople = sql.createDataFrame(people, schema) User-defined Functions (UDFs) # spark.registerFunction( \u0026#34;CTOF\u0026#34;, lambda degreesCelsius: ((degreesCelsius * 9.0 / 5.0) + 32.0) ) spark.sql( \u0026#34;SELECT city, CTOF(avgLow) AS avgLowF, CTOF(avgHigh) AS avgHighF FROM citytemps\u0026#34; ).show() User-defined Aggregate Functions # import org.apache.spark.sql.{Row, SparkSession} import org.apache.spark.sql.expressions.MutableAggregationBuffer import org.apache.spark.sql.expressions.UserDefinedAggregateFunction import org.apache.spark.sql.types._ object MyAverage extends UserDefinedAggregateFunction { // The data type of the input value def inputSchema: StructType = StructType( StructField(\u0026#34;inputColumn\u0026#34;, LongType) :: Nil) // The data type of the aggregation buffer def bufferSchema: StructType = StructType( StructField(\u0026#34;sum\u0026#34;, LongType) :: StructField(\u0026#34;count\u0026#34;, LongType) :: Nil) // The data type of the returned value def dataType: DataType = DoubleType // Whether this function always returns the same output on the identical input def deterministic: Boolean = true // Initializes the given aggregation buffer. The buffer itself is a `Row` that // in addition to standard methods like retrieving a value at an index // (e.g., get(), getBoolean()), provides the opportunity to update its values. // Note that arrays and maps inside the buffer are still immutable. def initialize(buffer: MutableAggregationBuffer): Unit = { buffer(0) = 0L // sum buffer(1) = 0L // count } // Updates the given aggregation buffer `buffer` with new input data from `input` def update(buffer: MutableAggregationBuffer, input: Row): Unit = { if (!input.isNullAt(0)) { buffer(0) = buffer.getLong(0) + input.getLong(0) buffer(1) = buffer.getLong(1) + 1 } } // Merges two aggregation buffers and stores the updated buffer values back to `buffer1` def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = { buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0) buffer1(1) = buffer1.getLong(1) + buffer2.getLong(1) } // Calculates the final result def evaluate(buffer: Row): Double = buffer.getLong(0).toDouble / buffer.getLong(1) } val df = spark.read.json(\u0026#34;examples/src/main/resources/employees.json\u0026#34;) df.createOrReplaceTempView(\u0026#34;employees\u0026#34;) spark.udf.register(\u0026#34;myAverage\u0026#34;, MyAverage) spark.sql(\u0026#34;SELECT myAverage(salary) as average_salary FROM employees\u0026#34;) Or, a type-safe version, using pre-defined case classes as schema:\nimport org.apache.spark.sql.{Encoder, Encoders, SparkSession} import org.apache.spark.sql.expressions.Aggregator case class Employee(name: String, salary: Long) case class Average(var sum: Long, var count: Long) object MyAverage extends Aggregator[Employee, Average, Double] { // Should satisfy the property that any b + zero = b def zero: Average = Average(0L, 0L) // Combine two values to produce a new value. For performance, the function may // modify `buffer` and return it instead of constructing a new object def reduce(buffer: Average, employee: Employee): Average = { buffer.sum += employee.salary buffer.count += 1 buffer } // Merge two intermediate values def merge(b1: Average, b2: Average): Average = { b1.sum += b2.sum b1.count += b2.count b1 } // Transform the output of the reduction def finish(reduction: Average): Double = reduction.sum.toDouble / reduction.count // Specifies the Encoder for the intermediate value type def bufferEncoder: Encoder[Average] = Encoders.product // Specifies the Encoder for the final output value type def outputEncoder: Encoder[Double] = Encoders.scalaDouble } // a DataSet[Employee] val ds = spark.read.json(\u0026#34;examples/src/main/resources/employees.json\u0026#34;).as[Employee] val averageSalary = MyAverage.toColumn.name(\u0026#34;average_salary\u0026#34;) ds.select(averageSalary) Data Sources # df = spark.read.load(\u0026#34;examples/src/main/resources/users.parquet\u0026#34;) df.select(\u0026#34;name\u0026#34;, \u0026#34;favorite_color\u0026#34;).write.save(\u0026#34;namesAndFavColors.parquet\u0026#34;) df = spark.read.load(\u0026#34;examples/src/main/resources/people.json\u0026#34;, format=\u0026#34;json\u0026#34;) df.select(\u0026#34;name\u0026#34;, \u0026#34;age\u0026#34;).write.save(\u0026#34;namesAndAges.parquet\u0026#34;, format=\u0026#34;parquet\u0026#34;) df = spark.read.load( \u0026#34;examples/src/main/resources/people.csv\u0026#34;, format=\u0026#34;csv\u0026#34;, sep=\u0026#34;:\u0026#34;, inferSchema=\u0026#34;true\u0026#34;, header=\u0026#34;true\u0026#34; ) df = spark.sql(\u0026#34;SELECT * FROM parquet.`examples/src/main/resources/users.parquet`\u0026#34;) SaveMode # ErrorIfExists Append Overwrite Ignore Save to Persistent Table # df.write.option(\u0026#34;path\u0026#34;, \u0026#34;/some/path\u0026#34;).saveAsTable(\u0026#34;t\u0026#34;) df.write.bucketBy(42, \u0026#34;name\u0026#34;).sortBy(\u0026#34;age\u0026#34;).saveAsTable(\u0026#34;people_bucketed\u0026#34;) df.write.partitionBy(\u0026#34;favorite_color\u0026#34;).format(\u0026#34;parquet\u0026#34;).save(\u0026#34;namesPartByColor.parquet\u0026#34;) Hive # For Spark SQL to operate Hive, you need hive jars in classpath of every node and hive-site.xml core-site.xml hdfs-site.xml file in conf/ . When not configured by the hive-site.xml, the context automatically creates metastore_db in the current directory and creates a directory configured by spark.sql.warehouse.dir, which defaults to the directory spark-warehouse in the directory the Spark application starts.\nwarehouse_location = abspath(\u0026#39;spark-warehouse\u0026#39;) # text file content: # 238\u0001val_238 # 86\u0001val_86 # 311\u0001val_311 spark.sql(\u0026#34;CREATE TABLE IF NOT EXISTS src (key INT, value STRING) USING hive\u0026#34;) spark.sql(\u0026#34;LOAD DATA LOCAL INPATH \u0026#39;examples/src/main/resources/kv1.txt\u0026#39; INTO TABLE src\u0026#34;) spark.sql(\u0026#34;SELECT * FROM src\u0026#34;).show() spark.sql(\u0026#34;SELECT COUNT(*) FROM src\u0026#34;).show() sqlDF = spark.sql(\u0026#34;SELECT key, value FROM src WHERE key \u0026lt; 10 ORDER BY key\u0026#34;) # RDD[Row] stringsDS = sqlDF.rdd.map(lambda row: \u0026#34;Key: %d, Value: %s\u0026#34; % (row.key, row.value)).collect() # RDD[String] # Create a Spark dataframe Record = Row(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;) recordsDF = spark.createDataFrame([Record(i, \u0026#34;val_\u0026#34; + str(i)) for i in range(1, 101)]) recordsDF.createOrReplaceTempView(\u0026#34;records\u0026#34;) # Can also join DataFrame data with data stored in Hive. spark.sql(\u0026#34;SELECT * FROM records r JOIN src s ON r.key = s.key\u0026#34;).show() Hive Settings # CREATE TABLE src(id int) USING hive OPTIONS(fileFormat \u0026#39;parquet\u0026#39;) fileFormat: A fileFormat is kind of a package of storage format specifications, including \u0026ldquo;serde\u0026rdquo;, \u0026ldquo;input format\u0026rdquo; and \u0026ldquo;output format\u0026rdquo;. Currently we support 6 fileFormats: \u0026lsquo;sequencefile\u0026rsquo;, \u0026lsquo;rcfile\u0026rsquo;, \u0026lsquo;orc\u0026rsquo;, \u0026lsquo;parquet\u0026rsquo;, \u0026rsquo;textfile\u0026rsquo; and \u0026lsquo;avro\u0026rsquo;. inputFormat outputFormat: These 2 options specify the name of a corresponding InputFormat and OutputFormat class as a string literal, e.g. org.apache.hadoop.hive.ql.io.orc.OrcInputFormat. These 2 options must be appeared in pair, and you can not specify them if you already specified the fileFormat option. serde: This option specifies the name of a serde class. When the fileFormat option is specified, do not specify this option if the given fileFormat already include the information of serde. Currently \u0026ldquo;sequencefile\u0026rdquo;, \u0026ldquo;textfile\u0026rdquo; and \u0026ldquo;rcfile\u0026rdquo; don\u0026rsquo;t include the serde information and you can use this option with these 3 fileFormats. fieldDelim escapeDelim collectionDelim mapkeyDelim lineDelim: These options can only be used with \u0026ldquo;textfile\u0026rdquo; fileFormat. They define how to read delimited files into rows. broadcast hints Spark to broadcast the table to the cluster when join with others. Broadcast Hash Join is preferred but not guaranteed. When both side is broadcasted, Spark chooses the one with lower statistics.\nfrom pyspark.sql.functions import broadcast broadcast(spark.table(\u0026#34;src\u0026#34;)).join(spark.table(\u0026#34;records\u0026#34;), \u0026#34;key\u0026#34;).show() Pandas # Transforming # import numpy as np import pandas as pd # Enable Arrow-based columnar data transfers spark.conf.set(\u0026#34;spark.sql.execution.arrow.enabled\u0026#34;, \u0026#34;true\u0026#34;) # Create a Spark DataFrame from a Pandas DataFrame using Arrow pdf = pd.DataFrame(np.random.rand(100, 3)) df = spark.createDataFrame(pdf) result_pdf = df.select(\u0026#34;*\u0026#34;).toPandas() Pandas Scalar (纯量) UDFs # import pandas as pd from pyspark.sql.functions import col, pandas_udf from pyspark.sql.types import LongType # Declare the function and create the UDF def multiply_func(a, b): return a * b multiply = pandas_udf(multiply_func, returnType=LongType()) x = pd.Series([1, 2, 3]) print(multiply_func(x, x)) # 0 1 # 1 4 # 2 9 # dtype: int64 # Execute function as a Spark vectorized UDF df = spark.createDataFrame(pd.DataFrame(x, columns=[\u0026#34;x\u0026#34;])) df.select(multiply(col(\u0026#34;x\u0026#34;), col(\u0026#34;x\u0026#34;))).show() # +-------------------+ # |multiply_func(x, x)| # +-------------------+ # | 1| # | 4| # | 9| # +-------------------+ Pandas Grouped Map UDFs # Grouped Map UDFs are used along with groupBy()\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType df = spark.createDataFrame( [(1, 1.0), (1, 2.0), (2, 3.0), (2, 5.0), (2, 10.0)], (\u0026#34;id\u0026#34;, \u0026#34;v\u0026#34;)) @pandas_udf(\u0026#34;id long, v double\u0026#34;, PandasUDFType.GROUPED_MAP) def substract_mean(pdf): # pdf is a pandas.DataFrame v = pdf.v return pdf.assign(v=v - v.mean()) df.groupby(\u0026#34;id\u0026#34;).apply(substract_mean).show() # +---+----+ # | id| v| # +---+----+ # | 1|-0.5| # | 1| 0.5| # | 2|-3.0| # | 2|-1.0| # | 2| 4.0| # +---+----+ Shuffle Hash Join / Broadcast Hash Join / Sort Merge Join # Hash Join 的思想是，首先将两个表分为小表 BuildTable 和大表 ProbeTable。将 BuildTable 以 Join Key 为 Key 构建为 HashMap，就可以将 ProbeTable 中的每条记录的 Key 在 HashMap 中进行 O(1) 的搜索，命中后再具体比较 Join Key 的实际值。整个算法的复杂度为 O(a+b)。\nBroadcast Hash Join 非常好理解：对于大小表相 join 的情况，将小表的 HashMap 广播至所有 Executor，然后在所有的节点上执行 Hash Join。\n当内存不足以存储小表时，使用 Shuffle Hash Join。通过 Spark 的 Shuffle 机制，将 Join Key 相同的数据发送到同一 Executor。\nSort Merge Join 目前用于两张大表互相 Join 的情况。先利用 Spark 的 Partition 将两张表重新分区，并在表内部进行排序。现在，每个节点上 A 表和 B 表数据的 Join Key 的范围相同，且都按照 Join Key 排序。然后，使用类似于归并排序的扫描方式来寻找相同的 Join Key。\n"},{"id":48,"href":"/notes/in-depth-jvm/threadlocal-reference/","title":"ThreadLocal and Reference","section":"In-depth Understanding JVM","content":" ThreadLocal 与 Java 中的引用 # ThreadLocal 类 # ThreadLocal\u0026lt;T\u0026gt; 类可以被赋值，并会将自身作为 key，值作为 value 存入 Thread 对象中的 ThreadLocalMap 中。这样，就能够保证使用这个类包装的变量仅存在于当前线程。而且，即使两个线程分别访问同一个 ThreadLocal 对象，从 get() 方法获取到的值也都是各自线程中的值。\nvoid func() { tl.set(Thread.currentThread().getName()); System.out.println(tl.get()); } ThreadLocal\u0026lt;String\u0026gt; tl = new ThreadLocal\u0026lt;\u0026gt;(); new Thread(() -\u0026gt; {this::func}).start(); // Thread-0 new Thread(() -\u0026gt; {this::func}).start(); // Thread-1 构造 # ThreadLocal\u0026lt;T\u0026gt; 类的构造方法没有任何内容和参数。\n每个对象有一个 final int threadLocalHashCode。这个变量的值由静态构造，来自一个 AtomicInteger 累加上一个魔法值。这个魔法值以尽量避免碰撞为依据。这个变量最终会被用在 ThreadLocalMap 的 hash 过程中。\nset 方法 # set(T value) 方法接收要传入的值，使用 getMap(Thread t) 方法得到当前所在线程的 ThreadLocalMap。如果结果为 null，就调用 createMap(Thread t, Object value)。否则，直接将参数值存入到 map 中：map.set(this, value)。\nThreadLocalMap getMap(Thread t) { return t.threadLocals; } void createMap(Thread t, T firstValue) { t.threadLocals = new ThreadLocalMap(this, firstValue); } get 方法 # get() 方法同样首先取得当前线程的 ThreadLocalMap。如果 map 为 null 或当前对象并没有事先 set() 过，则会将 null 作为初始值存入 map 并返回。\npublic T get() { Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) { ThreadLocalMap.Entry e = map.getEntry(this); if (e != null) { @SuppressWarnings(\u0026#34;unchecked\u0026#34;) T result = (T)e.value; return result; } } return setInitialValue(); } private T setInitialValue() { T value = initialValue(); Thread t = Thread.currentThread(); ThreadLocalMap map = getMap(t); if (map != null) map.set(this, value); else createMap(t, value); return value; } protected T initialValue() { return null; } 嵌套类 Entry # new Thread(() -\u0026gt; {this::func}).start(); 这个类用于存储实际的 Thread Local 变量，继承了 WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt;。Entry 在 WeakReference 的基础上增加了一个 value 域。\n这个类以父类 WeakReference 中存储的 ThreadLocal 对象为 key，以自己定义的 value 对象为 value，作为 Map 的 Entry 使用。\nstatic class Entry extends WeakReference\u0026lt;ThreadLocal\u0026lt;?\u0026gt;\u0026gt; { /** The value associated with this ThreadLocal. */ Object value; Entry(ThreadLocal\u0026lt;?\u0026gt; k, Object v) { super(k); value = v; } } Java 的四种引用 # 强引用 软引用 弱引用 虚引用 后三种在 java.lang.ref 包中都有对应的类。它们的父类 Reference 是一个抽象类。由于与这些类有关的很多操作由 JVM 完成，如果需要继承，必须从这三个类继承，而不能直接继承 Reference。\n软引用和弱引用 # java.lang.ref.WeakReference 和 java.lang.ref.SoftReference，二者的功能类似。区别是，软引用的对象只有在内存不足时才会被 GC，而弱引用的对象在失去强引用之后就会直接进入可被 GC 的状态。因此，软引用比弱引用更\u0026quot;强\u0026quot;一些。\n二者的典型应用场景类似于如上的 ThreadLocal：我们在一个集合内维护着所有元素的列表。当代码中已经不再使用这些元素时，这些元素就能够自动离开集合。\nT objs; T objw; SoftReference sref = new SoftReference(objs); WeakReference wref = new WeakReference(objw); objs = null; objw = null; // full gc sref.get(); // objs wref.get(); // null // null 软引用的对象内部有一个时间戳 private long timestamp，其值在每次 get 时被更新为当前时间。具体来说，软引用类内有一个静态变量 static private long clock，其值由垃圾收集器维护。在每次调用 get 方法时，如果引用指向的对象不是 null，就对时间戳进行更新。这个值有利于 GC 的优化。\n虚引用和引用队列 # 虚引用甚至不能用来访问引用的对象，对其调用 get 只能返回 null。这个类的存在是为了使用其另外一个功能，这个功能在另外两种引用中同样存在：\npublic PhantomReference(T referent, ReferenceQueue\u0026lt;? super T\u0026gt; q) { super(referent, q); } public WeakReference(T referent, ReferenceQueue\u0026lt;? super T\u0026gt; q) { super(referent, q); } 引用队列是一个链表包装类，其结点就是 Reference 对象。当引用被 GC 释放时，我们可能希望执行一些其他的操作。\nReferenceQueue\u0026lt;T\u0026gt; rq = new ReferenceQueue\u0026lt;T\u0026gt;(); T obj = ...; WeakReference\u0026lt;T\u0026gt; weakReference = new WeakReference\u0026lt;T\u0026gt;(bytes, rq); map.put(weakReference, value); obj = null; // full gc while((k = (WeakReference) rq.remove()) != null) { // do sth. } ReferenceQueue 有三个开放的方法。poll() 返回元素或 null，remove() 在队列为空时阻塞，remove(long) 提供超时时间。\n不过，需要注意的是，虚引用并不是非常弱的。\n"}]