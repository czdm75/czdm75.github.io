+++
title = '3. Replication and Partition'
+++

# 数据复制

多副本的目的：

-   扩展性，提高读写负载能力
-   容错与高可用
-   到端上的延迟，如CDN

Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。

另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。

相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。

将数据分布在多个节点上时，有两种方式：复制和分区。

本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。

## 主从复制

-   主节点接收所有写请求，再分发给从节点
-   从节点的写入顺序和主节点相同
-   读取时可以请求主节点或从节点

主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。

### 同步复制和异步复制

主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。

实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。

全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。

链式复制是其中一种折中方案。

### 配置新的从节点

需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。

### 处理节点失效

如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。

如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。

可能存在的问题：

1.  异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。
2.  其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露）
3.  脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。
4.  如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。

### 复制日志的实现

#### 基于语句复制

1.  如果语句使用了 `RAND()` `NOW()` 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式

2.  如果语句的写数据依赖读数据（`INSERT SELECT FROM`，`UPDATE WHERE` 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。

3.  其他有副作用的语句，对外部的副作用可能不同

#### 基于 WAL 复制

1.  SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。

2.  造成数据和存储引擎紧密耦合（如 B-Tree 的 WAL 包括很多树节点粒度的操作，不能直接适配到其他数据结构）

3.  造成数据和存储引擎版本紧密耦合，需要停机升级

#### 基于行的复制

1. 提供一种和 WAL 不同的逻辑日志，如 binlog

2. 方便外部处理（如 dump 到 Hive，或另外构建索引和缓存）

#### 基于触发器复制

1.  主节点触发外部触发器，可以执行过滤、处理等，再同步到从节点。
2.  通常比数据库内置的方案开销更好，错误风险更大，但更灵活

## 复制滞后问题

异步复制下，从从节点可能读到过期的数据。考虑到数据最终会追赶上，这种效应被称为最终一致性。然而，数据同步的延迟理论上并没有上限。

### 读自己的写

写入后马上从从节点读取自己刚刚写入的数据，可能读到过期的数据。保证这种读写一致性（写后读一致性）的方案：

1.  对于可能被修改的内容，从主节点读取。例如，只有用户自己能够修改用户资料，所以读取自己的用户资料时从主节点读，读取他人的可以从从节点读。
2.  确定一个时间上限，例如发生修改后一分钟内一直从主节点读，同时对滞后超过一分钟的从节点暂时停止访问
3.  客户端保留写入时的时间戳，与读出的数据中时间戳比较。如果发现不一致，就重试。时间戳可以是数据库的日志序列号，也可以是真实时间（需要考虑不可靠的本地时间）
4.  对于跨地区多数据中心，需要把需求路由到主节点所在的机房

用户还可能在 Web 上写，在移动端读，这时就不能依赖客户端的一致性（如时间戳）。

### 单调读 Monotonic Read

如果一个用户先从进度较快的节点读一次，又从进度较慢的节点读一次，就可能看到第二次读取的值反而更老。单调读一致性的简单实现是让同一个用户永远读同一个从节点，例如通过哈希。但如果这个节点失效，就只能路由到其他副本。

### 前缀一致读 Consistent Prefix Read

对一系列写请求，需要按照相同的顺序读取。例如，事件 A 依赖事件 B，但某个副本上 B 比 A 更早得到同步的不一致情况。避免这种情况发生的一致性保证是前缀一致。当数据存在分区而两个事件存在不同的分区时，这个情况就容易发生。直觉的解决办法是追踪事件之间的因果关系，但这一点的实现并不容易。

### 复制滞后的解决方案

这些问题的解决有两种思路：

1.  在应用层解决，比如上面的仅从主节点读取自己的用户资料的做法。但这种做法不太通用，且比较复杂，容易出错。
2.  在数据库侧做这样的保证，即事务。单机事务已经比较成熟，分布式数据库则很多放弃了支持事务。

## 多主节点复制

显然，主从复制中如果主节点故障，所有的写操作都会中断。如果允许多个主节点，主节点之间互相同步，就可以避免这种情况。

### 适用场景

在单个机房内部使用多主节点的意义不大，因为其实现比较复杂而机房内故障率相对低。对于跨机房数据库，如果所有写操作都只能通过主节点，就失去了多机房部署就近访问的意义。
 
因此，在每个机房设置一个主节点，各个机房之间进行主主同步，机房内部进行主从同步。这样做的优点是提升性能（本地访问，异步同步到其他机房），容忍机房级别的故障和跨机房专线级别的故障。当然，这种部署需要解决写冲突的问题，且和自增主键、触发器、完整性约束等功能共同使用时的陷阱还比较多。
 
使用多主复制的数据库包括：MySQL 的 Tungsten，PostgreSQL 的 BDR 和 Oracle 的 GoldenGate。

此外，一些应用设计也需要类似分布式数据库的同步。例如跨平台的日历应用，允许在没有网络时修改日程，此时就相当于有一个本地的主节点，需要再网络恢复时进行同步。文档的协作编辑也是一个典型的例子。

### 处理写冲突

有些冲突比较显然，比如同一行在两个不同的节点上被写不同的值；有的冲突比较隐晦，例如同一个会议室被预定两次：在两个节点上分别预定时，两个节点都认为这个会议室是空的。

1.  避免冲突，例如上面提到过的，对同一个文档路由到同一个主节点，对单个文档就变成了主从复制模型。但是，这仍然要面临数据中心故障或用户离另一个数据中心更近，一定要路由到其他数据中心的情况。

2.  收敛到一致状态
    1. 给每个写入分配唯一的 ID，如时间戳、哈希、UUID 等，选择最晚 / 最大的，即最后写入者获胜。这种方法比较普遍，且在很多场景下适用，但会造成写入较早的一方数据丢失
    2. 为每个节点分配一个 ID，取 ID 最大的节点的版本，问题同上，甚至更加无法和业务场景契合
    3. 合并冲突的值
    4. 保留冲突为一个特殊的结构，然后依靠应用层或通知用户来解决（想象异步的 Git Merge）

3.  自定义冲突解决逻辑
    1. 从实用角度来讲，最合适的方法仍然是在应用层解决冲突，让数据库的使用者来编写冲突解决逻辑。这个逻辑可以是在写时执行或读时执行
    2. 通常，这个解决逻辑的粒度是单个行（对于关系型数据库）或单个文档（对于文档型数据库），因此对于包含多个写的原子事务存在一定局限性

自定义冲突解决逻辑很难以编写，需要考虑不同的业务场景。有一些自动冲突解决的探索：

1.  CRDT (Conflict-free Replicated Data Types) ，是一些可以多个用户同时编辑的数据结构，如 map、list、counter 等。一些在 Riak 中已经被实现
2.  Mergable Persistent Data，使用类似 Git 的方式跟踪变更历史，使用三向合并(Three-way merge function）。相比之下，CRDT 是双向合并。
3.  Operational transformation，是 Google Doc 使用的冲突解决算法，适用于可同时编辑的有序列表。

### 拓扑结构

多个主节点的同步方向可以有不同的拓扑结构：星形，环形，全连接型。

星形和环形的问题是，每个数据需要经过多个节点才能同步完成，且单个节点宕机仍然会影响同步，需要重新配置拓扑结构。

全连接的问题是，每条链路的速度不一定一致，可能造成某一行的 UPDATE 操作比 INSERT 更早抵达之类的情况。

一种解决方案是使用版本向量。总的来说，在许多多主系统中，冲突检测技术还很不完善。

## 无主节点复制

### 节点失效时写入数据库

Dynamo 风格数据库，Riak、Cassandra、Voldemort 都使用类似的风格。客户端或客户端直接访问的协调者**将写请求同时发送到多个节点**，通过节点间的同步交互来进行冲突处理。在读请求的时候，也是同时读多个节点，经过协调确认最终结果。

一个失效的节点重新上线后，进行同步的方式：

1.  读修复：在读取时，从某些节点读到旧数据，另一些读到新数据，就把新数据写入到旧数据的节点里，相当于加快同步。
2.  反熵过程：由后台进程不断查找副本间的差异并同步。这个过程并不保证数据同步的顺序，同步滞后可能很长

如果系统只实现了读修复而没有实现反熵，那么没有被读取到的数据就会长期保留旧数据的状态，相当于副本数偏少，节点故障的容忍能力下降。

### 读写 Quorum

如果有 n 个副本，写入需要 w 个节点确认，读取至少读 r 个节点，那么当 w + r > n，就可以确保读到最新值，这种方式称为仲裁读和仲裁写。

通常设置 n 为一个奇数 3 或 5，w = r = (n+1)/2。注意这不代表集群只有 n 个节点。

通常读写请求都是同时发送到全部 n 个副本，w 和 r 只是读取和写入成功的判断标准。

-   当 w < n，在写入时可以容忍节点不可用
-   当 r < n，在读取时可以容忍节点不可用
-   当 n = 3, w = r = 2，可以容忍一个节点不可用
-   当 n = 5, w = r = 3，可以容忍两个节点不可用

### Quorum 一致性的局限性

上面的 w + r > n 的限制可以保证一定可以读到最新值，同时也确定了对节点失效的容忍能力。如果放开这个限制，那么就有可能读到旧值，降低一致性，但同时也可以提升性能（更低的延迟）和可用性（更高的节点失效容忍能力）。

即使约定 w + r > n，也可能存在一些局限：

1.  sloppy quorum
2.  写冲突
3.  如果在写入过程中进行读取，数据可能尚未抵达足够多的节点
4.  如果一次写入的部分节点成功，而部分节点失败，总的成功数量没有达到 w，整个写请求被认为是回滚状态，那么那些已经写入完成的节点上就存在了异常的值
5.  如果某个存有新值的节点失效，恢复时从存有旧值的节点同步数据，就打破了上面 w 个副本的要求
6.  其他异常情况，参见第九章“可线性化与 Quorum”

整体来说，我们仍然把 Dynamo 风格的数据库看作最终一致性的数据库，而不认为上面这些归档能够有真正的数据一致保证。

即使应用本身可以接受读到旧值，监控数据库的同步状态仍然有很大运维意义。对主从复制，因为同步存在顺序，可以直接通过偏移量监控。对于无主复制的系统，不仅没有同步顺序，如果只使用读修复，值的同步延迟就是无上限的。我们可以基于 w r n 的值对旧值的比例给出一个估计。

### Sloppy Quorum

如果一个大集群发生了网络分区，无法访问到 w 个节点，应该如何处理写请求？一个方案是，将写请求暂时写入到 n 个节点之外的可访问节点（其他分区的节点）上。这时我们认为写入和读取仍然需要 w 和 r 个节点，但不一定是原来的 n 个节点之内的。当网络得到恢复，就要把这些临时数据放回应该在的位置。

显然，这样做会失去读到最新的保证，同时也大大提高了写入可用性。Riak 默认开启，Cassandra 和 Voldemort 默认关闭。

### 跨数据中心的 Quorum

Cassandra 和 Voldemort 的做法是将副本数 n 分布到各个机房，写入时会同时向本地机房和远端机房发送写请求，但只等待本地机房的写入结果，从而同时获得容灾能力和低延迟。

Riak 的做法是在每个机房单独配置集群，在各个集群之间采用类似多主复制的方式来同步。

### 检测并发写

和多主复制一样，Quorum 机制不能解决写冲突的问题。

在使用 LWW（Last Write Wins）的方法时，在多节点同时写的情况下，如果不确定一个规则，就可能永远无法达成一致。但我们事实上很难确定事件发生的自然顺序，因此人为指定一个排序即可。这样可以达到最终一致性，但带来数据丢失。

当两个操作互相之间没有依赖关系（happens-before），就是并发的。

先考虑一个简单的情况：一个单副本的购物车数据库，在两个客户端并发写入。于是数据库会维护两个版本的数据，客户端在每次读取时带上自己已知的上一个版本，从而让数据库知道应该覆盖哪一个版本的数据；同时数据库会将两个版本都返回给客户端，让客户端自己来处理两个版本的合并。

![Data Merging under Quorum](../quorum-merge.png)

这样，我们同时获得了最终一致性，也没有发生值的丢失。

在确定合并逻辑时可以使用版本向量、软删除墓碑、CRDT 等方法。

# 数据分区

## 数据分区与数据复制

一个节点可能同时既是某个分区的主副本，又是其他分区的从副本。

## KV 数据的分区

-   range分区，手动或自动。存在查询热点问题，如大部分请求读的都是最新的数据。可以先按其他字段分区，再按时间分区。
-   hash分区
    - hash函数需要是一致的（反例：Java和ruby的hashcode在不同进程中不同），但不需要加密性很强
    - 丧失区间查询能力
-   Cassandra 的折中：复合主键的第一个字段用哈希，剩下的排序用range，确定第一个主键后剩下的可以区间查询

一致性哈希（consistent hash）解决的问题：对朴素的mod n哈希分区，如果增加一个分区，那么有大量的数据会改变前后所在的节点，需要大量数据迁移或缓存失效；一致性哈希在扩容时不改变大部分分区的hash范围，仅变更扩容处的，解决这个问题；ddia称目前并不常用，实际效果不好。

哈希仍有不能解决的倾斜，如明星热点事件，单个ID有大量访问。简单的办法是增加一个随机数拆分，查询完再合并。

## 分区与二级索引

HBase，Voldemort 等 kv 数据库不支持二级索引，Riak 等正在尝试增加；对于 Solr 和 Elastic Search 等，二级索引是其存在的根本意义。

-   基于文档的二级索引：每个分区一个二级索引表，对每一条新纪录，插入时在二级索引中添加这条新记录的ID。需要确保二级索引和kv的一致性。
    - 在二级索引上查询时，需要查询所有分区的二级索引，再合并，称分散/聚集（scatter/gather），写放大严重，延迟高
    - Mongo，Riak，Cassandra，ES，Solr，Volt等使用
-   基于词条的二级索引：全局使用同一个二级索引，二级索引本身也分区，索引本身可以使用range分区
    - 不需要读取所有分区
    - 写入慢且复杂
    - 数据和索引一致性和写入延迟之间需要权衡。如果要同步索引，则一个事务需要至少读写两个分区（KV 和索引），因此目前数据库都不支持同步索引

## 分区再平衡

直接取模会遇到大量数据迁移

-   固定数量分区：提前确定数量超多的分区，固定数量；节点负载增加时将分区移走但不需要改变哈希
    - Riak ES Couchbase Voldemont使用这种方式
    - 分区数量固定，大小和数据量成正比
-   动态分区：允许分区合并拆分，类似b树
    - HBase 和 Rethink 使用这种方式
    - 对 HBase，拆分的数据迁移依赖 HDFS
    - 当数据库为空时只有一个分区，会造成流量集中在一个节点；解决办法是预分裂
    - 分区大小有上下限（合并拆分），数量与数据量成正比
-   按节点比例分区：每个节点的分区数量一定
    - 分区数量不和数据量成正比，而是和节点数量成正比
    - Cassandra 和 Ketama 使用这种方式
    - 符合一致性哈希

再平衡可以是自动的或手动的。自动再平衡节省运维，但遇到故障容易出现负载失控；如果一个节点负载很高，可能将其认为失效，对其进行数据迁移反而进一步加重负载，甚至导致雪崩。

## 请求路由

-   随机选择（普通的负载均衡），允许客户端连接到任意节点，再路由到分区所在节点
-   增加一个路由层，客户端连接到路由层
-   客户端感知节点分配，直接连接目标节点

共同问题：及时更新分区变更

-   配置中心
    - Linkedin 的 Espresso 使用基于 ZK 的 Helix
    - HBase，Solr，Kafka 使用 ZK
    - Mongo 使用自己的配置中心
-   Gossip 协议，使用上述的路由方案1
    - Cassandra，Riak
-   Couchbase不支持自动再平衡，使用路由层 moxi 向集群学习路由变化

使用路由层或随机选择时，仍然需要知道 IP 地址，因为变更不频繁，使用 DNS 就足够。
