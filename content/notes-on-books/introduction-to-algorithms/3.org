#+TITLE: 3. Other Sorting, Hash, BST
#+MATH: true

* 非比较排序

** 比较排序的下界

之前介绍的归并排序、堆排序、快速排序都属于比较排序。比较排序在最坏情况下都为 \(\Omega(n\log_2n)\) 的。因此，复杂度为 \(\Theta(n\log_2n)\) 的归并排序和堆排序是渐进最优的，其他所有算法都只是在常数项上比它们更优。

一个比较排序可以抽象为一颗决策树。如，对于序列 \({a,b}\) 的决策树为，根节点判断两个元素的大小，左叶子节点是 \(a\leqslant b\) 的结果，右叶子节点是 \(a>b\) 的结果。相应地，对于长度为 3 的序列，树将有三层，六个叶子节点。决策树的最大层数既是我们在得到结果之前必须做的比较的次数。

对于一个长度为 \(n\) 的待排序序列，让决策树有 \(l\) 个可达节点，即这些个可能的排列，树的高度为 \(h\)。那么将有：

\[n!\leqslant l\leqslant 2^h\]

两边取对数得到：

\[h\geqslant\log_2(n!)=\Omega(n\log_2n)\]

** 计数排序

计数排序的基本思想是：对每一个元素，数出有多少个元素小于它，则这个元素应该被放在相应数量后面的下一个位置。假设 \(n\) 个输入元素均为 \(0\) 到 \(k\) 区间内的一个整数，那么当 \(k=O(n)\) 时，排序的运行时间为 \(\Theta(n)\)。

计数排序的实现是这样的：初始化一个长度为 \(k\) 的数组 \(C\) 用来计数，扫描原数组，对每个元素，如果值为 \(i\)，就在 \(C[i]\) 位置加一。扫描完成之后，就得到了每个值的元素数量。随后，在 \(C\) 数组上进行一次 \(C[i]=C[i]+C[i-1]\) 的扫描，这时 \(C\) 数组中储存的就是小于该下标值的元素的数量。最后，扫描原数组，根据 \(C\) 数组中的数据，将这些元素放置在另一个数组中合适的位置。

显然，整个算法经历了一次 \(\Theta(n)\) 的扫描，一次 \(\Theta(k)\) 的累加和一次 \(\Theta(n)\) 的复制，整个算法的复杂度为 \(\Theta(n+k)\) 。当 \(k=O(n)\) 时，复杂度就是 \(\Theta(n)\)。

在复制过程中，如果存在两个元素的值相等，就放到下一个位置，之前的技过程已经为它留计数了位置。因此，计数排序是稳定的。下面我们将看到，这个性质在其作为基数排序的一部分时，相当有用。

** 基数排序

*** 算法描述

基数排序（radix sort）来源于卡片排序机。卡片计算机一次只能判断一列孔，即只能判断一位数字。直觉上，我们倾向于从最高位开始，递归向下地进行排序。但是，这种方法在卡片计算机上使用时将会需要临时保存好另外 9 种数字的卡片，在数字位数较多时使用的容器和标签就会相当可观。

因此，实际上我们是从最低位开始进行排序的。在排序好最低位之后，可以直接把所有卡片按顺序叠在一起，开始进行第二位的排序。这样就不需要把卡片另行保存，直接排序完成就能使用。

所以，基数排序实际上是从低到高按位进行其他排序方法的算法。由于低位在之前的循环中已经排好，使用的其他排序方法必须是稳定的，否则低位的顺序就会被打乱。除了按十进制的位之外，还可以对日期按日 - 月 - 年进行排序等很多类似方式。

*** 代价分析

给定 \(n\) 个 \(d\) 位数，其中每个位有 \(k\) 个可能的取值，由于 \(k\) 有限且不大，计数排序十分合适，复杂度为 \(\Theta(n+k)\)。显然，总的复杂度为 \(\Theta(d(n+k))\)。

接下来讨论 “划分” 位的策略。如果把子排序中使用的位数从 1 位扩展到 \(r\) 位，而 整个数字一共有 \(b\) 位。那么，上式的 \(d\) 就是 \(b/r\)，\(k\) 就是 \(2^r-1\)。例如，对于一个 32 位的整数，我们将其分解为 4 个字节，于是 \(b=32\)，\(r=8\)，对每个字节进行计数排序。在相似的条件下，排序的复杂度就变成了 \(\Theta(\frac br(n+2^r))\)。

如果 \(b<\lfloor\log_2n\rfloor\)，那么对于任何 \(r\leqslant b\)，都有 \((n+2^r)=\Theta(n)\)，算法的复杂度取决于 \(b/r\)。显然，当 \(r=b\)，复杂度取得最小值 \(\Theta(n)\)。

如果 \(b\geqslant\lfloor\log_2n\rfloor\)，对 \(r\) 的值分情况来讨论。如果 \(r=\lfloor\log_2n\rfloor\)，可以得到最优时间代价 \(\Theta(bn/\log_2n)\)。如果 \(r<\lfloor\log_2n\rfloor\)，值越小，\(b/r\) 项的值就越大，\((n+2^r)\) 仍然为 \(\Theta(n)\)。反之，\(r\) 越大，分子中的 \(2^r\) 项比分母中的 \(r\) 项增长得更快。因此，时间代价为 \(\Omega(bn/\log_2n)\)。

*** 与比较排序的对比

虽然基数排序的复杂度比较小，其中的常数项相对于比较排序来说哪一个更大，要取决于机器具体的实现和数据的特点。例如，通常来说快排可以更加有效地利用机器的缓存。另外，计数排序是一个非原址排序。当机器主存紧缺的时候，原址排序的优势就更大一些。

** 桶排序

桶排序同样有一个假设：数据服从均匀分布，且有一定的范围。桶排序的流程是：将区间 \([0,1)\) 划分为若干个桶，将数据投入到各个桶中（实现上，可以用整数除法等操作实现）。在数据分布比较平均的情况下，各个桶里数据的量也比较平均。随后，对每个桶进行排序。最后，把所有的桶拼合起来即可。如果桶内的排序是稳定的，这个算法也是稳定的。

现在来分析时间代价。假设使用插入排序，那么时间代价 \(T(n)=\Theta(n)+\sum_{i=0}^{n-1}O(n_i^2)\)。对这个式子两边取期望，得到：

\[\mathrm E[T(n)]=\mathrm[\Theta(n)+\sum_{i=0}^{n-1}O(n_i^2)]=\Theta(n)+\sum_{i=0}^{n-1}O(\mathrm E[n_i^2])\]

我们断言 \(\mathrm E[n_i^2]=2-\frac1n\)，这个值可以利用随机指示器证明。最终，得到桶排序的时间复杂度：\(\Theta(n)+n\cdot O(2-\frac1n)=\Theta(n)\)。

* 顺序统计量

** 顺序统计量

一个 \(n\) 个数的集合的第 \(i\) 个顺序统计量表示集合中第 \(i\) 小的数。寻找顺序统计量的算法称为*选择算法* 。显然，可以通过排序以 \(O(n\log_2n)\) 的代价寻找到该统计量。

** 最大值和最小值

显然，最大值和最小值都属于顺序统计量的一种，可以通过代价为线性时间的一次扫描获得。一种同时取得最小值和最大值的方法是，对于每两个元素，先对这两个元素互相进行比较。随后，将较大者和最大值比较，较小者和最小值比较。这样，每两个元素只需要进行三次比较。比较次数从 \(2n-2\) 次减少到 \(\frac 32n\) 次。

** 选择算法

*** 期望为线性时间的分治选择算法

randomized-select 算法是以快速排序为原型的。区别在于，作为选择算法，这个算法只需要对划分的一边进行递归处理。randomized 的意思是，它采用了和随机化快排中使用的相同的划分算法：随机挑选一个值作为主元 pivot。随后，我们从目标所在的那个分区继续递归调用，寻找顺序统计量。显然，这个算法的最坏情况代价和快速排序一样，为 \(\Theta(n^2)\)。当所有元素都是互异的，这个算法的期望复杂度能够达到线性水平。证明过程比较繁复，按下不表。

*** 最坏情况下代价为线性时间的选择算法

这种选择算法是这样的：将输入按照每五个一组分组，并通过插入排序找到每一组的中位数，随后，递归调用本算法，找到中位数的中位数。如果这个中位数就是目标，此时就可以返回。然后，以这个中位数为 pivot 进行划分。随后，对目标所在的子数组再次递归调用。

显然，分组、插入排序和划分都是线性代价的。递归寻找中位数的中位数这个过程的代价是 \(T(\frac n5)\)，递归进行最后一步的代价最多是 \(T(\frac 7{10}n+6)\)。于是有递归式：

\[T(n)\leqslant \begin{cases}O(1)&\text{if }n<140\\T(\frac n5)+T(\frac 7{10}n)+O(n)&\text{if }n\geqslant140\end{cases}\]

结果为 \(O(n)\)。不过，这个算法更加具有理论性质，它的常数项过大，大部分情况下都不适用。

* 扩展：其他常见排序方法

希尔排序是这样一种算法：规定一个步长，从第一个元素开始，把所有间隔这个步长的元素作为一个子数组，对所有子数组进行原地的插入排序。随后缩小步长，继续进行类似的操作。直到步长为 1 ，经过插入排序的一遍扫描之后，序列即被排序好。

希尔排序的核心思想是通过步长和分组使前期排序的 \(n\) 变小，而后期排序的序列处于 “基本排好” 的状态，以避免插入排序中面临的大量元素移动操作。一次典型的希尔排序过程如下：

|    |    |    |    |    |     |    |    |    |    | step |
|----+----+----+----+----+-----+----+----+----+----+------|
| 49 | 38 | 65 | 97 | 76 | 131 | 27 | 49 | 55 | 04 |    5 |
| 13 | 27 | 49 | 55 | 04 |  49 | 38 | 65 | 97 | 76 |    3 |
| 13 | 04 | 49 | 38 | 27 |  49 | 55 | 65 | 97 | 76 |    1 |
| 04 | 13 | 27 | 38 | 49 |  49 | 55 | 65 | 76 | 97 |    - |

如，在算法的第一轮，以 5 为步长，则 (49, 13) 为一组，进行插入排序后二者被交换。第二轮以 3 为步长，则 (13, 55, 38, 76) 为一组，进行插入排序。

朴素的希尔排序最差情况下的复杂度与插入排序相同，为 \(O(n^2)\)。不过，在精心设计的步长序列下，希尔排序在小数组上的效率甚至可能比快排更好。与冒泡排序比较，希尔排序相当于使用步长这个特点来使元素一次跳过比较长的距离。由于希尔排序的各轮之间是独立的，这个算法是不稳定的。

** 鸡尾酒排序

原始的冒泡排序是这样的：每进行完一轮扫描，就回到序列头部重新开始。而鸡尾酒排序在进行完一次从左到右的排序后，继续进行从右到左的排序。鸡尾酒排序的双向冒泡可能会带来更好的性能，例如对于序列 (2, 3, 4, 5, 1) ，鸡尾酒排序只需要一个来回，而冒泡排序需要 4 轮。但大部分情况下提升不大。平均时间代价仍然为 \(O(n^2)\)。

** 梳排序

梳排序的想法有些类似于希尔排序：同样是使用一个步长，不过这个步长不用来划分出数组，而是仍然从头扫描，两两比较交换。如对于序列 (a, b, c, d, e) ，取步长为 3 ，将会对 a 和 d 比较交换，再对 b 和 e 比较交换。缩小步长为 2 ，则对 (a, c) 、 (b, d) 、 (c, e) 比较和交换。因为每一轮不是完整的排序过程，所以步长只能以 1 为单位递减。梳排序的最差复杂度为 \(O(n^2)\)，期望为
\(\Theta(n\log_2n)\)。

** 其他排序方法

除此之外，还可以使用*二叉搜索树*构造有序序列，这种结构非常适合插入和查找一定大小的值，复杂度为 \(O(n\log_2n)\)
。*选择排序*是不断取顺序统计量，也就是不断从后部分序列中寻找最小者并放在头部的算法，复杂度为 \(O(n^2)\)
。*内省排序*是快排的一种改进，当递归达到一定的深度之后改用堆排序，以兼有二者的优势，将最差代价控制在 \(O(n\log_2n)\)。

* 表和散列

** 链表的哨兵结点

链表的哨兵结点表示 \(nil\) 值，其 \(prev\) 属性指向表尾，\(next\) 属性指向表头。这样，就可以省略掉 \(head\) 属性，并简化边界条件的处理。如果我们使用的是很多个很短的链表，哨兵结点就会造成比较严重的存储浪费。

** 不使用指针的链表

- 多数组的实现：对于双向链表，至少使用 3 个数组，分别为 prev、key、next，其值即为所指向对象的数组下标。每个数组相同下标的值合起来是一个完整的结点对象。

- 单数组的实现：以一整个数组连续存储对象，使用下标代表指针。当需要访问对象的成员时，在指针上加一个偏移量。相对于多数组的实现，这种方式就可以支持不同长度的对象构成的链表。

- 自由表：未被使用的，可能是之前被释放的内存单元组成的链表。。数组表示中的每一个对象不是在链表中，就一定在自由表中。实现上，自由表常常是一个链表栈。刚刚被释放的空间，在下一次插入中就会被用来存储新的对象。显然，多个链表也可以共用同一个自由表。使用自由表的释放操作和插入操作运行代价仍然是 \(O(1)\)，因此非常实用。

** 有根树的表示

对于分叉数量未知的树，我们难以使用数组来储存孩子结点的指针。或者，如果最大孩子数很大，那么使用相同数量的指针空间将会浪费大量的存储空间。因此，在这里引入*左孩子右兄弟表示法*。

在这样表示的树中，每一个结点有三个指针：父结点，左孩子指针和一个兄弟指针，兄弟指针指向它右侧的具有同一个父结点的结点。同一个父结点的所有孩子结点实际上相当于构成一个链表。如果是最右子结点，就把兄弟指针设置为 \(nil\)。

** 散列函数

散列函数所需要的最基本性质是，尽量让 key 进入各个槽的概率平均。除此之外，还可能需要一些其他的性质。比如，可能希望相接近的关键字的散列值差距较大，在开放寻址法进行线性探查时需要这种性质，而这种性质由全域散列提供。此外，还可能需要把其他种类的关键字，或负数、浮点数等转换成自然数等。

*** 除法散列

最简单的除法散列适用于平均分布的自然数序列。被除数选择不接近 2 的整数幂的较大的质数有利于散列。如对于一个预备存储 2000 个元素的散列表，可取 \(h(k)=k\mod701\)。

*** 乘法散列

用关键字乘一个常数，通常为一个无理数，取小数部分，再乘上一个值，取整变回自然数。如：

\[h(k)=\lfloor m(kA\mod1)\rfloor\]

乘法散列对 \(m\) 的值并不挑剔，一般取为一个 2 的幂，这样在计算机内部可以直接通过取一个数的高位来获得散列值。 \(A\) 的一个比较理想的值为 \(\sqrt5-1\approx0.618033\cdots\)。

*** 全域散列

全域散列的思想是通过随机选择散列函数，避免最坏情况的，即所有元素都放置在同一个槽的情况出现。

*** 完全散列

完全散列的最坏情况查找只需要 \(O(1)\) 次访存。一种完全散列的方法是，使用两层散列表。通过精心设计第二层散列函数，使得在第一集中落到同一个槽中的元素在第二级不再出现冲突。为了达到这个目的，第二层的槽数需要为散列到该槽中的关键字数的平方。

** 散列冲突的解决

*** 链接法

每个散列的槽对应的是一个链表，其中存储所有该散列值的元素。如果要不重复地插入元素，或者删除指定 key 的元素，显然需要搜索整个链表。因此，操作的时间代价取决于链表的长度。

定义散列表的装载因子，即元素数与槽数的比 \(\frac nm\) 为 \(\alpha\) 。链表的查找时间取决于链表的长度，显然，最坏情况下整个散列表的查找时间为 \(\Theta(n)\)。每个槽链表长度期望等于 \(\alpha\)，时间代价期望为 \(\Theta(1+\alpha)\)。这是建立在散列完全均匀的假设下。

*** 开放寻址法

开放寻址法意味着散列表中不存在链表，表有可能被填满。当出现散列冲突时，就根据一定的原则继续寻找下一个可以存储的位置。常见的探查方法有三种：线性探查、二次探查和双重探查。在以下的表述中，我们将未加入探查功能的散列函数称为辅助散列函数，以
\(h'\) 表示。

- 线性探查

\[h(k,i)=(h'(k)+i)\modm\]

式中 \(i\) 为探查的次数。线性探查是在失败之后，线性地依次查看后面的槽位，直到找到空的槽为止。线性探查容易实现，但容易发生群集。即，被占用的槽很可能形成连续的长序列，当辅助函数落到这个序列的头部时，就需要相当长的探查序列。

- 二次探查

\[h(k,i)=(h'(k)+c_1i+c_2i^2)\modm\]

线性探查相当于这种方式在常数 \(c_1=1,c_2=0\) 时的情况。由于探查位置二次依赖于 \(c_2\)，探查序列不容易过于群集。

- 双重散列

\[h(k,i)=(h_1(k)+ih_2(k))\modm\]

双重散列是开放寻址的最好方法之一，需要注意的是 \(h_2(k)\) 必须与 \(m\) 互质。一个方法是，取 \(m\) 为 2 的幂，并设计一个永远产生奇数的 \(h_2\) 。另一种方式是，取 \(m\) 为质数，并让 \(h_2(k)\) 略小于 \(m\) 。前两种方法可能的探查序列有 \(\theta(m)\) 种，而双重散列将其提升到了 \(\Theta(m^2)\) 种。

*** 开放寻址的性能分析

仍旧使用装载因子 \(\alpha\) 的概念。不过，在开放寻址中，\(\alpha\) 始终小于 1 。和之前一样，\(i\) 是探查的次数。于是，探查次数期望的上界：

\[\mathrm E[X]=\sum_{i=1}^\infty\Pr\{X\geqslant i\}\leqslant\sum_{i=1}^\infty\alpha^{i-1}=\frac1{1-\alpha}\]

实际上，一次成功查找的探查期望次数是：

\[\frac 1\alpha\ln\frac 1{1-\alpha}\]

如果散列表是半满的，探查的期望小于 1.387. 如果散列表 90% 满，期望小于 2.559.

* 二叉搜索树

对一个二叉搜索树，任何一个结点的左子结点不大于它本身，右子结点不小于它本身。这样，就可以简单地使用中序遍历查找元素。中序遍历打印出来的序列，就是已经排序完成的序列。中序遍历的时间代价为 \(\Theta(n)\)。

** 二叉搜索树的基本操作

- 在高 \(h\) 的树上，以下操作的时间代价均为 \(O(h)\)。

- 查找：比较和当前结点的大小，选择子树。

- 最大和最小：不断取左子结点或右子结点。

** 后继前驱

如果关键字不重复，那么一个结点的中序遍历后继为大于这个结点的最小者，即升序序列中的下一个。如果结点的右子树非空，那么右树中的最左结点即为后继结点，不断向左寻找即可。

如果右子树为空，说明这个结点是某个左子树的最右结点，而这个左子树的父结点即为后继结点。这意味着，遍历这个结点之后，这个左子树遍历完成，进入某个遍历过程的根结点部分。于是，不断向上寻找，如果当前结点不再是右结点，说明已经找到了这个根结点。如果找到了 \(nil\)，则说明没有后继，这个结点是整个树的最右结点。

前驱和后继的过程对称，时间代价均为 \(O(h)\)。

** 插入和删除

插入过程比较简单。寻找结点的关键字应该在的位置，并修改父结点的指针即可。

删除结点可分为三种情况：

- 没有子结点，直接删除并修改父结点的指针即可。
- 只有一个孩子，则用这个孩子来替代这个结点。
- 两个孩子，则使用后继结点来替代这个结点。由于被删除的这个后继结点是右子树的最左结点，其一定没有左子节点。因此，使用其右子节点代替它的位置，并用这个节点代替待删除的结点。于是，删除完成。

显然，这两种操作的时间代价也是 \(O(h)\)。

** 随机构建二叉搜索树

二叉搜索树的构建由插入和删除操作完成。显然，实际情况中，这是一个随机过程。在最坏情况下，当元素严格升序或降序插入，二叉搜索树将成为一个链表。十分显而易见的是，在完全随机的情况下，元素均匀插入，二叉树接近完全，其高度的期望为
\(O(\log_2n)\)。
