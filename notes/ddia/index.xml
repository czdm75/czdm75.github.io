<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Designing Data-Intensive Applications on czdm75 Blog</title><link>/notes/ddia/</link><description>Recent content in Designing Data-Intensive Applications on czdm75 Blog</description><generator>Hugo</generator><language>en</language><atom:link href="/notes/ddia/index.xml" rel="self" type="application/rss+xml"/><item><title>1. Data System and Data Model</title><link>/notes/ddia/1/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/ddia/1/</guid><description>可扩展与可维护的与应用系统 # 数据密集型应用系统的典型例子包括：
数据库 高速缓存 索引 流式处理 批处理 可靠性 Reliability # 对数据密集型系统可靠性典型的基本期望：
执行期望的功能 可以容忍错误的使用方法 性能可以应对典型场景 可以防止未授权的访问 fault 指部分的功能不符合预期，反之即为 fault-tolerence 或 resilient；failure 指整个系统的完全不可用。可靠性意味着，我们希望部分的 fault 不会造成整个系统的 failure。
系统的可靠性需要涵盖的方面：
硬件故障 软件错误 人为失误 以最小出错的方式来设计系统界面 将容易出错的地方分离，如上线的测试环境 充分测试 快速回滚 监控子系统 管理流程 可扩展性 Scalability # 在评价可扩展性之前，需要先能够描述系统的负载和性能。
描述负载的指标通常包括 QPS、写入数据比例、同时活动用户数、缓存命中率、扇出数等，这些指标可以是均值、峰值、分位数。
描述性能的指标包括延迟、响应时间、吞吐量等。其中，响应时间是端到端的全链路延迟，而 latency 一般只指用在处理请求上的时间。同样地，经常观测其中位数或高分位数。
数据密集型系统的挑战通常来自于负载增加。需要考虑的问题：
维持性能，负载增加，需要增加多少资源 维持资源，负载增加，性能会如何变化 通常使用的解决方案可以大致分为两类：
Scale up 垂直扩展 Scale out 水平扩展 可维护性 Maintainability # 可维护性通常包括：
可运维性 简单性，即系统本身理解的复杂程度 可演化性，即系统迭代、改变设计的难度 运维团队的职责：
监视系统健康状况，进行快速恢复 追踪异常的原因（如系统故障或性能下降） 保持更新（如安全补丁） 了解不同系统之间的相互有影响，避免破坏性操作 预测可能的问题并解决（如扩容规划） 建立部署和配置的良好实践和 util 执行复杂的运维任务，如集群迁移 修改配置时维护系统正常 制定规范操作流程，保持生产环境稳定 保持相关知识的传承 数据模型与查询语言 # 关系模型与文档模型 # NoSQL # 从发展历史来看，最早的数据库是层次模型，所有数据都在一棵树上，可以良好地支持一对多关系，但不能支持多对多，且不支持 JOIN。</description></item><item><title>2. Storage, Query, Encoding</title><link>/notes/ddia/2/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/ddia/2/</guid><description>数据存储与检索 # 数据结构 # 最基本的数据结构：线性的 k-v 对，增加/更新时直接 append，查询时搜索整个日志找到最晚的。
哈希索引 # 在上面的日志基础上，增加一个 hashmap，记录每个 key 的最晚位置。插入更新仍然是线性的，查找的速度也接近线性。需要所有 key 能够放在内存中，适合所有 key 都经常更新的情况。
为避免用尽磁盘，将日志文件切分为段。对每个已经写完的文件段，可以进行压缩合并，即仅保留其中同一个 key 最晚的记录。这个过程可以异步，不影响正在进行的读写。注意，压缩后每个 key 的 offset 会变化，所以对每个压缩后的段需要保存新的 hashmap。读数据时，从新到旧依次查找每一个 hashmap。这是 Riak 中的 BitCask 的默认做法。
文件存储：使用二进制格式 删除记录：使用特殊的已删除标记代替 value，进行插入 崩溃恢复：可以从文件中直接还原出 hashmap，可能较慢；也可以在磁盘上保留 hashmap 的快照，减少还原时间 写入时崩溃：使用校验位，确保不会认可不完整的数据 并发控制：单线程追加，多线程读 优点：写入快，并发和崩溃恢复简单，并发能力强 缺点：哈希表需要全部放在内存，区间查询效率差（WHERE BETWEEN） SSTable 和 LSM-Tree # 在上述基础上，对于压缩合并后的段文件，对 key 进行排序；对正在写入的文件，使用平衡二叉树。LevelDB、RocksDB、HBase、Cassandra 都是基于 SSTable。SSTable术语来自 BigTable 论文。整个方法也称 LSM-Tree（Log-Structured Merge Tree)
合并段变成归并，更加高效 段文件的 hashmap 可以是稀疏的，因为可以通过排序来查找，稀疏程度参考文件块，文件块可以进行通用压缩（区分于上述的取最晚操作，而是 gzip 等通用压缩） 平衡二叉树变成已排序段文件（SS-Table）的效率较高（中序遍历） 为防止崩溃时正在写入的文件丢失，可以双写到二叉树和日志，日志用于恢复，二叉树用于查询。 当 key 完全不存在时，需要访问所有的 hashmap，可能有多次磁盘 IO；为此使用 bloomfilter 做预过滤 压缩合并的方式：LevelDB 和 RocksDB 使用分层压缩，旧数据采用更高的压缩等级；HBase 使用大小分级压缩，较新的段文件较小，被合并到较旧、较大的段文件去。Cassandra 两种都支持。TODO B 树 # 关系型数据库的标准实现，思路是 1.</description></item><item><title>3. Replication and Partition</title><link>/notes/ddia/3/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/ddia/3/</guid><description>数据复制 # 多副本的目的：
扩展性，提高读写负载能力 容错与高可用 到端上的延迟，如CDN Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。
另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。
相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。
将数据分布在多个节点上时，有两种方式：复制和分区。
本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。
主从复制 # 主节点接收所有写请求，再分发给从节点 从节点的写入顺序和主节点相同 读取时可以请求主节点或从节点 主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。
同步复制和异步复制 # 主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。
实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。
全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。
链式复制是其中一种折中方案。
配置新的从节点 # 需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。
处理节点失效 # 如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。
如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。
可能存在的问题：
异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。 其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露） 脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。 如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。 复制日志的实现 # 基于语句复制 # 如果语句使用了 RAND() NOW() 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式
如果语句的写数据依赖读数据（INSERT SELECT FROM，UPDATE WHERE 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。
其他有副作用的语句，对外部的副作用可能不同
基于 WAL 复制 # SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。</description></item><item><title>4. Transaction and Consistency</title><link>/notes/ddia/4/</link><pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate><guid>/notes/ddia/4/</guid><description/></item></channel></rss>