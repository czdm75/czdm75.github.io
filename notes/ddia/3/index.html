<!doctype html><html lang=en dir=ltr><head><meta charset=UTF-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="数据复制 # 多副本的目的：
扩展性，提高读写负载能力 容错与高可用 到端上的延迟，如CDN Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。
另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。
相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。
将数据分布在多个节点上时，有两种方式：复制和分区。
本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。
主从复制 # 主节点接收所有写请求，再分发给从节点 从节点的写入顺序和主节点相同 读取时可以请求主节点或从节点 主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。
同步复制和异步复制 # 主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。
实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。
全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。
链式复制是其中一种折中方案。
配置新的从节点 # 需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。
处理节点失效 # 如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。
如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。
可能存在的问题：
异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。 其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露） 脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。 如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。 复制日志的实现 # 基于语句复制 # 如果语句使用了 RAND() NOW() 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式
如果语句的写数据依赖读数据（INSERT SELECT FROM，UPDATE WHERE 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。
其他有副作用的语句，对外部的副作用可能不同
基于 WAL 复制 # SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。"><meta name=theme-color content="#FFFFFF"><meta name=color-scheme content="light dark"><meta property="og:url" content="/notes/ddia/3/"><meta property="og:site_name" content="czdm75 Blog"><meta property="og:title" content="3. Replication and Partition"><meta property="og:description" content="数据复制 # 多副本的目的：
扩展性，提高读写负载能力 容错与高可用 到端上的延迟，如CDN Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。
另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。
相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。
将数据分布在多个节点上时，有两种方式：复制和分区。
本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。
主从复制 # 主节点接收所有写请求，再分发给从节点 从节点的写入顺序和主节点相同 读取时可以请求主节点或从节点 主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。
同步复制和异步复制 # 主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。
实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。
全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。
链式复制是其中一种折中方案。
配置新的从节点 # 需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。
处理节点失效 # 如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。
如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。
可能存在的问题：
异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。 其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露） 脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。 如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。 复制日志的实现 # 基于语句复制 # 如果语句使用了 RAND() NOW() 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式
如果语句的写数据依赖读数据（INSERT SELECT FROM，UPDATE WHERE 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。
其他有副作用的语句，对外部的副作用可能不同
基于 WAL 复制 # SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。"><meta property="og:locale" content="en"><meta property="og:type" content="article"><meta property="article:section" content="notes"><title>3. Replication and Partition | czdm75 Blog</title>
<link rel=manifest href=/manifest.json><link rel=icon href=/favicon.png type=image/x-icon><link rel=stylesheet href=/book.min.97cfda4f5e3c9fa49a2bf8d401f4ddc0eec576c99cdcf6afbec19173200c37db.css><script defer src=/flexsearch.min.js></script><script defer src=/en.search.min.f7e98004e6b8d1bafd93b9d4b053644c96b18c50c1205ec6db396c209e97a5a3.js></script></head><body dir=ltr><input type=checkbox class="hidden toggle" id=menu-control>
<input type=checkbox class="hidden toggle" id=toc-control><main class="container flex"><aside class=book-menu><div class=book-menu-content><nav><h2 class=book-brand><a class="flex align-center" href=/><span>czdm75 Blog</span></a></h2><div class=book-search><input type=text id=book-search-input placeholder=Search aria-label=Search maxlength=64 data-hotkeys=s/><div class="book-search-spinner hidden"></div><ul id=book-search-results></ul></div><ul><li><input type=checkbox id=section-7258d8e1fea5a0c302a6de537a7b6f57 class=toggle>
<label for=section-7258d8e1fea5a0c302a6de537a7b6f57 class="flex justify-between"><a href=/cs/>Computer Science</a></label><ul><li><a href=/cs/linux-io-multiplex/>Linux IO Multiplexing</a></li></ul></li><li><input type=checkbox id=section-6c3d93bc59df31a703231f35ad75d678 class=toggle>
<label for=section-6c3d93bc59df31a703231f35ad75d678 class="flex justify-between"><a href=/distributed/>Distributed Systems</a></label><ul><li><a href=/distributed/hadoop-basic/>Hadoop Basic Concepts</a></li><li><a href=/distributed/spark-rdd/>Spark RDD Programming</a></li><li><a href=/distributed/spark-sql/>Spark SQL Programming</a></li></ul></li><li><a href=/notes/>Notes on Books</a><ul><li><input type=checkbox id=section-75aaf2c83c6ed8e1f079c5418c19dad4 class=toggle>
<label for=section-75aaf2c83c6ed8e1f079c5418c19dad4 class="flex justify-between"><a href=/notes/core-java-impatient/>Core Java for Impatients</a></label><ul><li><a href=/notes/core-java-impatient/1/>1. Basic OOP</a></li><li><a href=/notes/core-java-impatient/2/>2. Interface, Lambda</a></li><li><a href=/notes/core-java-impatient/3/>3. Inheritance, Reflection</a></li><li><a href=/notes/core-java-impatient/4/>4. Exception, Logging</a></li><li><a href=/notes/core-java-impatient/5/>5. Generics</a></li><li><a href=/notes/core-java-impatient/6/>6. Collections, Streams</a></li><li><a href=/notes/core-java-impatient/7/>7. IO, Regexp, Serialization</a></li><li><a href=/notes/core-java-impatient/8/>8. Threading</a></li><li><a href=/notes/core-java-impatient/9/>9. Notations</a></li></ul></li><li><input type=checkbox id=section-b2f01d2b22c35e214487b845322f7a58 class=toggle checked>
<label for=section-b2f01d2b22c35e214487b845322f7a58 class="flex justify-between"><a href=/notes/ddia/>Designing Data-Intensive Applications</a></label><ul><li><a href=/notes/ddia/1/>1. Data System and Data Model</a></li><li><a href=/notes/ddia/2/>2. Storage, Query, Encoding</a></li><li><a href=/notes/ddia/3/ class=active>3. Replication and Partition</a></li></ul></li><li><input type=checkbox id=section-8c64db930c5b756022bf5d3ba1af6015 class=toggle>
<label for=section-8c64db930c5b756022bf5d3ba1af6015 class="flex justify-between"><a href=/notes/in-depth-jvm/>In-depth Understanding JVM</a></label><ul><li><a href=/notes/in-depth-jvm/gc/>Garbage Collection</a></li><li><a href=/notes/in-depth-jvm/synchronization/>Java Synchronization</a></li><li><a href=/notes/in-depth-jvm/memory-model/>JVM Memory Model</a></li><li><a href=/notes/in-depth-jvm/memory-region/>JVM Memory Regions</a></li><li><a href=/notes/in-depth-jvm/threadlocal-reference/>ThreadLocal and Reference</a></li></ul></li><li><input type=checkbox id=section-15259ec15aa2cb7859c77500905f6b03 class=toggle>
<label for=section-15259ec15aa2cb7859c77500905f6b03 class="flex justify-between"><a href=/notes/intro-algo/>Introduction to Algorithms</a></label><ul><li><a href=/notes/intro-algo/1/>1. Compexity, Divide</a></li><li><a href=/notes/intro-algo/2/>2. Sorting, Order Statistic</a></li><li><a href=/notes/intro-algo/3/>3. LinkedList, HashTable</a></li><li><a href=/notes/intro-algo/4/>4. BST, Balanced BSTs</a></li><li><a href=/notes/intro-algo/5/>5. Trie-Tree, Extending Data Structures</a></li><li><a href=/notes/intro-algo/6/>6. Dynamic Programming, Greedy, Amortize</a></li><li><a href=/notes/intro-algo/7/>7. B-Tree, Fibonacci Heap, vEB Tree</a></li><li><a href=/notes/intro-algo/8/>8. Graphs</a></li></ul></li><li><input type=checkbox id=section-3efdc8e38ef7f47959bf45f30a9dec98 class=toggle>
<label for=section-3efdc8e38ef7f47959bf45f30a9dec98 class="flex justify-between"><a href=/notes/programming-scala/>Programming in Scala</a></label><ul><li><a href=/notes/programming-scala/1/>1. Basics</a></li><li><a href=/notes/programming-scala/2/>2. Functions</a></li><li><a href=/notes/programming-scala/3/>3 .Inheritance, Package, Assertion</a></li><li><a href=/notes/programming-scala/4/>4. Pattern Matching, Collections</a></li><li><a href=/notes/programming-scala/5/>5. Generics, Abstract, Implicits</a></li><li><a href=/notes/programming-scala/6/>6. Collections, Extractor, etc</a></li></ul></li></ul></li><li><input type=checkbox id=section-ebdb83a62411872593631ee1e4ab41d9 class=toggle>
<label for=section-ebdb83a62411872593631ee1e4ab41d9 class="flex justify-between"><a href=/pl/>Programming Languages</a></label><ul><li><a href=/pl/java-nio-2/>Java NIO Internal</a></li><li><a href=/pl/java-nio-1/>Java NIO Usage</a></li><li><a href=/pl/lambda/>Lambda Calculus and Y Combinator</a></li><li><a href=/pl/curry/>Scala: Currying, Partially Applied, Partial</a></li><li><a href=/pl/monad/>Scala: Monad, from Scala Perspective</a></li></ul></li></ul><ul><li><a href=https://github.com/czdm75 target=_blank rel=noopener>GitHub</a></li><li><a href=https://themes.gohugo.io/hugo-book/ target=_blank rel=noopener>Theme</a></li></ul></nav><script>(function(){var e=document.querySelector("aside .book-menu-content");addEventListener("beforeunload",function(){localStorage.setItem("menu.scrollTop",e.scrollTop)}),e.scrollTop=localStorage.getItem("menu.scrollTop")})()</script></div></aside><div class=book-page><header class=book-header><div class="flex align-center justify-between"><label for=menu-control><img src=/svg/menu.svg class=book-icon alt=Menu>
</label><strong>3. Replication and Partition</strong>
<label for=toc-control><img src=/svg/toc.svg class=book-icon alt="Table of Contents"></label></div><aside class="hidden clearfix"><nav id=TableOfContents><ul><li><a href=#数据复制>数据复制</a><ul><li><a href=#主从复制>主从复制</a><ul><li><a href=#同步复制和异步复制>同步复制和异步复制</a></li><li><a href=#配置新的从节点>配置新的从节点</a></li><li><a href=#处理节点失效>处理节点失效</a></li><li><a href=#复制日志的实现>复制日志的实现</a><ul><li><a href=#基于语句复制>基于语句复制</a></li><li><a href=#基于-wal-复制>基于 WAL 复制</a></li><li><a href=#基于行的复制>基于行的复制</a></li><li><a href=#基于触发器复制>基于触发器复制</a></li></ul></li></ul></li><li><a href=#复制滞后问题>复制滞后问题</a><ul><li><a href=#读自己的写>读自己的写</a></li><li><a href=#单调读-monotonic-read>单调读 Monotonic Read</a></li><li><a href=#前缀一致读-consistent-prefix-read>前缀一致读 Consistent Prefix Read</a></li><li><a href=#复制滞后的解决方案>复制滞后的解决方案</a></li></ul></li><li><a href=#多主节点复制>多主节点复制</a><ul><li><a href=#适用场景>适用场景</a></li><li><a href=#处理写冲突>处理写冲突</a></li><li><a href=#拓扑结构>拓扑结构</a></li></ul></li><li><a href=#无主节点复制>无主节点复制</a><ul><li><a href=#节点失效时写入数据库>节点失效时写入数据库</a></li><li><a href=#读写-quorum>读写 Quorum</a></li><li><a href=#quorum-一致性的局限性>Quorum 一致性的局限性</a></li><li><a href=#sloppy-quorum>Sloppy Quorum</a></li><li><a href=#跨数据中心的-quorum>跨数据中心的 Quorum</a></li><li><a href=#检测并发写>检测并发写</a></li></ul></li></ul></li><li><a href=#数据分区>数据分区</a><ul><li><a href=#数据分区与数据复制>数据分区与数据复制</a></li><li><a href=#kv-数据的分区>KV 数据的分区</a></li><li><a href=#分区与二级索引>分区与二级索引</a></li><li><a href=#分区再平衡>分区再平衡</a></li><li><a href=#请求路由>请求路由</a></li></ul></li></ul></nav></aside></header><article class=markdown><h1 id=数据复制>数据复制
<a class=anchor href=#%e6%95%b0%e6%8d%ae%e5%a4%8d%e5%88%b6>#</a></h1><p>多副本的目的：</p><ul><li>扩展性，提高读写负载能力</li><li>容错与高可用</li><li>到端上的延迟，如CDN</li></ul><p>Scale-up 共享内存的问题是，其性能增长有上限、非线性，不能异地容灾。由于 NUMA 的存在，即使用共享内存的架构，仍然需要一定的分区设计来达到最佳性能。</p><p>另一种传统数据仓库的方式是多个运算机器共享磁盘，通过类似 NAS 的架构。缺点是资源竞争和锁仍然需要。</p><p>相比之下，scale-out 是无共享的，每个节点的硬件都是独立的，协调在以太网上发生。</p><p>将数据分布在多个节点上时，有两种方式：复制和分区。</p><p>本章中我们假设数据大小可以在一台机器上存储，仅讨论数据复制。</p><h2 id=主从复制>主从复制
<a class=anchor href=#%e4%b8%bb%e4%bb%8e%e5%a4%8d%e5%88%b6>#</a></h2><ul><li>主节点接收所有写请求，再分发给从节点</li><li>从节点的写入顺序和主节点相同</li><li>读取时可以请求主节点或从节点</li></ul><p>主从复制的使用包括 PostgreSQL，MySQL，MongoDB，Kafka，RabbitMQ 等。</p><h3 id=同步复制和异步复制>同步复制和异步复制
<a class=anchor href=#%e5%90%8c%e6%ad%a5%e5%a4%8d%e5%88%b6%e5%92%8c%e5%bc%82%e6%ad%a5%e5%a4%8d%e5%88%b6>#</a></h3><p>主从复制的首要问题是选择同步复制还是异步复制。同步复制的情况下，只有所有的从节点都正确处理数据的改变之后，主节点上的写操作才会成功返回。这样做的数据一致性更好，但写入延迟没有保证，主节点会阻塞其后所有的写操作，可靠性更差（因为故障概率变高）。</p><p>实际上，通常会配置其中一个从节点为同步复制，其他的为异步复制，也称为半同步。如果同步节点的延迟变得不可接受，可以把某个异步节点变成同步节点。</p><p>全异步配置意味着如果主节点的数据不可恢复，写入操作就可能丢失，但系统的吞吐性能更好。</p><p>链式复制是其中一种折中方案。</p><h3 id=配置新的从节点>配置新的从节点
<a class=anchor href=#%e9%85%8d%e7%bd%ae%e6%96%b0%e7%9a%84%e4%bb%8e%e8%8a%82%e7%82%b9>#</a></h3><p>需要增加或切换从节点时，不能直接把数据移走，因为数据仍在写入；不能锁定数据库，因为违反高可用原则。因此，先将某个时间点的快照移走，然后将快照后发生的写入转移（追赶进度）。</p><h3 id=处理节点失效>处理节点失效
<a class=anchor href=#%e5%a4%84%e7%90%86%e8%8a%82%e7%82%b9%e5%a4%b1%e6%95%88>#</a></h3><p>如果从节点崩溃或网络断开，只要在恢复后追赶进度即可。</p><p>如果主节点失效，需要将某个从节点提升为主节点，同时客户端侧也需要切换写请求的目标。确认主节点失效的方法一般是节点间互相超时。随后，通过配置或选举来确认主节点，并重新配置系统，需要保证如果崩溃的主节点恢复，不会认为自己仍然是主节点。</p><p>可能存在的问题：</p><ol><li>异步复制情况下，如果新的主节点相对于崩溃的主节点有滞后，那么崩溃的主节点在恢复后会尝试把数据同步给新的主节点。</li><li>其中一种方案是直接丢弃那些没有被同步的数据。如果数据仅用在数据库内部，只会造成数据丢失；如果数据同时被外部引用，可能造成更多的不一致。（如 Github 事故中，新的从节点重复生成了 Redis 中已有的主键，造成了私有数据的泄露）</li><li>脑裂问题。如果在出现两个主节点时尝试关闭一个节点，在设计失误下又可能造成两个主节点都被关闭。</li><li>如何设置超时时间。时间过长，则系统恢复时间就很长；时间过短，则会发生很多不必要的切换，高负载下更可能雪崩。</li></ol><h3 id=复制日志的实现>复制日志的实现
<a class=anchor href=#%e5%a4%8d%e5%88%b6%e6%97%a5%e5%bf%97%e7%9a%84%e5%ae%9e%e7%8e%b0>#</a></h3><h4 id=基于语句复制>基于语句复制
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e8%af%ad%e5%8f%a5%e5%a4%8d%e5%88%b6>#</a></h4><ol><li><p>如果语句使用了 <code>RAND()</code> <code>NOW()</code> 等依赖环境的语句，数据会不一致。可以在遇到这种情况时由主节点计算出值并替换，或者临时切换到其他复制方式</p></li><li><p>如果语句的写数据依赖读数据（<code>INSERT SELECT FROM</code>，<code>UPDATE WHERE</code> 等），那么每个节点的语句执行顺序必须一样才能保证一致性，遇到事务则更加复杂。</p></li><li><p>其他有副作用的语句，对外部的副作用可能不同</p></li></ol><h4 id=基于-wal-复制>基于 WAL 复制
<a class=anchor href=#%e5%9f%ba%e4%ba%8e-wal-%e5%a4%8d%e5%88%b6>#</a></h4><ol><li><p>SS-Table 本身就是日志，B-Tree 的每一次操作会先写入 WAL。</p></li><li><p>造成数据和存储引擎紧密耦合（如 B-Tree 的 WAL 包括很多树节点粒度的操作，不能直接适配到其他数据结构）</p></li><li><p>造成数据和存储引擎版本紧密耦合，需要停机升级</p></li></ol><h4 id=基于行的复制>基于行的复制
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e8%a1%8c%e7%9a%84%e5%a4%8d%e5%88%b6>#</a></h4><ol><li><p>提供一种和 WAL 不同的逻辑日志，如 binlog</p></li><li><p>方便外部处理（如 dump 到 Hive，或另外构建索引和缓存）</p></li></ol><h4 id=基于触发器复制>基于触发器复制
<a class=anchor href=#%e5%9f%ba%e4%ba%8e%e8%a7%a6%e5%8f%91%e5%99%a8%e5%a4%8d%e5%88%b6>#</a></h4><ol><li>主节点触发外部触发器，可以执行过滤、处理等，再同步到从节点。</li><li>通常比数据库内置的方案开销更好，错误风险更大，但更灵活</li></ol><h2 id=复制滞后问题>复制滞后问题
<a class=anchor href=#%e5%a4%8d%e5%88%b6%e6%bb%9e%e5%90%8e%e9%97%ae%e9%a2%98>#</a></h2><p>异步复制下，从从节点可能读到过期的数据。考虑到数据最终会追赶上，这种效应被称为最终一致性。然而，数据同步的延迟理论上并没有上限。</p><h3 id=读自己的写>读自己的写
<a class=anchor href=#%e8%af%bb%e8%87%aa%e5%b7%b1%e7%9a%84%e5%86%99>#</a></h3><p>写入后马上从从节点读取自己刚刚写入的数据，可能读到过期的数据。保证这种读写一致性（写后读一致性）的方案：</p><ol><li>对于可能被修改的内容，从主节点读取。例如，只有用户自己能够修改用户资料，所以读取自己的用户资料时从主节点读，读取他人的可以从从节点读。</li><li>确定一个时间上限，例如发生修改后一分钟内一直从主节点读，同时对滞后超过一分钟的从节点暂时停止访问</li><li>客户端保留写入时的时间戳，与读出的数据中时间戳比较。如果发现不一致，就重试。时间戳可以是数据库的日志序列号，也可以是真实时间（需要考虑不可靠的本地时间）</li><li>对于跨地区多数据中心，需要把需求路由到主节点所在的机房</li></ol><p>用户还可能在 Web 上写，在移动端读，这时就不能依赖客户端的一致性（如时间戳）。</p><h3 id=单调读-monotonic-read>单调读 Monotonic Read
<a class=anchor href=#%e5%8d%95%e8%b0%83%e8%af%bb-monotonic-read>#</a></h3><p>如果一个用户先从进度较快的节点读一次，又从进度较慢的节点读一次，就可能看到第二次读取的值反而更老。单调读一致性的简单实现是让同一个用户永远读同一个从节点，例如通过哈希。但如果这个节点失效，就只能路由到其他副本。</p><h3 id=前缀一致读-consistent-prefix-read>前缀一致读 Consistent Prefix Read
<a class=anchor href=#%e5%89%8d%e7%bc%80%e4%b8%80%e8%87%b4%e8%af%bb-consistent-prefix-read>#</a></h3><p>对一系列写请求，需要按照相同的顺序读取。例如，事件 A 依赖事件 B，但某个副本上 B 比 A 更早得到同步的不一致情况。避免这种情况发生的一致性保证是前缀一致。当数据存在分区而两个事件存在不同的分区时，这个情况就容易发生。直觉的解决办法是追踪事件之间的因果关系，但这一点的实现并不容易。</p><h3 id=复制滞后的解决方案>复制滞后的解决方案
<a class=anchor href=#%e5%a4%8d%e5%88%b6%e6%bb%9e%e5%90%8e%e7%9a%84%e8%a7%a3%e5%86%b3%e6%96%b9%e6%a1%88>#</a></h3><p>这些问题的解决有两种思路：</p><ol><li>在应用层解决，比如上面的仅从主节点读取自己的用户资料的做法。但这种做法不太通用，且比较复杂，容易出错。</li><li>在数据库侧做这样的保证，即事务。单机事务已经比较成熟，分布式数据库则很多放弃了支持事务。</li></ol><h2 id=多主节点复制>多主节点复制
<a class=anchor href=#%e5%a4%9a%e4%b8%bb%e8%8a%82%e7%82%b9%e5%a4%8d%e5%88%b6>#</a></h2><p>显然，主从复制中如果主节点故障，所有的写操作都会中断。如果允许多个主节点，主节点之间互相同步，就可以避免这种情况。</p><h3 id=适用场景>适用场景
<a class=anchor href=#%e9%80%82%e7%94%a8%e5%9c%ba%e6%99%af>#</a></h3><p>在单个机房内部使用多主节点的意义不大，因为其实现比较复杂而机房内故障率相对低。对于跨机房数据库，如果所有写操作都只能通过主节点，就失去了多机房部署就近访问的意义。</p><p>因此，在每个机房设置一个主节点，各个机房之间进行主主同步，机房内部进行主从同步。这样做的优点是提升性能（本地访问，异步同步到其他机房），容忍机房级别的故障和跨机房专线级别的故障。当然，这种部署需要解决写冲突的问题，且和自增主键、触发器、完整性约束等功能共同使用时的陷阱还比较多。</p><p>使用多主复制的数据库包括：MySQL 的 Tungsten，PostgreSQL 的 BDR 和 Oracle 的 GoldenGate。</p><p>此外，一些应用设计也需要类似分布式数据库的同步。例如跨平台的日历应用，允许在没有网络时修改日程，此时就相当于有一个本地的主节点，需要再网络恢复时进行同步。文档的协作编辑也是一个典型的例子。</p><h3 id=处理写冲突>处理写冲突
<a class=anchor href=#%e5%a4%84%e7%90%86%e5%86%99%e5%86%b2%e7%aa%81>#</a></h3><p>有些冲突比较显然，比如同一行在两个不同的节点上被写不同的值；有的冲突比较隐晦，例如同一个会议室被预定两次：在两个节点上分别预定时，两个节点都认为这个会议室是空的。</p><ol><li><p>避免冲突，例如上面提到过的，对同一个文档路由到同一个主节点，对单个文档就变成了主从复制模型。但是，这仍然要面临数据中心故障或用户离另一个数据中心更近，一定要路由到其他数据中心的情况。</p></li><li><p>收敛到一致状态</p><ol><li>给每个写入分配唯一的 ID，如时间戳、哈希、UUID 等，选择最晚 / 最大的，即最后写入者获胜。这种方法比较普遍，且在很多场景下适用，但会造成写入较早的一方数据丢失</li><li>为每个节点分配一个 ID，取 ID 最大的节点的版本，问题同上，甚至更加无法和业务场景契合</li><li>合并冲突的值</li><li>保留冲突为一个特殊的结构，然后依靠应用层或通知用户来解决（想象异步的 Git Merge）</li></ol></li><li><p>自定义冲突解决逻辑</p><ol><li>从实用角度来讲，最合适的方法仍然是在应用层解决冲突，让数据库的使用者来编写冲突解决逻辑。这个逻辑可以是在写时执行或读时执行</li><li>通常，这个解决逻辑的粒度是单个行（对于关系型数据库）或单个文档（对于文档型数据库），因此对于包含多个写的原子事务存在一定局限性</li></ol></li></ol><p>自定义冲突解决逻辑很难以编写，需要考虑不同的业务场景。有一些自动冲突解决的探索：</p><ol><li>CRDT (Conflict-free Replicated Data Types) ，是一些可以多个用户同时编辑的数据结构，如 map、list、counter 等。一些在 Riak 中已经被实现</li><li>Mergable Persistent Data，使用类似 Git 的方式跟踪变更历史，使用三向合并(Three-way merge function）。相比之下，CRDT 是双向合并。</li><li>Operational transformation，是 Google Doc 使用的冲突解决算法，适用于可同时编辑的有序列表。</li></ol><h3 id=拓扑结构>拓扑结构
<a class=anchor href=#%e6%8b%93%e6%89%91%e7%bb%93%e6%9e%84>#</a></h3><p>多个主节点的同步方向可以有不同的拓扑结构：星形，环形，全连接型。</p><p>星形和环形的问题是，每个数据需要经过多个节点才能同步完成，且单个节点宕机仍然会影响同步，需要重新配置拓扑结构。</p><p>全连接的问题是，每条链路的速度不一定一致，可能造成某一行的 UPDATE 操作比 INSERT 更早抵达之类的情况。</p><p>一种解决方案是使用版本向量。总的来说，在许多多主系统中，冲突检测技术还很不完善。</p><h2 id=无主节点复制>无主节点复制
<a class=anchor href=#%e6%97%a0%e4%b8%bb%e8%8a%82%e7%82%b9%e5%a4%8d%e5%88%b6>#</a></h2><h3 id=节点失效时写入数据库>节点失效时写入数据库
<a class=anchor href=#%e8%8a%82%e7%82%b9%e5%a4%b1%e6%95%88%e6%97%b6%e5%86%99%e5%85%a5%e6%95%b0%e6%8d%ae%e5%ba%93>#</a></h3><p>Dynamo 风格数据库，Riak、Cassandra、Voldemort 都使用类似的风格。客户端或客户端直接访问的协调者<strong>将写请求同时发送到多个节点</strong>，通过节点间的同步交互来进行冲突处理。在读请求的时候，也是同时读多个节点，经过协调确认最终结果。</p><p>一个失效的节点重新上线后，进行同步的方式：</p><ol><li>读修复：在读取时，从某些节点读到旧数据，另一些读到新数据，就把新数据写入到旧数据的节点里，相当于加快同步。</li><li>反熵过程：由后台进程不断查找副本间的差异并同步。这个过程并不保证数据同步的顺序，同步滞后可能很长</li></ol><p>如果系统只实现了读修复而没有实现反熵，那么没有被读取到的数据就会长期保留旧数据的状态，相当于副本数偏少，节点故障的容忍能力下降。</p><h3 id=读写-quorum>读写 Quorum
<a class=anchor href=#%e8%af%bb%e5%86%99-quorum>#</a></h3><p>如果有 n 个副本，写入需要 w 个节点确认，读取至少读 r 个节点，那么当 w + r > n，就可以确保读到最新值，这种方式称为仲裁读和仲裁写。</p><p>通常设置 n 为一个奇数 3 或 5，w = r = (n+1)/2。注意这不代表集群只有 n 个节点。</p><p>通常读写请求都是同时发送到全部 n 个副本，w 和 r 只是读取和写入成功的判断标准。</p><ul><li>当 w &lt; n，在写入时可以容忍节点不可用</li><li>当 r &lt; n，在读取时可以容忍节点不可用</li><li>当 n = 3, w = r = 2，可以容忍一个节点不可用</li><li>当 n = 5, w = r = 3，可以容忍两个节点不可用</li></ul><h3 id=quorum-一致性的局限性>Quorum 一致性的局限性
<a class=anchor href=#quorum-%e4%b8%80%e8%87%b4%e6%80%a7%e7%9a%84%e5%b1%80%e9%99%90%e6%80%a7>#</a></h3><p>上面的 w + r > n 的限制可以保证一定可以读到最新值，同时也确定了对节点失效的容忍能力。如果放开这个限制，那么就有可能读到旧值，降低一致性，但同时也可以提升性能（更低的延迟）和可用性（更高的节点失效容忍能力）。</p><p>即使约定 w + r > n，也可能存在一些局限：</p><ol><li>sloppy quorum</li><li>写冲突</li><li>如果在写入过程中进行读取，数据可能尚未抵达足够多的节点</li><li>如果一次写入的部分节点成功，而部分节点失败，总的成功数量没有达到 w，整个写请求被认为是回滚状态，那么那些已经写入完成的节点上就存在了异常的值</li><li>如果某个存有新值的节点失效，恢复时从存有旧值的节点同步数据，就打破了上面 w 个副本的要求</li><li>其他异常情况，参见第九章“可线性化与 Quorum”</li></ol><p>整体来说，我们仍然把 Dynamo 风格的数据库看作最终一致性的数据库，而不认为上面这些归档能够有真正的数据一致保证。</p><p>即使应用本身可以接受读到旧值，监控数据库的同步状态仍然有很大运维意义。对主从复制，因为同步存在顺序，可以直接通过偏移量监控。对于无主复制的系统，不仅没有同步顺序，如果只使用读修复，值的同步延迟就是无上限的。我们可以基于 w r n 的值对旧值的比例给出一个估计。</p><h3 id=sloppy-quorum>Sloppy Quorum
<a class=anchor href=#sloppy-quorum>#</a></h3><p>如果一个大集群发生了网络分区，无法访问到 w 个节点，应该如何处理写请求？一个方案是，将写请求暂时写入到 n 个节点之外的可访问节点（其他分区的节点）上。这时我们认为写入和读取仍然需要 w 和 r 个节点，但不一定是原来的 n 个节点之内的。当网络得到恢复，就要把这些临时数据放回应该在的位置。</p><p>显然，这样做会失去读到最新的保证，同时也大大提高了写入可用性。Riak 默认开启，Cassandra 和 Voldemort 默认关闭。</p><h3 id=跨数据中心的-quorum>跨数据中心的 Quorum
<a class=anchor href=#%e8%b7%a8%e6%95%b0%e6%8d%ae%e4%b8%ad%e5%bf%83%e7%9a%84-quorum>#</a></h3><p>Cassandra 和 Voldemort 的做法是将副本数 n 分布到各个机房，写入时会同时向本地机房和远端机房发送写请求，但只等待本地机房的写入结果，从而同时获得容灾能力和低延迟。</p><p>Riak 的做法是在每个机房单独配置集群，在各个集群之间采用类似多主复制的方式来同步。</p><h3 id=检测并发写>检测并发写
<a class=anchor href=#%e6%a3%80%e6%b5%8b%e5%b9%b6%e5%8f%91%e5%86%99>#</a></h3><p>和多主复制一样，Quorum 机制不能解决写冲突的问题。</p><p>在使用 LWW（Last Write Wins）的方法时，在多节点同时写的情况下，如果不确定一个规则，就可能永远无法达成一致。但我们事实上很难确定事件发生的自然顺序，因此人为指定一个排序即可。这样可以达到最终一致性，但带来数据丢失。</p><p>当两个操作互相之间没有依赖关系（happens-before），就是并发的。</p><p>先考虑一个简单的情况：一个单副本的购物车数据库，在两个客户端并发写入。于是数据库会维护两个版本的数据，客户端在每次读取时带上自己已知的上一个版本，从而让数据库知道应该覆盖哪一个版本的数据；同时数据库会将两个版本都返回给客户端，让客户端自己来处理两个版本的合并。</p><p><img src=../quorum-merge.png alt="Data Merging under Quorum"></p><p>这样，我们同时获得了最终一致性，也没有发生值的丢失。</p><p>在确定合并逻辑时可以使用版本向量、软删除墓碑、CRDT 等方法。</p><h1 id=数据分区>数据分区
<a class=anchor href=#%e6%95%b0%e6%8d%ae%e5%88%86%e5%8c%ba>#</a></h1><h2 id=数据分区与数据复制>数据分区与数据复制
<a class=anchor href=#%e6%95%b0%e6%8d%ae%e5%88%86%e5%8c%ba%e4%b8%8e%e6%95%b0%e6%8d%ae%e5%a4%8d%e5%88%b6>#</a></h2><p>一个节点可能同时既是某个分区的主副本，又是其他分区的从副本。</p><h2 id=kv-数据的分区>KV 数据的分区
<a class=anchor href=#kv-%e6%95%b0%e6%8d%ae%e7%9a%84%e5%88%86%e5%8c%ba>#</a></h2><ul><li>range分区，手动或自动。存在查询热点问题，如大部分请求读的都是最新的数据。可以先按其他字段分区，再按时间分区。</li><li>hash分区<ul><li>hash函数需要是一致的（反例：Java和ruby的hashcode在不同进程中不同），但不需要加密性很强</li><li>丧失区间查询能力</li></ul></li><li>Cassandra 的折中：复合主键的第一个字段用哈希，剩下的排序用range，确定第一个主键后剩下的可以区间查询</li></ul><p>一致性哈希（consistent hash）解决的问题：对朴素的mod n哈希分区，如果增加一个分区，那么有大量的数据会改变前后所在的节点，需要大量数据迁移或缓存失效；一致性哈希在扩容时不改变大部分分区的hash范围，仅变更扩容处的，解决这个问题；ddia称目前并不常用，实际效果不好。</p><p>哈希仍有不能解决的倾斜，如明星热点事件，单个ID有大量访问。简单的办法是增加一个随机数拆分，查询完再合并。</p><h2 id=分区与二级索引>分区与二级索引
<a class=anchor href=#%e5%88%86%e5%8c%ba%e4%b8%8e%e4%ba%8c%e7%ba%a7%e7%b4%a2%e5%bc%95>#</a></h2><p>HBase，Voldemort 等 kv 数据库不支持二级索引，Riak 等正在尝试增加；对于 Solr 和 Elastic Search 等，二级索引是其存在的根本意义。</p><ul><li>基于文档的二级索引：每个分区一个二级索引表，对每一条新纪录，插入时在二级索引中添加这条新记录的ID。需要确保二级索引和kv的一致性。<ul><li>在二级索引上查询时，需要查询所有分区的二级索引，再合并，称分散/聚集（scatter/gather），写放大严重，延迟高</li><li>Mongo，Riak，Cassandra，ES，Solr，Volt等使用</li></ul></li><li>基于词条的二级索引：全局使用同一个二级索引，二级索引本身也分区，索引本身可以使用range分区<ul><li>不需要读取所有分区</li><li>写入慢且复杂</li><li>数据和索引一致性和写入延迟之间需要权衡。如果要同步索引，则一个事务需要至少读写两个分区（KV 和索引），因此目前数据库都不支持同步索引</li></ul></li></ul><h2 id=分区再平衡>分区再平衡
<a class=anchor href=#%e5%88%86%e5%8c%ba%e5%86%8d%e5%b9%b3%e8%a1%a1>#</a></h2><p>直接取模会遇到大量数据迁移</p><ul><li>固定数量分区：提前确定数量超多的分区，固定数量；节点负载增加时将分区移走但不需要改变哈希<ul><li>Riak ES Couchbase Voldemont使用这种方式</li><li>分区数量固定，大小和数据量成正比</li></ul></li><li>动态分区：允许分区合并拆分，类似b树<ul><li>HBase 和 Rethink 使用这种方式</li><li>对 HBase，拆分的数据迁移依赖 HDFS</li><li>当数据库为空时只有一个分区，会造成流量集中在一个节点；解决办法是预分裂</li><li>分区大小有上下限（合并拆分），数量与数据量成正比</li></ul></li><li>按节点比例分区：每个节点的分区数量一定<ul><li>分区数量不和数据量成正比，而是和节点数量成正比</li><li>Cassandra 和 Ketama 使用这种方式</li><li>符合一致性哈希</li></ul></li></ul><p>再平衡可以是自动的或手动的。自动再平衡节省运维，但遇到故障容易出现负载失控；如果一个节点负载很高，可能将其认为失效，对其进行数据迁移反而进一步加重负载，甚至导致雪崩。</p><h2 id=请求路由>请求路由
<a class=anchor href=#%e8%af%b7%e6%b1%82%e8%b7%af%e7%94%b1>#</a></h2><ul><li>随机选择（普通的负载均衡），允许客户端连接到任意节点，再路由到分区所在节点</li><li>增加一个路由层，客户端连接到路由层</li><li>客户端感知节点分配，直接连接目标节点</li></ul><p>共同问题：及时更新分区变更</p><ul><li>配置中心<ul><li>Linkedin 的 Espresso 使用基于 ZK 的 Helix</li><li>HBase，Solr，Kafka 使用 ZK</li><li>Mongo 使用自己的配置中心</li></ul></li><li>Gossip 协议，使用上述的路由方案1<ul><li>Cassandra，Riak</li></ul></li><li>Couchbase不支持自动再平衡，使用路由层 moxi 向集群学习路由变化</li></ul><p>使用路由层或随机选择时，仍然需要知道 IP 地址，因为变更不频繁，使用 DNS 就足够。</p></article><footer class=book-footer><div class="flex flex-wrap justify-between"></div><script>(function(){function e(e){const t=window.getSelection(),n=document.createRange();n.selectNodeContents(e),t.removeAllRanges(),t.addRange(n)}document.querySelectorAll("pre code").forEach(t=>{t.addEventListener("click",function(){e(t.parentElement),navigator.clipboard&&navigator.clipboard.writeText(t.parentElement.textContent)})})})()</script></footer><label for=menu-control class="hidden book-menu-overlay"></label></div><aside class=book-toc><div class=book-toc-content><nav id=TableOfContents><ul><li><a href=#数据复制>数据复制</a><ul><li><a href=#主从复制>主从复制</a><ul><li><a href=#同步复制和异步复制>同步复制和异步复制</a></li><li><a href=#配置新的从节点>配置新的从节点</a></li><li><a href=#处理节点失效>处理节点失效</a></li><li><a href=#复制日志的实现>复制日志的实现</a><ul><li><a href=#基于语句复制>基于语句复制</a></li><li><a href=#基于-wal-复制>基于 WAL 复制</a></li><li><a href=#基于行的复制>基于行的复制</a></li><li><a href=#基于触发器复制>基于触发器复制</a></li></ul></li></ul></li><li><a href=#复制滞后问题>复制滞后问题</a><ul><li><a href=#读自己的写>读自己的写</a></li><li><a href=#单调读-monotonic-read>单调读 Monotonic Read</a></li><li><a href=#前缀一致读-consistent-prefix-read>前缀一致读 Consistent Prefix Read</a></li><li><a href=#复制滞后的解决方案>复制滞后的解决方案</a></li></ul></li><li><a href=#多主节点复制>多主节点复制</a><ul><li><a href=#适用场景>适用场景</a></li><li><a href=#处理写冲突>处理写冲突</a></li><li><a href=#拓扑结构>拓扑结构</a></li></ul></li><li><a href=#无主节点复制>无主节点复制</a><ul><li><a href=#节点失效时写入数据库>节点失效时写入数据库</a></li><li><a href=#读写-quorum>读写 Quorum</a></li><li><a href=#quorum-一致性的局限性>Quorum 一致性的局限性</a></li><li><a href=#sloppy-quorum>Sloppy Quorum</a></li><li><a href=#跨数据中心的-quorum>跨数据中心的 Quorum</a></li><li><a href=#检测并发写>检测并发写</a></li></ul></li></ul></li><li><a href=#数据分区>数据分区</a><ul><li><a href=#数据分区与数据复制>数据分区与数据复制</a></li><li><a href=#kv-数据的分区>KV 数据的分区</a></li><li><a href=#分区与二级索引>分区与二级索引</a></li><li><a href=#分区再平衡>分区再平衡</a></li><li><a href=#请求路由>请求路由</a></li></ul></li></ul></nav></div></aside></main></body></html>